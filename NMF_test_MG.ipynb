{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71b62231-ffba-43ba-a846-b5aea7f03dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dce042a7-a776-455b-b74f-fdc516eb6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# df = pd.read_csv(\"English_fiction_pre_PCA_3.csv\")\n",
    "# df = df.iloc[:300]\n",
    "# train_index = list(range(0, 241))\n",
    "# test_index = list(range(241, 301))\n",
    "\n",
    "df = pd.read_csv(\"English_fiction_pre_PCA_3.csv\")\n",
    "train_df = pd.read_csv(\"original_data/train_indices.csv\")\n",
    "test_df = pd.read_csv(\"original_data/test_indices.csv\")\n",
    "train_index = train_df[\"index\"].tolist()\n",
    "test_index = test_df[\"index\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d942066-7911-4454-8e6f-c8e82bc5756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_nouns(text):\n",
    "    \"\"\"\n",
    "    concatenate and apply lowercase lettering to proper nouns like names, publishers, and book titles\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: remove leading or ending brackets (if applicable) and internal quote marks\n",
    "    text = text.strip(\"[]\")\n",
    "    text = text.replace(\"'\", \"\")\n",
    "\n",
    "    # Step 2: If there are multiple nouns, split at the comma\n",
    "    text = text.split(\", \")\n",
    "\n",
    "    # Step 3: Concatenate each noun and put all letters in lowercase:\n",
    "    text = [x.replace(\" \", \"\").lower() for x in text]\n",
    "\n",
    "    # Step 4: Convert the list of tokens to a string\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def add_tokens_to_description(df):\n",
    "    df[\"description\"] += \" \" + df[\"authors\"].apply(concat_nouns)\n",
    "    df[\"description\"] += \" \" + df[\"publisher\"].apply(concat_nouns)\n",
    "    df[\"description\"] += \" \" + df[\"Title\"].str.lower()\n",
    "\n",
    "def calculate_ngrams_RAKE(text: str):\n",
    "    r_unigram = Rake()\n",
    "    r_unigram.extract_keywords_from_text(text)\n",
    "    \n",
    "    keyword_dict_scores = r_unigram.get_word_degrees()\n",
    "    words = list(keyword_dict_scores.keys())\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "def create_tokens(df, input_column: str, output_column: str):  \n",
    "    df[output_column] = df[\"description\"].apply(calculate_ngrams_RAKE)\n",
    "\n",
    "# Calculate tfidf matrix:\n",
    "def calculate_TFIDF(df, BOW_column: str, train_index, test_index):\n",
    "    # Split the incoming dataframe into train and test slices base on the list of train and test indices provided:\n",
    "    X_train_df = df[df[\"index\"].isin(train_index)]\n",
    "    X_test_df = df[df[\"index\"].isin(test_index)]\n",
    "    \n",
    "    #instantiating and generating the tfidf\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train_df[BOW_column])\n",
    "    X_test_tfidf = vectorizer.transform(X_test_df[BOW_column])\n",
    "\n",
    "    # convert the tfidf matrix to a dense matrix\n",
    "    dense_X_train_tfidf = X_train_tfidf.toarray()\n",
    "    dense_X_test_tfidf = X_test_tfidf.toarray()\n",
    "\n",
    "    # # combine the arrays:\n",
    "    # dense_X = np.concatenate((dense_X_train_tfidf, dense_X_test_tfidf), axis = 0)\n",
    "\n",
    "    # Determine the column names for our dense matrix and create a dataframe with the \n",
    "    # vocabulary as columns:\n",
    "    temp_dict = {}\n",
    "    for counter, i in enumerate(list(vectorizer.vocabulary_.items())):\n",
    "            temp_dict[i[1]] = i[0]\n",
    "    \n",
    "    column_names = []\n",
    "    for i in range(len(temp_dict)):\n",
    "        column_names.append(temp_dict[i])\n",
    "\n",
    "    # Convert the array back into a dataframe:\n",
    "    scaled_dataframe_X_train=pd.DataFrame(dense_X_train_tfidf, columns= column_names)\n",
    "    scaled_dataframe_X_test=pd.DataFrame(dense_X_test_tfidf, columns= column_names) \n",
    "\n",
    "    return scaled_dataframe_X_train, scaled_dataframe_X_test, column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc517f4d-917b-493a-a260-2ff9f97c5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tokens from other columns to the description column, specifically author, title, and publisher\n",
    "add_tokens_to_description(df)\n",
    "\n",
    "# Create tokens from the book descriptions and save this in a new column called \"tokens\"\n",
    "create_tokens(df, \"description\", \"tokens\")\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, column_names = calculate_TFIDF(df, \"tokens\", train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c1b51953-e38e-44e8-a8a5-3e1a38b28299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21419, 80652)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "296fe87d-edaa-48e9-9d43-0c713b0159b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01 0.   0.   ... 0.   0.   0.  ]\n",
      " [0.02 0.   0.02 ... 0.   0.   0.  ]\n",
      " [0.   0.   0.   ... 0.   0.   0.03]\n",
      " ...\n",
      " [0.   0.   0.   ... 0.01 0.   0.03]\n",
      " [0.   0.   0.   ... 0.   0.01 0.03]\n",
      " [0.01 0.   0.   ... 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "NMF_model = NMF(n_components=20)\n",
    "\n",
    "# Fit the model to the training tfidf matrix\n",
    "NMF_model.fit(X_train_tfidf)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = NMF_model.transform(X_train_tfidf)\n",
    "\n",
    "# Print the NMF features\n",
    "print(nmf_features.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bd253b7-7383-4b23-afb4-ebc8cffa2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_df = pd.DataFrame(NMF_model.components_, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1878f9cf-bc46-4b3a-b7ef-11824c73e66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 80652)\n",
      "novel         0.859089\n",
      "story         0.662610\n",
      "century       0.493440\n",
      "american      0.488379\n",
      "war           0.360296\n",
      "life          0.352732\n",
      "one           0.341700\n",
      "world         0.306742\n",
      "first         0.298919\n",
      "set           0.288706\n",
      "history       0.285090\n",
      "written       0.281847\n",
      "great         0.281543\n",
      "work          0.272101\n",
      "published     0.267047\n",
      "fiction       0.250500\n",
      "america       0.249732\n",
      "novels        0.248344\n",
      "characters    0.247551\n",
      "tale          0.235711\n",
      "writer        0.232756\n",
      "early         0.221420\n",
      "literary      0.220722\n",
      "modern        0.217667\n",
      "two           0.213262\n",
      "historical    0.207342\n",
      "also          0.200618\n",
      "tells         0.197228\n",
      "love          0.195827\n",
      "time          0.188737\n",
      "Name: 15, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "\n",
    "# Select row 3: component\n",
    "component = components_df.iloc[15]\n",
    "\n",
    "# Print result of nlargest\n",
    "print(component.nlargest(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ddd560bf-5d8f-4fd9-8035-b25f0bb2de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {0: \"nostalgia\", \n",
    "          1: \"self-published/debut\",\n",
    "          2: \"story/anthology\",\n",
    "          3: \"womens_fiction\",\n",
    "          4: \"childrens_books\",\n",
    "          5: \"classic\",\n",
    "          6: \"family_drama\",\n",
    "          7: \"digital_books/recreations\",\n",
    "          8: \"reproduced\",\n",
    "          9: \"murder_mystery\",\n",
    "          10: \"reprint\",\n",
    "          11: \"bestselling_author\",\n",
    "          12: \"romance\",\n",
    "          13: \"unkonwn\",\n",
    "          14: \"teen\",\n",
    "          15: \"novel\",\n",
    "          16: \"world/battle\",\n",
    "          17: \"unknown\",\n",
    "          18: \"young_adult\",\n",
    "          19: \"coming_of_age\",\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ac667d93-cafe-4444-95ca-295e8c3f167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02 0.   0.   0.   0.   0.   0.   0.02 0.   0.   0.   0.   0.   0.\n",
      " 0.06 0.   0.   0.   0.03 0.  ]\n",
      "['teen']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sixteen-year-old Haley Andromeda would like to think she's just a normal high school senior, but during a disastrous time in her life, she turns to the Ouija board and tries to communicate with the Other Side, which only leads to further complications. Original. karenrivers raincoastbooks the healing time of hickeys\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df = df[df[\"index\"].isin(train_index)]\n",
    "X_test_df = df[df[\"index\"].isin(test_index)]\n",
    "book_index = 95\n",
    "\n",
    "print(nmf_features[book_index].round(2))\n",
    "\n",
    "# retrieve the nmf features for a predefined index\n",
    "nmf_feature_list = nmf_features[book_index].round(2).tolist()\n",
    "\n",
    "# calculate the maxium nmf value(s)\n",
    "max_nmf_value = max(nmf_feature_list)\n",
    "\n",
    "# find the indices in the nmf feature list to be used to convert the values to category labels\n",
    "indices = [i for i, x in enumerate(nmf_feature_list) if x == max_nmf_value]\n",
    "indices\n",
    "\n",
    "category_label = [topics[i] for i in indices]\n",
    "print(category_label)\n",
    "X_train_df[\"description\"].iloc[book_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0236e678-c010-461f-9ac2-116136a210cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5633940-ecf6-4619-be56-918ff352c5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
