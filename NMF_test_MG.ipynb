{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "71b62231-ffba-43ba-a846-b5aea7f03dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "dce042a7-a776-455b-b74f-fdc516eb6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# df = pd.read_csv(\"English_fiction_pre_PCA_3.csv\")\n",
    "# df = df.iloc[:300]\n",
    "# train_index = list(range(0, 241))\n",
    "# test_index = list(range(241, 301))\n",
    "\n",
    "df = pd.read_csv(\"English_fiction_pre_PCA_3.csv\")\n",
    "train_df = pd.read_csv(\"original_data/train_indices.csv\")\n",
    "test_df = pd.read_csv(\"original_data/test_indices.csv\")\n",
    "train_index = train_df[\"index\"].tolist()\n",
    "test_index = test_df[\"index\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5d942066-7911-4454-8e6f-c8e82bc5756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_nouns(text):\n",
    "    \"\"\"\n",
    "    concatenate and apply lowercase lettering to proper nouns like names, publishers, and book titles\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: remove leading or ending brackets (if applicable) and internal quote marks\n",
    "    text = text.strip(\"[]\")\n",
    "    text = text.replace(\"'\", \"\")\n",
    "\n",
    "    # Step 2: If there are multiple nouns, split at the comma\n",
    "    text = text.split(\", \")\n",
    "\n",
    "    # Step 3: Concatenate each noun and put all letters in lowercase:\n",
    "    text = [x.replace(\" \", \"\").lower() for x in text]\n",
    "\n",
    "    # Step 4: Convert the list of tokens to a string\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def add_tokens_to_description(df):\n",
    "    df[\"description\"] += \" \" + df[\"authors\"].apply(concat_nouns)\n",
    "    df[\"description\"] += \" \" + df[\"publisher\"].apply(concat_nouns)\n",
    "    df[\"description\"] += \" \" + df[\"Title\"].str.lower()\n",
    "\n",
    "def calculate_ngrams_RAKE(text: str):\n",
    "    r_unigram = Rake()\n",
    "    r_unigram.extract_keywords_from_text(text)\n",
    "    \n",
    "    keyword_dict_scores = r_unigram.get_word_degrees()\n",
    "    words = list(keyword_dict_scores.keys())\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "def create_tokens(df, input_column: str, output_column: str):  \n",
    "    df[output_column] = df[\"description\"].apply(calculate_ngrams_RAKE)\n",
    "\n",
    "# Calculate tfidf matrix:\n",
    "def calculate_TFIDF(df, BOW_column: str, train_index, test_index):\n",
    "    # Split the incoming dataframe into train and test slices base on the list of train and test indices provided:\n",
    "    X_train_df = df[df[\"index\"].isin(train_index)]\n",
    "    X_test_df = df[df[\"index\"].isin(test_index)]\n",
    "    \n",
    "    #instantiating and generating the tfidf\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train_df[BOW_column])\n",
    "    X_test_tfidf = vectorizer.transform(X_test_df[BOW_column])\n",
    "\n",
    "    # convert the tfidf matrix to a dense matrix\n",
    "    dense_X_train_tfidf = X_train_tfidf.toarray()\n",
    "    dense_X_test_tfidf = X_test_tfidf.toarray()\n",
    "\n",
    "    # # combine the arrays:\n",
    "    # dense_X = np.concatenate((dense_X_train_tfidf, dense_X_test_tfidf), axis = 0)\n",
    "\n",
    "    # Determine the column names for our dense matrix and create a dataframe with the \n",
    "    # vocabulary as columns:\n",
    "    temp_dict = {}\n",
    "    for counter, i in enumerate(list(vectorizer.vocabulary_.items())):\n",
    "            temp_dict[i[1]] = i[0]\n",
    "    \n",
    "    column_names = []\n",
    "    for i in range(len(temp_dict)):\n",
    "        column_names.append(temp_dict[i])\n",
    "\n",
    "    # Convert the array back into a dataframe:\n",
    "    scaled_dataframe_X_train=pd.DataFrame(dense_X_train_tfidf, columns= column_names)\n",
    "    scaled_dataframe_X_test=pd.DataFrame(dense_X_test_tfidf, columns= column_names) \n",
    "\n",
    "    return scaled_dataframe_X_train, scaled_dataframe_X_test, column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bc517f4d-917b-493a-a260-2ff9f97c5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tokens from other columns to the description column, specifically author, title, and publisher\n",
    "add_tokens_to_description(df)\n",
    "\n",
    "# Create tokens from the book descriptions and save this in a new column called \"tokens\"\n",
    "create_tokens(df, \"description\", \"tokens\")\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, column_names = calculate_TFIDF(df, \"tokens\", train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c1b51953-e38e-44e8-a8a5-3e1a38b28299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21419, 80652)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "296fe87d-edaa-48e9-9d43-0c713b0159b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   ... 0.   0.   0.03]\n",
      " [0.   0.   0.   ... 0.   0.   0.  ]\n",
      " [0.   0.   0.   ... 0.   0.   0.  ]\n",
      " ...\n",
      " [0.   0.   0.   ... 0.   0.   0.  ]\n",
      " [0.   0.   0.   ... 0.02 0.05 0.  ]\n",
      " [0.   0.   0.   ... 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "NMF_model = NMF(n_components=100)\n",
    "\n",
    "# Fit the model to the training tfidf matrix\n",
    "NMF_model.fit(X_train_tfidf)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = NMF_model.transform(X_train_tfidf)\n",
    "\n",
    "# Print the NMF features\n",
    "print(nmf_features.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "21cd009c-3070-45fd-acdf-4e8a1e09e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_features_test = NMF_model.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8084df97-a954-4404-88a3-5aa10acf4c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21419\n",
      "(21419, 100)\n",
      "5355\n",
      "(5355, 100)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_index))\n",
    "print(nmf_features.shape)\n",
    "\n",
    "print(len(test_index))\n",
    "print(nmf_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3bd253b7-7383-4b23-afb4-ebc8cffa2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_df = pd.DataFrame(NMF_model.components_, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1878f9cf-bc46-4b3a-b7ef-11824c73e66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 80652)\n",
      "friend        4.547039\n",
      "best          4.352509\n",
      "selling       0.697111\n",
      "friendship    0.347126\n",
      "dog           0.247043\n",
      "away          0.210772\n",
      "childhood     0.157979\n",
      "ever          0.141236\n",
      "loved         0.137807\n",
      "summer        0.134270\n",
      "helps         0.125767\n",
      "close         0.123370\n",
      "also          0.118257\n",
      "full          0.114116\n",
      "seller        0.110791\n",
      "takes         0.109695\n",
      "jason         0.107355\n",
      "better        0.107301\n",
      "college       0.106310\n",
      "decide        0.099847\n",
      "Name: 50, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "\n",
    "# Select row 3: component\n",
    "component = components_df.iloc[50]\n",
    "\n",
    "# Print result of nlargest\n",
    "print(component.nlargest(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ddd560bf-5d8f-4fd9-8035-b25f0bb2de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {0: \"nostalgia\", \n",
    "          1: \"self-published/debut\",\n",
    "          2: \"story/anthology\",\n",
    "          3: \"womens_fiction\",\n",
    "          4: \"childrens_books\",\n",
    "          5: \"classic\",\n",
    "          6: \"family_drama\",\n",
    "          7: \"digital_books/recreations\",\n",
    "          8: \"reproduced\",\n",
    "          9: \"murder_mystery\",\n",
    "          10: \"reprint\",\n",
    "          11: \"bestselling_author\",\n",
    "          12: \"romance\",\n",
    "          13: \"unkonwn\",\n",
    "          14: \"teen\",\n",
    "          15: \"novel\",\n",
    "          16: \"world/war/historical_fiction\",\n",
    "          17: \"unknown\",\n",
    "          18: \"young_adult\",\n",
    "          19: \"coming_of_age\",\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ac667d93-cafe-4444-95ca-295e8c3f167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02]\n",
      "['romance']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Always The Chaperon And Never The Bride... At least, that's the way it was for Lady Annis Wyncherley. If this young widow was to remain as chaperon to society's misses, there could be no hint of scandal attached to her name. Rakes and romance were strictly off-limits, most especially a rogue like the handsome Lord Adam Ashwick! But that proved nearly impossible when Adam made his daughter's chaperon the subject of his relentless seduction. Adam knew any attention from him could destroy Lady Wyncherley's fine reputation. But he was powerless to control the strong desires she aroused in him. And all too soon this reformed rogue was hell-bent on convincing a very stubborn Annis to become his chaperon bride.... nicolacornick harlequin chaperon bride, the (historical romance)\""
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df = df[df[\"index\"].isin(train_index)]\n",
    "X_test_df = df[df[\"index\"].isin(test_index)]\n",
    "book_index = 12\n",
    "\n",
    "# retrieve the nmf features for a predefined index\n",
    "nmf_feature_list = nmf_features_test[book_index].round(2).tolist()\n",
    "print(nmf_feature_list)\n",
    "\n",
    "# calculate the maxium nmf value(s)\n",
    "max_nmf_value = max(nmf_feature_list)\n",
    "\n",
    "# find the indices in the nmf feature list to be used to convert the values to category labels\n",
    "indices = [i for i, x in enumerate(nmf_feature_list) if x == max_nmf_value]\n",
    "indices\n",
    "\n",
    "category_label = [topics[i] for i in indices]\n",
    "print(category_label)\n",
    "X_test_df[\"description\"].iloc[book_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0236e678-c010-461f-9ac2-116136a210cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train_df.insert(location, f\"NMF_{i}\", nmf_features[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train_df.insert(location, f\"NMF_{i}\", nmf_features[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train_df.insert(location, f\"NMF_{i}\", nmf_features[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train_df.insert(location, f\"NMF_{i}\", nmf_features[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train_df.insert(location, f\"NMF_{i}\", nmf_features[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train_df.insert(location, f\"NMF_{i}\", nmf_features[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test_df.insert(location, f\"NMF_{i}\", nmf_features_test[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test_df.insert(location, f\"NMF_{i}\", nmf_features_test[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test_df.insert(location, f\"NMF_{i}\", nmf_features_test[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test_df.insert(location, f\"NMF_{i}\", nmf_features_test[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test_df.insert(location, f\"NMF_{i}\", nmf_features_test[:,i].tolist())\n",
      "/var/folders/62/rv9lbyxx6l110tffps2wcg2r0000gn/T/ipykernel_15362/1199177461.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test_df.insert(location, f\"NMF_{i}\", nmf_features_test[:,i].tolist())\n"
     ]
    }
   ],
   "source": [
    "for i in range(nmf_features.shape[1]):\n",
    "    location= X_train_df.shape[1]\n",
    "    X_train_df.insert(location, f\"NMF_{i}\", nmf_features[:,i].tolist())\n",
    "\n",
    "for i in range(nmf_features_test.shape[1]):\n",
    "    location= X_test_df.shape[1]\n",
    "    X_test_df.insert(location, f\"NMF_{i}\", nmf_features_test[:,i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e5633940-ecf6-4619-be56-918ff352c5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5355, 20)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "697d4701-55f9-4bc0-8669-9e6ed82d15f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "43fc63a2-b2bf-4094-ae18-5f198faa9f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21419, 119)\n",
      "(5355, 119)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_df.shape)\n",
    "print(X_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6835e915-ae42-40bd-a1a8-94354fa72b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nostalgia'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2447851b-a151-477e-8079-a0060cd94943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>index</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>...</th>\n",
       "      <th>NMF_90</th>\n",
       "      <th>NMF_91</th>\n",
       "      <th>NMF_92</th>\n",
       "      <th>NMF_93</th>\n",
       "      <th>NMF_94</th>\n",
       "      <th>NMF_95</th>\n",
       "      <th>NMF_96</th>\n",
       "      <th>NMF_97</th>\n",
       "      <th>NMF_98</th>\n",
       "      <th>NMF_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From Potter's Field</td>\n",
       "      <td>The sixth book in the Kay Scarpetta series, fr...</td>\n",
       "      <td>['Patricia Cornwell']</td>\n",
       "      <td>http://books.google.com/books/content?id=prefg...</td>\n",
       "      <td>http://books.google.nl/books?id=prefgSxnGOwC&amp;p...</td>\n",
       "      <td>Hachette UK</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>115</td>\n",
       "      <td>157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00958</td>\n",
       "      <td>0.018879</td>\n",
       "      <td>0.000346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Riverworld and Other Stories</td>\n",
       "      <td>Three stories of a world shared by resurrected...</td>\n",
       "      <td>['Philip José Farmer']</td>\n",
       "      <td>http://books.google.com/books/content?id=TP4oD...</td>\n",
       "      <td>http://books.google.nl/books?id=TP4oDwAAQBAJ&amp;p...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>209</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.019001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.013162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title  \\\n",
       "6            From Potter's Field   \n",
       "17  Riverworld and Other Stories   \n",
       "\n",
       "                                          description                 authors  \\\n",
       "6   The sixth book in the Kay Scarpetta series, fr...   ['Patricia Cornwell']   \n",
       "17  Three stories of a world shared by resurrected...  ['Philip José Farmer']   \n",
       "\n",
       "                                                image  \\\n",
       "6   http://books.google.com/books/content?id=prefg...   \n",
       "17  http://books.google.com/books/content?id=TP4oD...   \n",
       "\n",
       "                                          previewLink        publisher  \\\n",
       "6   http://books.google.nl/books?id=prefgSxnGOwC&p...      Hachette UK   \n",
       "17  http://books.google.nl/books?id=TP4oDwAAQBAJ&p...  Open Road Media   \n",
       "\n",
       "                                             infoLink   categories  index  \\\n",
       "6   https://play.google.com/store/books/details?id...  ['fiction']    115   \n",
       "17  https://play.google.com/store/books/details?id...  ['fiction']    209   \n",
       "\n",
       "    reviews number  ...    NMF_90    NMF_91 NMF_92 NMF_93    NMF_94    NMF_95  \\\n",
       "6              157  ...  0.000625  0.000343    0.0    0.0  0.002655  0.003043   \n",
       "17               7  ...  0.000978  0.019001    0.0    0.0  0.000000  0.021188   \n",
       "\n",
       "    NMF_96   NMF_97    NMF_98    NMF_99  \n",
       "6      0.0  0.00958  0.018879  0.000346  \n",
       "17     0.0  0.00000  0.000852  0.013162  \n",
       "\n",
       "[2 rows x 119 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a1961323-6851-40bb-a56d-686c73362e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.to_csv(\n",
    "path_or_buf = \"X_train_NMF_topics_100.csv\",\n",
    "index = False\n",
    ")\n",
    "\n",
    "X_test_df.to_csv(\n",
    "path_or_buf = \"X_test_NMF_topics_100.csv\",\n",
    "index = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae03eac-659e-4d96-93dc-bda70d606546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
