{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up folders\n",
    "from EDA_functions import folders_set_up\n",
    "import os\n",
    "\n",
    "# Work with datarames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charts\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# X, Y preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# Neural Network\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Evaluate models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Time\n",
    "import time\n",
    "\n",
    "#from scipy.sparse import spmatrixc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM\n",
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders\n",
    "Run the code below if you have the following structure:\n",
    "- Group-project: GitHub folder\n",
    "- 01 Input\n",
    "- 02 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_folder, input_folder, output_folder = folders_set_up.generate_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'description', 'authors', 'image', 'previewLink',\n",
       "       'publisher', 'infoLink', 'categories', 'reviews number',\n",
       "       'average rating', 'median rating', 'min review date', 'max review date',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Title-level dataset with embeddings\n",
    "title_embeddings_df = pd.read_pickle(\n",
    "    os.path.join(output_folder, 'English_fiction_pre_PCA_3_with_av_pool_embeddings')\n",
    ")\n",
    "\n",
    "title_embeddings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                     int64\n",
       "Title                    object\n",
       "description              object\n",
       "authors                  object\n",
       "image                    object\n",
       "previewLink              object\n",
       "publisher                object\n",
       "infoLink                 object\n",
       "categories               object\n",
       "reviews number            int64\n",
       "average rating          float64\n",
       "median rating           float64\n",
       "min review date          object\n",
       "max review date          object\n",
       "weighted rating         float64\n",
       "date                     object\n",
       "year                    float64\n",
       "description_language     object\n",
       "Embedding                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_columns = ['min review date', 'max review date', 'date']\n",
    "\n",
    "for date in dates_columns:\n",
    "    # get date from strings with time\n",
    "    title_embeddings_df[date] = title_embeddings_df[date].str.split().str[0]\n",
    "    # convert in datetime\n",
    "    title_embeddings_df[date] = pd.to_datetime(title_embeddings_df[date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min review date    0\n",
       "max review date    0\n",
       "date               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df[dates_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8228249664151188\n",
      "4.886083503427672\n"
     ]
    }
   ],
   "source": [
    "# what is the max and minimu of the ratings?\n",
    "print(title_embeddings_df['weighted rating'].min())\n",
    "print(title_embeddings_df['weighted rating'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we work on a subset of data for now to make the ML run faster\n",
    "#title_embeddings_df = title_embeddings_df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image embeddings\n",
    "These need may need to be transformed in from arrays to columns if the model we use is not NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>median rating</th>\n",
       "      <th>min review date</th>\n",
       "      <th>max review date</th>\n",
       "      <th>weighted rating</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>description_language</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>index_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-02-14</td>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>3.938400</td>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.5179044, -0.7533603, -1.1291503, -0.4418345...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>4.306145</td>\n",
       "      <td>2001-03-06</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.706188, -0.4773652, -0.17887038, 0.07989502...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-22</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>4.256189</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.294651, -0.24902871, -0.6188333, -0.7722471...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-04-16</td>\n",
       "      <td>2000-05-14</td>\n",
       "      <td>4.336313</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.37794992, -0.6178984, -0.81393754, -0.66795...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-08-07</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>4.701517</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.34032565, -2.1706967, -0.21470371, -0.10447...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26769</th>\n",
       "      <td>212361</td>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>4.137453</td>\n",
       "      <td>2009-03-17</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[1.1648176, 0.56768346, -0.22511423, -0.185316...</td>\n",
       "      <td>212361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26770</th>\n",
       "      <td>212365</td>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-05-17</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>4.466716</td>\n",
       "      <td>1998-01-27</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.023786038, -1.9050528, -0.38564998, 0.14921...</td>\n",
       "      <td>212365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26771</th>\n",
       "      <td>212394</td>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>4.260690</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.2700834, -0.11750376, -2.0253444, -1.039558...</td>\n",
       "      <td>212394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26772</th>\n",
       "      <td>212399</td>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-07-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>4.504800</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.6904726, -0.96442795, 0.093034565, -1.69420...</td>\n",
       "      <td>212399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773</th>\n",
       "      <td>212402</td>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2002-11-11</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>3.989408</td>\n",
       "      <td>2003-08-12</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.47349918, -0.8046489, -0.88566315, -0.04958...</td>\n",
       "      <td>212402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26774 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                              Title  \\\n",
       "0           3                      Whispers of the Wicked Saints   \n",
       "1          24           The Forbidden Stories of Marta Veneranda   \n",
       "2          42                            Tess and the Highlander   \n",
       "3          49  Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "4          73                 Night World: Daughters Of Darkness   \n",
       "...       ...                                                ...   \n",
       "26769  212361                                       Calder Pride   \n",
       "26770  212365                                      The Road Back   \n",
       "26771  212394                                       Final things   \n",
       "26772  212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "26773  212402                                  The Autograph Man   \n",
       "\n",
       "                                             description  \\\n",
       "0      Julia Thomas finds her life spinning out of co...   \n",
       "1      Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "2      In 1543, on a windswept isle off of Scotland, ...   \n",
       "3      Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "4      \"There’s something strange about the new girls...   \n",
       "...                                                  ...   \n",
       "26769  The Long-Awaited Addition to the Beloved Calde...   \n",
       "26770  The sequel to the masterpiece All Quiet on the...   \n",
       "26771  Grace's father believes in science and builds ...   \n",
       "26772  During a school trip to Ellis Island, Dominick...   \n",
       "26773  Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                        authors  \\\n",
       "0           ['Veronica Haddon']   \n",
       "1       ['Sonia Rivera-Valdes']   \n",
       "2            ['May Mcgoldrick']   \n",
       "3        ['Elizabeth Sinclair']   \n",
       "4                ['L.J. Smith']   \n",
       "...                         ...   \n",
       "26769          ['Janet Dailey']   \n",
       "26770  ['Erich Maria Remarque']   \n",
       "26771          ['Jenny Offill']   \n",
       "26772       ['Elvira Woodruff']   \n",
       "26773           ['Zadie Smith']   \n",
       "\n",
       "                                                   image  \\\n",
       "0      http://books.google.com/books/content?id=aRSIg...   \n",
       "1      http://books.google.com/books/content?id=A7aYb...   \n",
       "2      http://books.google.com/books/content?id=VmCRS...   \n",
       "3      http://books.google.com/books/content?id=Z6uzJ...   \n",
       "4      http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                  ...   \n",
       "26769  http://books.google.com/books/content?id=nlsgd...   \n",
       "26770  http://books.google.com/books/content?id=obZdA...   \n",
       "26771  http://books.google.com/books/content?id=UbSFB...   \n",
       "26772  http://books.google.com/books/content?id=J7M-N...   \n",
       "26773  http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                             previewLink  \\\n",
       "0      http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "1      http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "2      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "3      http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "4      http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                  ...   \n",
       "26769  http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "26770  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "26771  http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "26772  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "26773  http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                               publisher  \\\n",
       "0                                              iUniverse   \n",
       "1                                    Seven Stories Press   \n",
       "2                                         Harper Collins   \n",
       "3      Harlequin Treasury-Harlequin American Romance 90s   \n",
       "4                                     Simon and Schuster   \n",
       "...                                                  ...   \n",
       "26769                                     Harper Collins   \n",
       "26770                      Random House Trade Paperbacks   \n",
       "26771                                            Vintage   \n",
       "26772                              Scholastic Paperbacks   \n",
       "26773                                            Vintage   \n",
       "\n",
       "                                                infoLink  \\\n",
       "0      http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "1      http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "2      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "3      http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "4      http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                  ...   \n",
       "26769  https://play.google.com/store/books/details?id...   \n",
       "26770  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "26771  https://play.google.com/store/books/details?id...   \n",
       "26772  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "26773  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                 categories  reviews number  average rating  median rating  \\\n",
       "0               ['fiction']              32        3.718750            5.0   \n",
       "1               ['fiction']               1        5.000000            5.0   \n",
       "2      ['juvenile fiction']              17        4.235294            5.0   \n",
       "3               ['fiction']               2        5.000000            5.0   \n",
       "4      ['juvenile fiction']             134        4.768657            5.0   \n",
       "...                     ...             ...             ...            ...   \n",
       "26769           ['fiction']              28        4.035714            5.0   \n",
       "26770           ['fiction']              17        4.705882            5.0   \n",
       "26771           ['fiction']               1        4.000000            4.0   \n",
       "26772  ['juvenile fiction']              28        4.678571            5.0   \n",
       "26773           ['fiction']               4        2.500000            2.5   \n",
       "\n",
       "      min review date max review date  weighted rating       date    year  \\\n",
       "0          2005-02-14      2006-07-01         3.938400 2005-02-01  2005.0   \n",
       "1          2005-01-24      2005-01-24         4.306145 2001-03-06  2001.0   \n",
       "2          2002-10-22      2011-05-25         4.256189 2002-11-01  2002.0   \n",
       "3          1998-04-16      2000-05-14         4.336313 1997-01-01  1997.0   \n",
       "4          1996-08-07      2012-09-18         4.701517 2016-12-06  2016.0   \n",
       "...               ...             ...              ...        ...     ...   \n",
       "26769      1999-09-30      2012-04-04         4.137453 2009-03-17  2009.0   \n",
       "26770      1997-05-17      2012-01-23         4.466716 1998-01-27  1998.0   \n",
       "26771      2012-01-26      2012-01-26         4.260690 2015-03-17  2015.0   \n",
       "26772      1998-07-10      2011-12-31         4.504800 2000-06-01  2000.0   \n",
       "26773      2002-11-11      2005-11-14         3.989408 2003-08-12  2003.0   \n",
       "\n",
       "      description_language                                          Embedding  \\\n",
       "0                  English  [0.5179044, -0.7533603, -1.1291503, -0.4418345...   \n",
       "1                  English  [0.706188, -0.4773652, -0.17887038, 0.07989502...   \n",
       "2                  English  [2.294651, -0.24902871, -0.6188333, -0.7722471...   \n",
       "3                  English  [0.37794992, -0.6178984, -0.81393754, -0.66795...   \n",
       "4                  English  [0.34032565, -2.1706967, -0.21470371, -0.10447...   \n",
       "...                    ...                                                ...   \n",
       "26769              English  [1.1648176, 0.56768346, -0.22511423, -0.185316...   \n",
       "26770              English  [0.023786038, -1.9050528, -0.38564998, 0.14921...   \n",
       "26771              English  [2.2700834, -0.11750376, -2.0253444, -1.039558...   \n",
       "26772              English  [2.6904726, -0.96442795, 0.093034565, -1.69420...   \n",
       "26773              English  [0.47349918, -0.8046489, -0.88566315, -0.04958...   \n",
       "\n",
       "       index_key  \n",
       "0              3  \n",
       "1             24  \n",
       "2             42  \n",
       "3             49  \n",
       "4             73  \n",
       "...          ...  \n",
       "26769     212361  \n",
       "26770     212365  \n",
       "26771     212394  \n",
       "26772     212399  \n",
       "26773     212402  \n",
       "\n",
       "[26774 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df['index_key'] = title_embeddings_df['index']\n",
    "title_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings_df = title_embeddings_df.set_index('index_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>median rating</th>\n",
       "      <th>min review date</th>\n",
       "      <th>max review date</th>\n",
       "      <th>weighted rating</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>description_language</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-02-14</td>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>3.938400</td>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.5179044, -0.7533603, -1.1291503, -0.4418345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>4.306145</td>\n",
       "      <td>2001-03-06</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.706188, -0.4773652, -0.17887038, 0.07989502...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-22</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>4.256189</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.294651, -0.24902871, -0.6188333, -0.7722471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-04-16</td>\n",
       "      <td>2000-05-14</td>\n",
       "      <td>4.336313</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.37794992, -0.6178984, -0.81393754, -0.66795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-08-07</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>4.701517</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.34032565, -2.1706967, -0.21470371, -0.10447...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>212361</td>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>4.137453</td>\n",
       "      <td>2009-03-17</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[1.1648176, 0.56768346, -0.22511423, -0.185316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>212365</td>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-05-17</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>4.466716</td>\n",
       "      <td>1998-01-27</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.023786038, -1.9050528, -0.38564998, 0.14921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>212394</td>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>4.260690</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.2700834, -0.11750376, -2.0253444, -1.039558...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>212399</td>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-07-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>4.504800</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.6904726, -0.96442795, 0.093034565, -1.69420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>212402</td>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2002-11-11</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>3.989408</td>\n",
       "      <td>2003-08-12</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.47349918, -0.8046489, -0.88566315, -0.04958...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26774 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index                                              Title  \\\n",
       "index_key                                                              \n",
       "3               3                      Whispers of the Wicked Saints   \n",
       "24             24           The Forbidden Stories of Marta Veneranda   \n",
       "42             42                            Tess and the Highlander   \n",
       "49             49  Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "73             73                 Night World: Daughters Of Darkness   \n",
       "...           ...                                                ...   \n",
       "212361     212361                                       Calder Pride   \n",
       "212365     212365                                      The Road Back   \n",
       "212394     212394                                       Final things   \n",
       "212399     212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "212402     212402                                  The Autograph Man   \n",
       "\n",
       "                                                 description  \\\n",
       "index_key                                                      \n",
       "3          Julia Thomas finds her life spinning out of co...   \n",
       "24         Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "42         In 1543, on a windswept isle off of Scotland, ...   \n",
       "49         Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "73         \"There’s something strange about the new girls...   \n",
       "...                                                      ...   \n",
       "212361     The Long-Awaited Addition to the Beloved Calde...   \n",
       "212365     The sequel to the masterpiece All Quiet on the...   \n",
       "212394     Grace's father believes in science and builds ...   \n",
       "212399     During a school trip to Ellis Island, Dominick...   \n",
       "212402     Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                            authors  \\\n",
       "index_key                             \n",
       "3               ['Veronica Haddon']   \n",
       "24          ['Sonia Rivera-Valdes']   \n",
       "42               ['May Mcgoldrick']   \n",
       "49           ['Elizabeth Sinclair']   \n",
       "73                   ['L.J. Smith']   \n",
       "...                             ...   \n",
       "212361             ['Janet Dailey']   \n",
       "212365     ['Erich Maria Remarque']   \n",
       "212394             ['Jenny Offill']   \n",
       "212399          ['Elvira Woodruff']   \n",
       "212402              ['Zadie Smith']   \n",
       "\n",
       "                                                       image  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.com/books/content?id=aRSIg...   \n",
       "24         http://books.google.com/books/content?id=A7aYb...   \n",
       "42         http://books.google.com/books/content?id=VmCRS...   \n",
       "49         http://books.google.com/books/content?id=Z6uzJ...   \n",
       "73         http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                      ...   \n",
       "212361     http://books.google.com/books/content?id=nlsgd...   \n",
       "212365     http://books.google.com/books/content?id=obZdA...   \n",
       "212394     http://books.google.com/books/content?id=UbSFB...   \n",
       "212399     http://books.google.com/books/content?id=J7M-N...   \n",
       "212402     http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                                 previewLink  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24         http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "42         http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49         http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "73         http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                      ...   \n",
       "212361     http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "212365     http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394     http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "212399     http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402     http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                                   publisher  \\\n",
       "index_key                                                      \n",
       "3                                                  iUniverse   \n",
       "24                                       Seven Stories Press   \n",
       "42                                            Harper Collins   \n",
       "49         Harlequin Treasury-Harlequin American Romance 90s   \n",
       "73                                        Simon and Schuster   \n",
       "...                                                      ...   \n",
       "212361                                        Harper Collins   \n",
       "212365                         Random House Trade Paperbacks   \n",
       "212394                                               Vintage   \n",
       "212399                                 Scholastic Paperbacks   \n",
       "212402                                               Vintage   \n",
       "\n",
       "                                                    infoLink  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24         http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "42         http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49         http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "73         http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                      ...   \n",
       "212361     https://play.google.com/store/books/details?id...   \n",
       "212365     http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394     https://play.google.com/store/books/details?id...   \n",
       "212399     http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402     https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                     categories  reviews number  average rating  \\\n",
       "index_key                                                         \n",
       "3                   ['fiction']              32        3.718750   \n",
       "24                  ['fiction']               1        5.000000   \n",
       "42         ['juvenile fiction']              17        4.235294   \n",
       "49                  ['fiction']               2        5.000000   \n",
       "73         ['juvenile fiction']             134        4.768657   \n",
       "...                         ...             ...             ...   \n",
       "212361              ['fiction']              28        4.035714   \n",
       "212365              ['fiction']              17        4.705882   \n",
       "212394              ['fiction']               1        4.000000   \n",
       "212399     ['juvenile fiction']              28        4.678571   \n",
       "212402              ['fiction']               4        2.500000   \n",
       "\n",
       "           median rating min review date max review date  weighted rating  \\\n",
       "index_key                                                                   \n",
       "3                    5.0      2005-02-14      2006-07-01         3.938400   \n",
       "24                   5.0      2005-01-24      2005-01-24         4.306145   \n",
       "42                   5.0      2002-10-22      2011-05-25         4.256189   \n",
       "49                   5.0      1998-04-16      2000-05-14         4.336313   \n",
       "73                   5.0      1996-08-07      2012-09-18         4.701517   \n",
       "...                  ...             ...             ...              ...   \n",
       "212361               5.0      1999-09-30      2012-04-04         4.137453   \n",
       "212365               5.0      1997-05-17      2012-01-23         4.466716   \n",
       "212394               4.0      2012-01-26      2012-01-26         4.260690   \n",
       "212399               5.0      1998-07-10      2011-12-31         4.504800   \n",
       "212402               2.5      2002-11-11      2005-11-14         3.989408   \n",
       "\n",
       "                date    year description_language  \\\n",
       "index_key                                           \n",
       "3         2005-02-01  2005.0              English   \n",
       "24        2001-03-06  2001.0              English   \n",
       "42        2002-11-01  2002.0              English   \n",
       "49        1997-01-01  1997.0              English   \n",
       "73        2016-12-06  2016.0              English   \n",
       "...              ...     ...                  ...   \n",
       "212361    2009-03-17  2009.0              English   \n",
       "212365    1998-01-27  1998.0              English   \n",
       "212394    2015-03-17  2015.0              English   \n",
       "212399    2000-06-01  2000.0              English   \n",
       "212402    2003-08-12  2003.0              English   \n",
       "\n",
       "                                                   Embedding  \n",
       "index_key                                                     \n",
       "3          [0.5179044, -0.7533603, -1.1291503, -0.4418345...  \n",
       "24         [0.706188, -0.4773652, -0.17887038, 0.07989502...  \n",
       "42         [2.294651, -0.24902871, -0.6188333, -0.7722471...  \n",
       "49         [0.37794992, -0.6178984, -0.81393754, -0.66795...  \n",
       "73         [0.34032565, -2.1706967, -0.21470371, -0.10447...  \n",
       "...                                                      ...  \n",
       "212361     [1.1648176, 0.56768346, -0.22511423, -0.185316...  \n",
       "212365     [0.023786038, -1.9050528, -0.38564998, 0.14921...  \n",
       "212394     [2.2700834, -0.11750376, -2.0253444, -1.039558...  \n",
       "212399     [2.6904726, -0.96442795, 0.093034565, -1.69420...  \n",
       "212402     [0.47349918, -0.8046489, -0.88566315, -0.04958...  \n",
       "\n",
       "[26774 rows x 19 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "Most of the cleaning is done in '02 Consolidate books dataset':\n",
    "- English description\n",
    "- category containing the word 'fiction'\n",
    "- non-missing date\n",
    "- non-missing author\n",
    "- non-missing publisher\n",
    "- non-missing cover image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'description', 'authors', 'image', 'previewLink',\n",
       "       'publisher', 'infoLink', 'categories', 'reviews number',\n",
       "       'average rating', 'median rating', 'min review date', 'max review date',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y including all X features and all all teh possible target variables\n",
    "# NOTE: we will have to add the description PCA in X_features\n",
    "X_columns = ['year', 'Embedding', 'index', 'Title']\n",
    "\n",
    "X = title_embeddings_df[X_columns]\n",
    "y = title_embeddings_df[['average rating', 'weighted rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'Embedding', 'index', 'Title'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['average rating', 'weighted rating'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "\n",
    "# Need to create train test split for different combinations of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size= 0.2, \n",
    "    random_state= 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store indices of train test split for the NLP of description\n",
    "train_indices = X_train[['Title', 'index']]\n",
    "test_indices = X_test[['Title', 'index']]\n",
    "\n",
    "train_indices.to_csv(\n",
    "    os.path.join(output_folder, 'train_indices.csv')\n",
    ")\n",
    "\n",
    "\n",
    "test_indices.to_csv(\n",
    "    os.path.join(output_folder, 'test_indices.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y train with average rating\n",
    "y_avg_r_train = y_train['average rating']\n",
    "y_avg_r_test = y_test['average rating']\n",
    "\n",
    "# Y train with weighted rating\n",
    "y_wr_train = y_train['weighted rating']\n",
    "y_wr_test = y_test['weighted rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings X\n",
    "Transform the arrays into columns so that they can feed into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images_train = X_train['Embedding'].apply(pd.Series)\n",
    "X_images_test = X_test['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "X_images_train = X_images_train.add_prefix('image_')\n",
    "X_images_test = X_images_test.add_prefix('image_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_0</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "      <th>image_6</th>\n",
       "      <th>image_7</th>\n",
       "      <th>image_8</th>\n",
       "      <th>image_9</th>\n",
       "      <th>...</th>\n",
       "      <th>image_246</th>\n",
       "      <th>image_247</th>\n",
       "      <th>image_248</th>\n",
       "      <th>image_249</th>\n",
       "      <th>image_250</th>\n",
       "      <th>image_251</th>\n",
       "      <th>image_252</th>\n",
       "      <th>image_253</th>\n",
       "      <th>image_254</th>\n",
       "      <th>image_255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19757</th>\n",
       "      <td>1.310540</td>\n",
       "      <td>0.253238</td>\n",
       "      <td>-0.208362</td>\n",
       "      <td>-0.584103</td>\n",
       "      <td>-0.794551</td>\n",
       "      <td>-1.803357</td>\n",
       "      <td>-2.700018</td>\n",
       "      <td>-0.848385</td>\n",
       "      <td>0.949902</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475167</td>\n",
       "      <td>-0.116055</td>\n",
       "      <td>0.735540</td>\n",
       "      <td>-2.354414</td>\n",
       "      <td>0.956939</td>\n",
       "      <td>-1.065875</td>\n",
       "      <td>-0.428229</td>\n",
       "      <td>-0.285047</td>\n",
       "      <td>1.098027</td>\n",
       "      <td>-1.029737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111405</th>\n",
       "      <td>1.078856</td>\n",
       "      <td>-0.691140</td>\n",
       "      <td>-0.908770</td>\n",
       "      <td>-0.527087</td>\n",
       "      <td>-1.044688</td>\n",
       "      <td>-0.904328</td>\n",
       "      <td>0.210946</td>\n",
       "      <td>-1.238919</td>\n",
       "      <td>2.290273</td>\n",
       "      <td>-0.155667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833435</td>\n",
       "      <td>-1.053398</td>\n",
       "      <td>-2.031961</td>\n",
       "      <td>-2.716383</td>\n",
       "      <td>0.817275</td>\n",
       "      <td>-0.434370</td>\n",
       "      <td>-1.456125</td>\n",
       "      <td>0.112614</td>\n",
       "      <td>0.199815</td>\n",
       "      <td>-2.946273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12269</th>\n",
       "      <td>0.350093</td>\n",
       "      <td>-0.135313</td>\n",
       "      <td>0.512653</td>\n",
       "      <td>0.466054</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>0.014968</td>\n",
       "      <td>-2.224684</td>\n",
       "      <td>-0.740723</td>\n",
       "      <td>0.951188</td>\n",
       "      <td>0.978717</td>\n",
       "      <td>...</td>\n",
       "      <td>1.401945</td>\n",
       "      <td>-0.270900</td>\n",
       "      <td>-1.967142</td>\n",
       "      <td>-0.814089</td>\n",
       "      <td>0.170715</td>\n",
       "      <td>0.335253</td>\n",
       "      <td>-0.030882</td>\n",
       "      <td>-0.557203</td>\n",
       "      <td>0.037506</td>\n",
       "      <td>-1.951925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186303</th>\n",
       "      <td>4.560193</td>\n",
       "      <td>0.550376</td>\n",
       "      <td>-0.331547</td>\n",
       "      <td>-2.215812</td>\n",
       "      <td>0.023501</td>\n",
       "      <td>-0.370375</td>\n",
       "      <td>-0.838666</td>\n",
       "      <td>-0.905349</td>\n",
       "      <td>2.655245</td>\n",
       "      <td>0.461321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073267</td>\n",
       "      <td>0.354717</td>\n",
       "      <td>-1.745163</td>\n",
       "      <td>-2.610591</td>\n",
       "      <td>-0.239567</td>\n",
       "      <td>-1.714204</td>\n",
       "      <td>-0.914772</td>\n",
       "      <td>-0.354720</td>\n",
       "      <td>0.098914</td>\n",
       "      <td>-3.272672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134045</th>\n",
       "      <td>2.236698</td>\n",
       "      <td>-0.498289</td>\n",
       "      <td>-1.946595</td>\n",
       "      <td>-0.618339</td>\n",
       "      <td>-2.150359</td>\n",
       "      <td>1.257699</td>\n",
       "      <td>-1.547900</td>\n",
       "      <td>-1.492419</td>\n",
       "      <td>2.133226</td>\n",
       "      <td>1.513557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192120</td>\n",
       "      <td>-0.731990</td>\n",
       "      <td>-3.023546</td>\n",
       "      <td>-2.410438</td>\n",
       "      <td>-0.701821</td>\n",
       "      <td>-0.403381</td>\n",
       "      <td>0.522945</td>\n",
       "      <td>0.256027</td>\n",
       "      <td>1.826598</td>\n",
       "      <td>-3.091393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167206</th>\n",
       "      <td>0.998582</td>\n",
       "      <td>-1.018484</td>\n",
       "      <td>-0.603367</td>\n",
       "      <td>-0.467136</td>\n",
       "      <td>0.090738</td>\n",
       "      <td>-1.786356</td>\n",
       "      <td>-1.359131</td>\n",
       "      <td>-0.057187</td>\n",
       "      <td>-0.699324</td>\n",
       "      <td>-0.277699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967072</td>\n",
       "      <td>-0.407894</td>\n",
       "      <td>0.675619</td>\n",
       "      <td>-2.690143</td>\n",
       "      <td>0.811195</td>\n",
       "      <td>0.162421</td>\n",
       "      <td>-0.319673</td>\n",
       "      <td>-0.655105</td>\n",
       "      <td>1.837679</td>\n",
       "      <td>-2.292945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40262</th>\n",
       "      <td>0.993674</td>\n",
       "      <td>-0.236895</td>\n",
       "      <td>-1.161556</td>\n",
       "      <td>-0.831213</td>\n",
       "      <td>-0.535987</td>\n",
       "      <td>1.596271</td>\n",
       "      <td>-1.988977</td>\n",
       "      <td>-0.020737</td>\n",
       "      <td>0.298276</td>\n",
       "      <td>0.706597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235213</td>\n",
       "      <td>-0.146208</td>\n",
       "      <td>-1.484007</td>\n",
       "      <td>-1.698481</td>\n",
       "      <td>-0.935188</td>\n",
       "      <td>-0.480799</td>\n",
       "      <td>0.807755</td>\n",
       "      <td>0.346888</td>\n",
       "      <td>1.523457</td>\n",
       "      <td>-1.739759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>0.622462</td>\n",
       "      <td>-0.793748</td>\n",
       "      <td>-0.837395</td>\n",
       "      <td>-0.745482</td>\n",
       "      <td>-0.137351</td>\n",
       "      <td>0.551706</td>\n",
       "      <td>0.313828</td>\n",
       "      <td>-0.853430</td>\n",
       "      <td>0.280148</td>\n",
       "      <td>-0.483826</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114822</td>\n",
       "      <td>-0.845614</td>\n",
       "      <td>-1.312257</td>\n",
       "      <td>-0.366303</td>\n",
       "      <td>-0.078197</td>\n",
       "      <td>1.005683</td>\n",
       "      <td>0.627290</td>\n",
       "      <td>-1.296975</td>\n",
       "      <td>1.453560</td>\n",
       "      <td>-0.719741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124694</th>\n",
       "      <td>1.551901</td>\n",
       "      <td>0.323544</td>\n",
       "      <td>-1.019795</td>\n",
       "      <td>-0.265791</td>\n",
       "      <td>-0.342785</td>\n",
       "      <td>0.346762</td>\n",
       "      <td>0.363122</td>\n",
       "      <td>-0.480910</td>\n",
       "      <td>0.268287</td>\n",
       "      <td>-0.495149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396447</td>\n",
       "      <td>0.472859</td>\n",
       "      <td>-1.687484</td>\n",
       "      <td>-2.504541</td>\n",
       "      <td>-0.655285</td>\n",
       "      <td>0.751038</td>\n",
       "      <td>0.302664</td>\n",
       "      <td>-0.154664</td>\n",
       "      <td>1.174814</td>\n",
       "      <td>-2.710322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185394</th>\n",
       "      <td>1.494337</td>\n",
       "      <td>-0.428976</td>\n",
       "      <td>-0.336699</td>\n",
       "      <td>-0.481797</td>\n",
       "      <td>-1.894806</td>\n",
       "      <td>0.845233</td>\n",
       "      <td>-1.619887</td>\n",
       "      <td>-0.521737</td>\n",
       "      <td>-0.534672</td>\n",
       "      <td>-2.321126</td>\n",
       "      <td>...</td>\n",
       "      <td>1.549637</td>\n",
       "      <td>-0.056938</td>\n",
       "      <td>-0.942204</td>\n",
       "      <td>-1.869788</td>\n",
       "      <td>0.764314</td>\n",
       "      <td>-1.347982</td>\n",
       "      <td>-0.955746</td>\n",
       "      <td>-0.672004</td>\n",
       "      <td>1.069264</td>\n",
       "      <td>-1.114935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_0   image_1   image_2   image_3   image_4   image_5  \\\n",
       "index_key                                                               \n",
       "19757      1.310540  0.253238 -0.208362 -0.584103 -0.794551 -1.803357   \n",
       "111405     1.078856 -0.691140 -0.908770 -0.527087 -1.044688 -0.904328   \n",
       "12269      0.350093 -0.135313  0.512653  0.466054  0.326599  0.014968   \n",
       "186303     4.560193  0.550376 -0.331547 -2.215812  0.023501 -0.370375   \n",
       "134045     2.236698 -0.498289 -1.946595 -0.618339 -2.150359  1.257699   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "167206     0.998582 -1.018484 -0.603367 -0.467136  0.090738 -1.786356   \n",
       "40262      0.993674 -0.236895 -1.161556 -0.831213 -0.535987  1.596271   \n",
       "7002       0.622462 -0.793748 -0.837395 -0.745482 -0.137351  0.551706   \n",
       "124694     1.551901  0.323544 -1.019795 -0.265791 -0.342785  0.346762   \n",
       "185394     1.494337 -0.428976 -0.336699 -0.481797 -1.894806  0.845233   \n",
       "\n",
       "            image_6   image_7   image_8   image_9  ...  image_246  image_247  \\\n",
       "index_key                                          ...                         \n",
       "19757     -2.700018 -0.848385  0.949902  0.030431  ...  -0.475167  -0.116055   \n",
       "111405     0.210946 -1.238919  2.290273 -0.155667  ...   0.833435  -1.053398   \n",
       "12269     -2.224684 -0.740723  0.951188  0.978717  ...   1.401945  -0.270900   \n",
       "186303    -0.838666 -0.905349  2.655245  0.461321  ...   1.073267   0.354717   \n",
       "134045    -1.547900 -1.492419  2.133226  1.513557  ...   0.192120  -0.731990   \n",
       "...             ...       ...       ...       ...  ...        ...        ...   \n",
       "167206    -1.359131 -0.057187 -0.699324 -0.277699  ...   0.967072  -0.407894   \n",
       "40262     -1.988977 -0.020737  0.298276  0.706597  ...   1.235213  -0.146208   \n",
       "7002       0.313828 -0.853430  0.280148 -0.483826  ...   1.114822  -0.845614   \n",
       "124694     0.363122 -0.480910  0.268287 -0.495149  ...   1.396447   0.472859   \n",
       "185394    -1.619887 -0.521737 -0.534672 -2.321126  ...   1.549637  -0.056938   \n",
       "\n",
       "           image_248  image_249  image_250  image_251  image_252  image_253  \\\n",
       "index_key                                                                     \n",
       "19757       0.735540  -2.354414   0.956939  -1.065875  -0.428229  -0.285047   \n",
       "111405     -2.031961  -2.716383   0.817275  -0.434370  -1.456125   0.112614   \n",
       "12269      -1.967142  -0.814089   0.170715   0.335253  -0.030882  -0.557203   \n",
       "186303     -1.745163  -2.610591  -0.239567  -1.714204  -0.914772  -0.354720   \n",
       "134045     -3.023546  -2.410438  -0.701821  -0.403381   0.522945   0.256027   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "167206      0.675619  -2.690143   0.811195   0.162421  -0.319673  -0.655105   \n",
       "40262      -1.484007  -1.698481  -0.935188  -0.480799   0.807755   0.346888   \n",
       "7002       -1.312257  -0.366303  -0.078197   1.005683   0.627290  -1.296975   \n",
       "124694     -1.687484  -2.504541  -0.655285   0.751038   0.302664  -0.154664   \n",
       "185394     -0.942204  -1.869788   0.764314  -1.347982  -0.955746  -0.672004   \n",
       "\n",
       "           image_254  image_255  \n",
       "index_key                        \n",
       "19757       1.098027  -1.029737  \n",
       "111405      0.199815  -2.946273  \n",
       "12269       0.037506  -1.951925  \n",
       "186303      0.098914  -3.272672  \n",
       "134045      1.826598  -3.091393  \n",
       "...              ...        ...  \n",
       "167206      1.837679  -2.292945  \n",
       "40262       1.523457  -1.739759  \n",
       "7002        1.453560  -0.719741  \n",
       "124694      1.174814  -2.710322  \n",
       "185394      1.069264  -1.114935  \n",
       "\n",
       "[21419 rows x 256 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description from NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description NLP test\n",
    "NMF_df_test = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_test_NMF_topics_100.csv')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'NMF_90', 'NMF_91', 'NMF_92', 'NMF_93', 'NMF_94', 'NMF_95', 'NMF_96',\n",
       "       'NMF_97', 'NMF_98', 'NMF_99'],\n",
       "      dtype='object', length=118)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set indices as in train test split\n",
    "NMF_df_test = NMF_df_test.set_index('index')\n",
    "\n",
    "NMF_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([   115,    209,    330,    333,    371,    381,    441,    490,    507,\n",
       "          549,\n",
       "       ...\n",
       "       211852, 211860, 211945, 211952, 211970, 212041, 212144, 212256, 212260,\n",
       "       212333],\n",
       "      dtype='int64', name='index', length=5355)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_df_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([   115,    209,    330,    333,    371,    381,    441,    490,    507,\n",
       "          549,\n",
       "       ...\n",
       "       211852, 211860, 211945, 211952, 211970, 212041, 212144, 212256, 212260,\n",
       "       212333],\n",
       "      dtype='int64', name='index_key', length=5355)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_images_test.index.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'NMF_90', 'NMF_91', 'NMF_92', 'NMF_93', 'NMF_94', 'NMF_95', 'NMF_96',\n",
       "       'NMF_97', 'NMF_98', 'NMF_99'],\n",
       "      dtype='object', length=118)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP train\n",
    "NMF_df_train = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_train_NMF_topics_100.csv')\n",
    ")\n",
    "\n",
    "NMF_df_train = NMF_df_train.set_index('index')\n",
    "\n",
    "NMF_df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'NMF_90', 'NMF_91', 'NMF_92', 'NMF_93', 'NMF_94', 'NMF_95', 'NMF_96',\n",
       "       'NMF_97', 'NMF_98', 'NMF_99'],\n",
       "      dtype='object', length=118)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to keep\n",
    "columns_to_keep = [col for col in NMF_df_train.columns if col.startswith('NMF_')]\n",
    "\n",
    "NMF_df_train = NMF_df_train[columns_to_keep]\n",
    "NMF_df_test = NMF_df_test[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description from tSDV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'tSVD2991', 'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996',\n",
       "       'tSVD2997', 'tSVD2998', 'tSVD2999', 'tSVD3000'],\n",
       "      dtype='object', length=3018)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP test\n",
    "NLP_df_test = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_test_tSVD_3000.csv')\n",
    ")\n",
    "\n",
    "# Set indices as in train test split\n",
    "NLP_df_test = NLP_df_test.set_index('index')\n",
    "\n",
    "NLP_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>From Potter's Field</td>\n",
       "      <td>The sixth book in the Kay Scarpetta series, fr...</td>\n",
       "      <td>['Patricia Cornwell']</td>\n",
       "      <td>http://books.google.com/books/content?id=prefg...</td>\n",
       "      <td>http://books.google.nl/books?id=prefgSxnGOwC&amp;p...</td>\n",
       "      <td>Hachette UK</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>157</td>\n",
       "      <td>3.783439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.011035</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>-0.009888</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.021092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Riverworld and Other Stories</td>\n",
       "      <td>Three stories of a world shared by resurrected...</td>\n",
       "      <td>['Philip José Farmer']</td>\n",
       "      <td>http://books.google.com/books/content?id=TP4oD...</td>\n",
       "      <td>http://books.google.nl/books?id=TP4oDwAAQBAJ&amp;p...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>7</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.007662</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>-0.007470</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>0.012055</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>0.011430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Kenny Doin' Just Fine</td>\n",
       "      <td>KENNY DOIN' JUST FINE Miriam Greenfield, a pro...</td>\n",
       "      <td>['Sadie Wernick Hurwitz']</td>\n",
       "      <td>http://books.google.com/books/content?id=D6Wgi...</td>\n",
       "      <td>http://books.google.nl/books?id=D6WgitXrr8sC&amp;p...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=D6WgitXrr8sC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>-0.004538</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>-0.015920</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>-0.003591</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>-0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Harry on the Rocks</td>\n",
       "      <td>Harry and his boat become stranded on an islan...</td>\n",
       "      <td>['Susan Meddaugh']</td>\n",
       "      <td>http://books.google.com/books/content?id=u5r79...</td>\n",
       "      <td>http://books.google.nl/books?id=u5r79DAUeIYC&amp;q...</td>\n",
       "      <td>Houghton Mifflin Harcourt</td>\n",
       "      <td>http://books.google.nl/books?id=u5r79DAUeIYC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.003397</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.011397</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>-0.004257</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>-0.002277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>The National Review Treasury of Classic Childr...</td>\n",
       "      <td>A collection of over forty stories, tales, poe...</td>\n",
       "      <td>['William F. Buckley, Jr.']</td>\n",
       "      <td>http://books.google.com/books/content?id=NZm7P...</td>\n",
       "      <td>http://books.google.nl/books?id=NZm7PAAACAAJ&amp;d...</td>\n",
       "      <td>Isi Books</td>\n",
       "      <td>http://books.google.nl/books?id=NZm7PAAACAAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002185</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>-0.000960</td>\n",
       "      <td>-0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212041</th>\n",
       "      <td>Man For Maggie Moore (Montana Matchmakers) (Ha...</td>\n",
       "      <td>You don't know what love is until you have los...</td>\n",
       "      <td>['Steven Labree']</td>\n",
       "      <td>http://books.google.com/books/content?id=NZpeJ...</td>\n",
       "      <td>http://books.google.com/books?id=NZpeJhtmGo8C&amp;...</td>\n",
       "      <td>Steven LaBree</td>\n",
       "      <td>http://books.google.com/books?id=NZpeJhtmGo8C&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>3</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>-0.010747</td>\n",
       "      <td>-0.012375</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>-0.006146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212144</th>\n",
       "      <td>Prancing Tiger</td>\n",
       "      <td>To clear the name of his ex-girlfriend's son, ...</td>\n",
       "      <td>['Philip Singerman']</td>\n",
       "      <td>http://books.google.com/books/content?id=68R7S...</td>\n",
       "      <td>http://books.google.com/books?id=68R7SppHYHcC&amp;...</td>\n",
       "      <td>William Morrow</td>\n",
       "      <td>http://books.google.com/books?id=68R7SppHYHcC&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011748</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>-0.006230</td>\n",
       "      <td>-0.008297</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>-0.002252</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>-0.001916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212256</th>\n",
       "      <td>Nude Men: A Novel</td>\n",
       "      <td>The internationally acclaimed debut of a novel...</td>\n",
       "      <td>['Amanda Filipacchi']</td>\n",
       "      <td>http://books.google.com/books/content?id=uM-1A...</td>\n",
       "      <td>http://books.google.com/books?id=uM-1AwAAQBAJ&amp;...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>23</td>\n",
       "      <td>3.739130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010690</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>-0.001923</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>-0.002761</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>0.018299</td>\n",
       "      <td>-0.007997</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.007875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212260</th>\n",
       "      <td>The Tale of Digby</td>\n",
       "      <td>In Digby, Willy Wink finds himself in the midd...</td>\n",
       "      <td>['Timothy Lee Bonnette, Jr.']</td>\n",
       "      <td>http://books.google.com/books/content?id=pcgBA...</td>\n",
       "      <td>http://books.google.com/books?id=pcgBAAAACAAJ&amp;...</td>\n",
       "      <td>Publishamerica Incorporated</td>\n",
       "      <td>http://books.google.com/books?id=pcgBAAAACAAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>0.008438</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>-0.006659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212333</th>\n",
       "      <td>Blueprint For Revenge</td>\n",
       "      <td>Johanna is devastated when her beloved grandmo...</td>\n",
       "      <td>['Martine Jardin']</td>\n",
       "      <td>http://books.google.com/books/content?id=I96XB...</td>\n",
       "      <td>http://books.google.com/books?id=I96XBQAAQBAJ&amp;...</td>\n",
       "      <td>Devine Destinies</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001040</td>\n",
       "      <td>-0.004460</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.008113</td>\n",
       "      <td>-0.002739</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>-0.008496</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5355 rows × 3018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "index                                                       \n",
       "115                                   From Potter's Field   \n",
       "209                          Riverworld and Other Stories   \n",
       "330                                 Kenny Doin' Just Fine   \n",
       "333                                    Harry on the Rocks   \n",
       "371     The National Review Treasury of Classic Childr...   \n",
       "...                                                   ...   \n",
       "212041  Man For Maggie Moore (Montana Matchmakers) (Ha...   \n",
       "212144                                     Prancing Tiger   \n",
       "212256                                  Nude Men: A Novel   \n",
       "212260                                  The Tale of Digby   \n",
       "212333                              Blueprint For Revenge   \n",
       "\n",
       "                                              description  \\\n",
       "index                                                       \n",
       "115     The sixth book in the Kay Scarpetta series, fr...   \n",
       "209     Three stories of a world shared by resurrected...   \n",
       "330     KENNY DOIN' JUST FINE Miriam Greenfield, a pro...   \n",
       "333     Harry and his boat become stranded on an islan...   \n",
       "371     A collection of over forty stories, tales, poe...   \n",
       "...                                                   ...   \n",
       "212041  You don't know what love is until you have los...   \n",
       "212144  To clear the name of his ex-girlfriend's son, ...   \n",
       "212256  The internationally acclaimed debut of a novel...   \n",
       "212260  In Digby, Willy Wink finds himself in the midd...   \n",
       "212333  Johanna is devastated when her beloved grandmo...   \n",
       "\n",
       "                              authors  \\\n",
       "index                                   \n",
       "115             ['Patricia Cornwell']   \n",
       "209            ['Philip José Farmer']   \n",
       "330         ['Sadie Wernick Hurwitz']   \n",
       "333                ['Susan Meddaugh']   \n",
       "371       ['William F. Buckley, Jr.']   \n",
       "...                               ...   \n",
       "212041              ['Steven Labree']   \n",
       "212144           ['Philip Singerman']   \n",
       "212256          ['Amanda Filipacchi']   \n",
       "212260  ['Timothy Lee Bonnette, Jr.']   \n",
       "212333             ['Martine Jardin']   \n",
       "\n",
       "                                                    image  \\\n",
       "index                                                       \n",
       "115     http://books.google.com/books/content?id=prefg...   \n",
       "209     http://books.google.com/books/content?id=TP4oD...   \n",
       "330     http://books.google.com/books/content?id=D6Wgi...   \n",
       "333     http://books.google.com/books/content?id=u5r79...   \n",
       "371     http://books.google.com/books/content?id=NZm7P...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books/content?id=NZpeJ...   \n",
       "212144  http://books.google.com/books/content?id=68R7S...   \n",
       "212256  http://books.google.com/books/content?id=uM-1A...   \n",
       "212260  http://books.google.com/books/content?id=pcgBA...   \n",
       "212333  http://books.google.com/books/content?id=I96XB...   \n",
       "\n",
       "                                              previewLink  \\\n",
       "index                                                       \n",
       "115     http://books.google.nl/books?id=prefgSxnGOwC&p...   \n",
       "209     http://books.google.nl/books?id=TP4oDwAAQBAJ&p...   \n",
       "330     http://books.google.nl/books?id=D6WgitXrr8sC&p...   \n",
       "333     http://books.google.nl/books?id=u5r79DAUeIYC&q...   \n",
       "371     http://books.google.nl/books?id=NZm7PAAACAAJ&d...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books?id=NZpeJhtmGo8C&...   \n",
       "212144  http://books.google.com/books?id=68R7SppHYHcC&...   \n",
       "212256  http://books.google.com/books?id=uM-1AwAAQBAJ&...   \n",
       "212260  http://books.google.com/books?id=pcgBAAAACAAJ&...   \n",
       "212333  http://books.google.com/books?id=I96XBQAAQBAJ&...   \n",
       "\n",
       "                          publisher  \\\n",
       "index                                 \n",
       "115                     Hachette UK   \n",
       "209                 Open Road Media   \n",
       "330                       iUniverse   \n",
       "333       Houghton Mifflin Harcourt   \n",
       "371                       Isi Books   \n",
       "...                             ...   \n",
       "212041                Steven LaBree   \n",
       "212144               William Morrow   \n",
       "212256              Open Road Media   \n",
       "212260  Publishamerica Incorporated   \n",
       "212333             Devine Destinies   \n",
       "\n",
       "                                                 infoLink  \\\n",
       "index                                                       \n",
       "115     https://play.google.com/store/books/details?id...   \n",
       "209     https://play.google.com/store/books/details?id...   \n",
       "330     http://books.google.nl/books?id=D6WgitXrr8sC&d...   \n",
       "333     http://books.google.nl/books?id=u5r79DAUeIYC&d...   \n",
       "371     http://books.google.nl/books?id=NZm7PAAACAAJ&d...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books?id=NZpeJhtmGo8C&...   \n",
       "212144  http://books.google.com/books?id=68R7SppHYHcC&...   \n",
       "212256  https://play.google.com/store/books/details?id...   \n",
       "212260  http://books.google.com/books?id=pcgBAAAACAAJ&...   \n",
       "212333  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                  categories  reviews number  average rating  ...  tSVD2991  \\\n",
       "index                                                         ...             \n",
       "115              ['fiction']             157        3.783439  ...  0.004673   \n",
       "209              ['fiction']               7        4.285714  ...  0.000682   \n",
       "330              ['fiction']               1        5.000000  ... -0.005573   \n",
       "333     ['juvenile fiction']               2        5.000000  ...  0.003240   \n",
       "371     ['juvenile fiction']               3        5.000000  ... -0.002185   \n",
       "...                      ...             ...             ...  ...       ...   \n",
       "212041           ['fiction']               3        4.666667  ...  0.004356   \n",
       "212144           ['fiction']               4        4.250000  ...  0.011748   \n",
       "212256           ['fiction']              23        3.739130  ... -0.010690   \n",
       "212260           ['fiction']               2        4.000000  ...  0.002488   \n",
       "212333           ['fiction']               1        5.000000  ... -0.001040   \n",
       "\n",
       "        tSVD2992  tSVD2993  tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  \\\n",
       "index                                                                          \n",
       "115     0.011035  0.000885  0.011355  0.003110 -0.009888  0.001707 -0.000568   \n",
       "209    -0.000072 -0.007662  0.010220  0.001084 -0.007470 -0.006064  0.012055   \n",
       "330    -0.004538  0.000019  0.002786 -0.015920  0.004716 -0.000231 -0.003591   \n",
       "333    -0.001592 -0.003397  0.001563  0.008503  0.011397 -0.003413 -0.004257   \n",
       "371    -0.001413  0.003879  0.004376  0.000110  0.002358  0.005980  0.005749   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212041  0.004014  0.000388 -0.010747 -0.012375 -0.001525  0.006544  0.013144   \n",
       "212144 -0.000504  0.006213 -0.006230 -0.008297 -0.009560 -0.002252  0.006993   \n",
       "212256  0.012784 -0.001923  0.008254 -0.002761 -0.003366  0.018299 -0.007997   \n",
       "212260  0.002354  0.004941 -0.000441  0.008438  0.012563  0.002361  0.008794   \n",
       "212333 -0.004460 -0.001457 -0.008113 -0.002739 -0.001419  0.011348 -0.008496   \n",
       "\n",
       "        tSVD2999  tSVD3000  \n",
       "index                       \n",
       "115     0.000850  0.021092  \n",
       "209    -0.007764  0.011430  \n",
       "330     0.000867 -0.001421  \n",
       "333     0.006653 -0.002277  \n",
       "371    -0.000960 -0.003924  \n",
       "...          ...       ...  \n",
       "212041  0.003183 -0.006146  \n",
       "212144  0.007963 -0.001916  \n",
       "212256  0.004508  0.007875  \n",
       "212260  0.000451 -0.006659  \n",
       "212333  0.005376  0.000832  \n",
       "\n",
       "[5355 rows x 3018 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'tSVD2991', 'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996',\n",
       "       'tSVD2997', 'tSVD2998', 'tSVD2999', 'tSVD3000'],\n",
       "      dtype='object', length=3018)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP train\n",
    "NLP_df_train = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_train_tSVD_3000.csv')\n",
    ")\n",
    "\n",
    "NLP_df_train = NLP_df_train.set_index('index')\n",
    "\n",
    "NLP_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "index                                                       \n",
       "3                           Whispers of the Wicked Saints   \n",
       "24               The Forbidden Stories of Marta Veneranda   \n",
       "42                                Tess and the Highlander   \n",
       "49      Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "73                     Night World: Daughters Of Darkness   \n",
       "...                                                   ...   \n",
       "212361                                       Calder Pride   \n",
       "212365                                      The Road Back   \n",
       "212394                                       Final things   \n",
       "212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "212402                                  The Autograph Man   \n",
       "\n",
       "                                              description  \\\n",
       "index                                                       \n",
       "3       Julia Thomas finds her life spinning out of co...   \n",
       "24      Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "42      In 1543, on a windswept isle off of Scotland, ...   \n",
       "49      Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "73      \"There’s something strange about the new girls...   \n",
       "...                                                   ...   \n",
       "212361  The Long-Awaited Addition to the Beloved Calde...   \n",
       "212365  The sequel to the masterpiece All Quiet on the...   \n",
       "212394  Grace's father believes in science and builds ...   \n",
       "212399  During a school trip to Ellis Island, Dominick...   \n",
       "212402  Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                         authors  \\\n",
       "index                              \n",
       "3            ['Veronica Haddon']   \n",
       "24       ['Sonia Rivera-Valdes']   \n",
       "42            ['May Mcgoldrick']   \n",
       "49        ['Elizabeth Sinclair']   \n",
       "73                ['L.J. Smith']   \n",
       "...                          ...   \n",
       "212361          ['Janet Dailey']   \n",
       "212365  ['Erich Maria Remarque']   \n",
       "212394          ['Jenny Offill']   \n",
       "212399       ['Elvira Woodruff']   \n",
       "212402           ['Zadie Smith']   \n",
       "\n",
       "                                                    image  \\\n",
       "index                                                       \n",
       "3       http://books.google.com/books/content?id=aRSIg...   \n",
       "24      http://books.google.com/books/content?id=A7aYb...   \n",
       "42      http://books.google.com/books/content?id=VmCRS...   \n",
       "49      http://books.google.com/books/content?id=Z6uzJ...   \n",
       "73      http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                   ...   \n",
       "212361  http://books.google.com/books/content?id=nlsgd...   \n",
       "212365  http://books.google.com/books/content?id=obZdA...   \n",
       "212394  http://books.google.com/books/content?id=UbSFB...   \n",
       "212399  http://books.google.com/books/content?id=J7M-N...   \n",
       "212402  http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                              previewLink  \\\n",
       "index                                                       \n",
       "3       http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24      http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "42      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49      http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "73      http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                   ...   \n",
       "212361  http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "212365  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394  http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "212399  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402  http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                                publisher  \\\n",
       "index                                                       \n",
       "3                                               iUniverse   \n",
       "24                                    Seven Stories Press   \n",
       "42                                         Harper Collins   \n",
       "49      Harlequin Treasury-Harlequin American Romance 90s   \n",
       "73                                     Simon and Schuster   \n",
       "...                                                   ...   \n",
       "212361                                     Harper Collins   \n",
       "212365                      Random House Trade Paperbacks   \n",
       "212394                                            Vintage   \n",
       "212399                              Scholastic Paperbacks   \n",
       "212402                                            Vintage   \n",
       "\n",
       "                                                 infoLink  \\\n",
       "index                                                       \n",
       "3       http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24      http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "42      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49      http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "73      http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                   ...   \n",
       "212361  https://play.google.com/store/books/details?id...   \n",
       "212365  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394  https://play.google.com/store/books/details?id...   \n",
       "212399  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                  categories  reviews number  average rating  ...  tSVD2991  \\\n",
       "index                                                         ...             \n",
       "3                ['fiction']              32        3.718750  ...  0.001714   \n",
       "24               ['fiction']               1        5.000000  ...  0.000900   \n",
       "42      ['juvenile fiction']              17        4.235294  ... -0.002241   \n",
       "49               ['fiction']               2        5.000000  ...  0.004947   \n",
       "73      ['juvenile fiction']             134        4.768657  ...  0.000070   \n",
       "...                      ...             ...             ...  ...       ...   \n",
       "212361           ['fiction']              28        4.035714  ... -0.003849   \n",
       "212365           ['fiction']              17        4.705882  ...  0.014579   \n",
       "212394           ['fiction']               1        4.000000  ...  0.007651   \n",
       "212399  ['juvenile fiction']              28        4.678571  ... -0.012930   \n",
       "212402           ['fiction']               4        2.500000  ... -0.007902   \n",
       "\n",
       "        tSVD2992  tSVD2993  tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  \\\n",
       "index                                                                          \n",
       "3      -0.012096  0.012755  0.004345  0.016297  0.022422  0.026395 -0.001808   \n",
       "24      0.002294 -0.002052 -0.013243 -0.002329 -0.005818 -0.000012  0.007939   \n",
       "42     -0.007230 -0.005164  0.000416 -0.007837 -0.002487 -0.004066  0.010140   \n",
       "49     -0.003206  0.011330 -0.004898 -0.002988  0.001996 -0.008123 -0.000639   \n",
       "73      0.003473 -0.013758  0.000954 -0.011287 -0.008795  0.003780  0.002349   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361 -0.000571  0.001393 -0.001693 -0.012675 -0.000349  0.002634 -0.004115   \n",
       "212365 -0.010071 -0.008840  0.011693  0.006862 -0.001960  0.004995 -0.007247   \n",
       "212394  0.005601  0.000670 -0.010949 -0.012086  0.002205 -0.004838 -0.008230   \n",
       "212399  0.005578 -0.000719 -0.004401  0.003777 -0.001121 -0.006025 -0.007464   \n",
       "212402  0.017672  0.005686  0.007956 -0.003892 -0.007589 -0.002541  0.005038   \n",
       "\n",
       "        tSVD2999  tSVD3000  \n",
       "index                       \n",
       "3      -0.028102  0.011717  \n",
       "24     -0.024247 -0.006868  \n",
       "42     -0.007082  0.001236  \n",
       "49     -0.016424 -0.004971  \n",
       "73     -0.013346  0.001237  \n",
       "...          ...       ...  \n",
       "212361  0.004868 -0.006720  \n",
       "212365 -0.013905 -0.001202  \n",
       "212394  0.000084 -0.002611  \n",
       "212399 -0.008325  0.005250  \n",
       "212402 -0.010622 -0.009332  \n",
       "\n",
       "[21419 rows x 3018 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [col for col in NLP_df_test.columns if col.startswith('tSVD')]\n",
    "\n",
    "NLP_df_train = NLP_df_train[columns_to_keep]\n",
    "NLP_df_test = NLP_df_test[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "index                                                                          \n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2991  tSVD2992  tSVD2993  \\\n",
       "index                                 ...                                 \n",
       "3       0.006327  0.040408  0.035931  ...  0.001714 -0.012096  0.012755   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.000900  0.002294 -0.002052   \n",
       "42      0.042483  0.019502  0.016915  ... -0.002241 -0.007230 -0.005164   \n",
       "49     -0.027805 -0.038459 -0.016390  ...  0.004947 -0.003206  0.011330   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.000070  0.003473 -0.013758   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.003849 -0.000571  0.001393   \n",
       "212365 -0.028389  0.026683  0.003763  ...  0.014579 -0.010071 -0.008840   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.007651  0.005601  0.000670   \n",
       "212399 -0.028692 -0.017410  0.020350  ... -0.012930  0.005578 -0.000719   \n",
       "212402 -0.004967 -0.007246 -0.009244  ... -0.007902  0.017672  0.005686   \n",
       "\n",
       "        tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000  \n",
       "index                                                                         \n",
       "3       0.004345  0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  \n",
       "24     -0.013243 -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  \n",
       "42      0.000416 -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  \n",
       "49     -0.004898 -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  \n",
       "73      0.000954 -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "212361 -0.001693 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  \n",
       "212365  0.011693  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  \n",
       "212394 -0.010949 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  \n",
       "212399 -0.004401  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  \n",
       "212402  0.007956 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  \n",
       "\n",
       "[21419 rows x 3000 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description dimension reduction NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "Inputs to choose:\n",
    "- number of layers:\n",
    "    - Description NN\n",
    "        - input\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - final layer\n",
    "    - Description and image embeddings NN\n",
    "        - input\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - noise\n",
    "        - final layer\n",
    "    Too many?   \n",
    "- add dense layers to avoid overfitting?\n",
    "- activation functions\n",
    "    - ReLu (Rectified linear activation function): piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. Simple but effective.\n",
    "- Use linear in the last layer to obtain a continuous variable\n",
    "- optimizer: \n",
    "    - Adam; works with momentums of first and second order. \n",
    "    - sdg: variant of Gradient Descent (Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. It's easy and works well but there is the risk that the model gets stuck in local minima)\n",
    "- loss function\n",
    "    - MSE?\n",
    "- number of epochs\n",
    "- which metric to use to evaluate the model?\n",
    "    - MSE\n",
    "    - MAE\n",
    "\n",
    "- Use gridsearch to optimise hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,536,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,667,840</span> (6.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,667,840\u001b[0m (6.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,667,840</span> (6.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,667,840\u001b[0m (6.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get number of inputs - second element of shape (i.e. number of columns in X)\n",
    "input_shape = NLP_df_train.shape[1]\n",
    "\n",
    "# neurons number\n",
    "n_neurons = 512\n",
    "\n",
    "# define a model\n",
    "baseline_model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "baseline_model.add(layers.Dense(\n",
    "            n_neurons, # number of neurons\n",
    "            input_dim = input_shape, # number of inputs \n",
    "            activation = 'relu' # activation faunction\n",
    "            ))\n",
    "\n",
    "# Hidden - Layers\n",
    "baseline_model.add(layers.Dense(\n",
    "                    256, \n",
    "                    activation = \"linear\"))\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mean_squared_error'], \n",
    "    metrics = ['mae', 'mean_squared_error']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# epochs_hist = baseline_model.fit(\n",
    "#     NLP_df_train, # input\n",
    "#     y_wr_train, # output\n",
    "#     epochs=100, # number of iterations\n",
    "#     batch_size=50, # number of observations taken to train the data\n",
    "#     verbose=1,\n",
    "#     validation_data = (NLP_df_test, y_wr_test),\n",
    "#     shuffle = True\n",
    "#     #validation_split=0.2,    \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate intermediate description features with lower dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m670/670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 961us/step\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step\n"
     ]
    }
   ],
   "source": [
    "# Predict baseline X train and X test \n",
    "\n",
    "NLP_intermediate_train = baseline_model.predict(NLP_df_train)\n",
    "NLP_intermediate_test = baseline_model.predict(NLP_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0108245 ,  0.00065164, -0.00545153, ..., -0.01571335,\n",
       "         0.02955209,  0.00690551],\n",
       "       [-0.01072803, -0.00168242,  0.00295767, ...,  0.02182963,\n",
       "         0.01031993,  0.0028618 ],\n",
       "       [ 0.00878206, -0.01319817, -0.00111535, ..., -0.01655175,\n",
       "        -0.00869224,  0.02895204],\n",
       "       ...,\n",
       "       [-0.00909403, -0.01920968,  0.00390711, ...,  0.01819135,\n",
       "        -0.01559369,  0.01616014],\n",
       "       [-0.01123476,  0.00360018, -0.00851824, ..., -0.00435246,\n",
       "         0.01443648,  0.02126275],\n",
       "       [-0.00156955,  0.01984624,  0.00322695, ...,  0.00288491,\n",
       "         0.01403594,  0.02786623]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_intermediate_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NLP_intermediate_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store these into a dataframe\n",
    "NLP_intermediate_train_df = pd.DataFrame(NLP_intermediate_train, index=NLP_df_train.index)\n",
    "NLP_intermediate_test_df = pd.DataFrame(NLP_intermediate_test, index=NLP_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010824</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>-0.005452</td>\n",
       "      <td>-0.005104</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>0.012393</td>\n",
       "      <td>-0.020817</td>\n",
       "      <td>-0.013228</td>\n",
       "      <td>-0.021574</td>\n",
       "      <td>-0.025036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005803</td>\n",
       "      <td>0.011315</td>\n",
       "      <td>-0.008799</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>-0.013595</td>\n",
       "      <td>-0.012163</td>\n",
       "      <td>-0.011097</td>\n",
       "      <td>-0.015713</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>0.006906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.010728</td>\n",
       "      <td>-0.001682</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>-0.025117</td>\n",
       "      <td>-0.012259</td>\n",
       "      <td>-0.011210</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>-0.011660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012165</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>-0.001354</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>-0.016445</td>\n",
       "      <td>0.021830</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>0.002862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.008782</td>\n",
       "      <td>-0.013198</td>\n",
       "      <td>-0.001115</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.024446</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.007805</td>\n",
       "      <td>-0.020572</td>\n",
       "      <td>-0.018259</td>\n",
       "      <td>-0.013887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>-0.019505</td>\n",
       "      <td>-0.001703</td>\n",
       "      <td>0.012063</td>\n",
       "      <td>-0.017577</td>\n",
       "      <td>-0.024271</td>\n",
       "      <td>-0.016552</td>\n",
       "      <td>-0.008692</td>\n",
       "      <td>0.028952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.023215</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.018049</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>-0.022593</td>\n",
       "      <td>-0.022354</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>-0.020443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019795</td>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.023168</td>\n",
       "      <td>-0.002733</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-0.025730</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.015105</td>\n",
       "      <td>0.015534</td>\n",
       "      <td>0.018749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.001495</td>\n",
       "      <td>-0.009027</td>\n",
       "      <td>0.016320</td>\n",
       "      <td>0.015037</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>-0.005140</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.002125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019005</td>\n",
       "      <td>-0.024857</td>\n",
       "      <td>-0.009578</td>\n",
       "      <td>-0.023527</td>\n",
       "      <td>0.040908</td>\n",
       "      <td>-0.011673</td>\n",
       "      <td>-0.010300</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>-0.005872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>-0.004288</td>\n",
       "      <td>0.024036</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>-0.002934</td>\n",
       "      <td>-0.000448</td>\n",
       "      <td>0.008812</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>-0.033428</td>\n",
       "      <td>-0.023333</td>\n",
       "      <td>-0.006778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>-0.011342</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>-0.028393</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>0.017774</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.011150</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.047359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>-0.013106</td>\n",
       "      <td>-0.016671</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>-0.010140</td>\n",
       "      <td>-0.007901</td>\n",
       "      <td>-0.001613</td>\n",
       "      <td>-0.006083</td>\n",
       "      <td>-0.013647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011640</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.020960</td>\n",
       "      <td>-0.022987</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>-0.001866</td>\n",
       "      <td>0.007043</td>\n",
       "      <td>0.015792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>-0.009094</td>\n",
       "      <td>-0.019210</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>-0.005946</td>\n",
       "      <td>-0.004643</td>\n",
       "      <td>-0.003325</td>\n",
       "      <td>-0.011158</td>\n",
       "      <td>-0.034528</td>\n",
       "      <td>-0.022334</td>\n",
       "      <td>-0.027316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.037557</td>\n",
       "      <td>-0.016791</td>\n",
       "      <td>0.022755</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>-0.010139</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>-0.015594</td>\n",
       "      <td>0.016160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>-0.011235</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>-0.008518</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>-0.002780</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>-0.009787</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>-0.044573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004709</td>\n",
       "      <td>-0.010563</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>0.014436</td>\n",
       "      <td>0.021263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>-0.001570</td>\n",
       "      <td>0.019846</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>-0.001438</td>\n",
       "      <td>-0.010180</td>\n",
       "      <td>-0.010945</td>\n",
       "      <td>-0.008209</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.006685</td>\n",
       "      <td>-0.034251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006005</td>\n",
       "      <td>0.010599</td>\n",
       "      <td>-0.009038</td>\n",
       "      <td>0.013026</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>-0.009628</td>\n",
       "      <td>-0.016519</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.027866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "index                                                                          \n",
       "3      -0.010824  0.000652 -0.005452 -0.005104 -0.007692  0.012393 -0.020817   \n",
       "24     -0.010728 -0.001682  0.002958 -0.025117 -0.012259 -0.011210  0.009935   \n",
       "42      0.008782 -0.013198 -0.001115  0.001088  0.024446 -0.000051  0.007805   \n",
       "49     -0.023215  0.003079  0.018049  0.008903  0.002351 -0.013697 -0.022593   \n",
       "73     -0.001495 -0.009027  0.016320  0.015037  0.019504 -0.005140  0.015892   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361 -0.004288  0.024036 -0.001788 -0.002934 -0.000448  0.008812  0.002584   \n",
       "212365 -0.013106 -0.016671  0.008757  0.008555  0.001254 -0.010140 -0.007901   \n",
       "212394 -0.009094 -0.019210  0.003907 -0.005946 -0.004643 -0.003325 -0.011158   \n",
       "212399 -0.011235  0.003600 -0.008518  0.011440  0.003762 -0.002780  0.020722   \n",
       "212402 -0.001570  0.019846  0.003227 -0.001438 -0.010180 -0.010945 -0.008209   \n",
       "\n",
       "             7         8         9    ...       246       247       248  \\\n",
       "index                                 ...                                 \n",
       "3      -0.013228 -0.021574 -0.025036  ... -0.005803  0.011315 -0.008799   \n",
       "24      0.006766 -0.000344 -0.011660  ... -0.012165  0.002450  0.005167   \n",
       "42     -0.020572 -0.018259 -0.013887  ... -0.000133  0.006721 -0.019505   \n",
       "49     -0.022354  0.004705 -0.020443  ... -0.019795  0.022665  0.023168   \n",
       "73      0.005465 -0.015741 -0.002125  ...  0.019005 -0.024857 -0.009578   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361 -0.033428 -0.023333 -0.006778  ...  0.002116 -0.011342  0.005092   \n",
       "212365 -0.001613 -0.006083 -0.013647  ... -0.011640  0.008297  0.020960   \n",
       "212394 -0.034528 -0.022334 -0.027316  ...  0.003540  0.008372  0.037557   \n",
       "212399 -0.009787  0.008061 -0.044573  ... -0.004709 -0.010563  0.001698   \n",
       "212402 -0.012181 -0.006685 -0.034251  ... -0.006005  0.010599 -0.009038   \n",
       "\n",
       "             249       250       251       252       253       254       255  \n",
       "index                                                                         \n",
       "3       0.004064 -0.013595 -0.012163 -0.011097 -0.015713  0.029552  0.006906  \n",
       "24     -0.001354  0.019977 -0.001397 -0.016445  0.021830  0.010320  0.002862  \n",
       "42     -0.001703  0.012063 -0.017577 -0.024271 -0.016552 -0.008692  0.028952  \n",
       "49     -0.002733  0.000274 -0.025730  0.010744  0.015105  0.015534  0.018749  \n",
       "73     -0.023527  0.040908 -0.011673 -0.010300  0.007110  0.009161 -0.005872  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "212361 -0.028393  0.004821  0.017774  0.007223  0.011150  0.002605  0.047359  \n",
       "212365 -0.022987  0.003424  0.004767  0.010191 -0.001866  0.007043  0.015792  \n",
       "212394 -0.016791  0.022755  0.009669 -0.010139  0.018191 -0.015594  0.016160  \n",
       "212399 -0.000701  0.008038  0.001382  0.004782 -0.004352  0.014436  0.021263  \n",
       "212402  0.013026 -0.004544 -0.009628 -0.016519  0.002885  0.014036  0.027866  \n",
       "\n",
       "[21419 rows x 256 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that indices are correct\n",
    "NLP_intermediate_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y cuts\n",
    "We are going to run two models for two target variables\n",
    "- Target variable: Average rating\n",
    "  - baseline (i.e. excluding image embeddings)\n",
    "  - including image embeddings\n",
    "- Target variable: weighted rating\n",
    "  - baseline (i.e. excluding image embeddings)\n",
    "  - including image embeddings\n",
    "\n",
    "We therefore need to create the following datsets\n",
    "- X train and X test with embeddings\n",
    "- X train and X text without embeddings\n",
    "- y train and y test using average rating\n",
    "- y train and y test using weighted rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model data for SVR\n",
    "X_baseline_train = pd.merge(\n",
    "    NLP_df_train,\n",
    "    X_train['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "\n",
    "X_baseline_test = pd.merge(\n",
    "    NLP_df_test,\n",
    "    X_test['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2992  tSVD2993  tSVD2994  \\\n",
       "3       0.006327  0.040408  0.035931  ... -0.012096  0.012755  0.004345   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.002294 -0.002052 -0.013243   \n",
       "42      0.042483  0.019502  0.016915  ... -0.007230 -0.005164  0.000416   \n",
       "49     -0.027805 -0.038459 -0.016390  ... -0.003206  0.011330 -0.004898   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.003473 -0.013758  0.000954   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.000571  0.001393 -0.001693   \n",
       "212365 -0.028389  0.026683  0.003763  ... -0.010071 -0.008840  0.011693   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.005601  0.000670 -0.010949   \n",
       "212399 -0.028692 -0.017410  0.020350  ...  0.005578 -0.000719 -0.004401   \n",
       "212402 -0.004967 -0.007246 -0.009244  ...  0.017672  0.005686  0.007956   \n",
       "\n",
       "        tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000    year  \n",
       "3       0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  2005.0  \n",
       "24     -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  2001.0  \n",
       "42     -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  2002.0  \n",
       "49     -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  1997.0  \n",
       "73     -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  2016.0  \n",
       "...          ...       ...       ...       ...       ...       ...     ...  \n",
       "212361 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  2009.0  \n",
       "212365  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  1998.0  \n",
       "212394 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  2015.0  \n",
       "212399  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  2000.0  \n",
       "212402 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  2003.0  \n",
       "\n",
       "[21419 rows x 3001 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21419, 3001)\n",
      "(5355, 3001)\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline_train.shape)\n",
    "print(X_baseline_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2992  tSVD2993  tSVD2994  \\\n",
       "3       0.006327  0.040408  0.035931  ... -0.012096  0.012755  0.004345   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.002294 -0.002052 -0.013243   \n",
       "42      0.042483  0.019502  0.016915  ... -0.007230 -0.005164  0.000416   \n",
       "49     -0.027805 -0.038459 -0.016390  ... -0.003206  0.011330 -0.004898   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.003473 -0.013758  0.000954   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.000571  0.001393 -0.001693   \n",
       "212365 -0.028389  0.026683  0.003763  ... -0.010071 -0.008840  0.011693   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.005601  0.000670 -0.010949   \n",
       "212399 -0.028692 -0.017410  0.020350  ...  0.005578 -0.000719 -0.004401   \n",
       "212402 -0.004967 -0.007246 -0.009244  ...  0.017672  0.005686  0.007956   \n",
       "\n",
       "        tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000    year  \n",
       "3       0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  2005.0  \n",
       "24     -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  2001.0  \n",
       "42     -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  2002.0  \n",
       "49     -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  1997.0  \n",
       "73     -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  2016.0  \n",
       "...          ...       ...       ...       ...       ...       ...     ...  \n",
       "212361 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  2009.0  \n",
       "212365  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  1998.0  \n",
       "212394 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  2015.0  \n",
       "212399  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  2000.0  \n",
       "212402 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  2003.0  \n",
       "\n",
       "[21419 rows x 3001 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With image embeddings\n",
    "X_final_train = pd.merge(\n",
    "    X_images_train,\n",
    "    X_baseline_train,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "    \n",
    "X_final_test = pd.merge(\n",
    "    X_images_test,\n",
    "    X_baseline_test,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_0', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5',\n",
       "       'image_6', 'image_7', 'image_8', 'image_9',\n",
       "       ...\n",
       "       'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996', 'tSVD2997',\n",
       "       'tSVD2998', 'tSVD2999', 'tSVD3000', 'year'],\n",
       "      dtype='object', length=3257)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model data for NN\n",
    "X_baseline_train_NN = NLP_intermediate_train_df\n",
    "X_baseline_test_NN = NLP_intermediate_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack description + publish year and images embeddings\n",
    "\n",
    "X_final_train_NN = pd.merge(\n",
    "    X_baseline_train_NN, \n",
    "    X_images_train, \n",
    "    left_index = True, \n",
    "    right_index = True)\n",
    "\n",
    "X_final_test_NN = pd.merge(\n",
    "    X_baseline_test_NN, \n",
    "    X_images_test, \n",
    "    left_index = True, \n",
    "    right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using NMF output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "X_baseline_train_NMF = pd.merge(\n",
    "    NMF_df_train,\n",
    "    X_train['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "\n",
    "X_baseline_test_NMF = pd.merge(\n",
    "    NMF_df_test,\n",
    "    X_test['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With image embeddings\n",
    "X_final_train_NMF = pd.merge(\n",
    "    X_images_train,\n",
    "    X_baseline_train_NMF,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "    \n",
    "X_final_test_NMF = pd.merge(\n",
    "    X_images_test,\n",
    "    X_baseline_test_NMF,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression & co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "\n",
    "# Random forest\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# SVR\n",
    "svr_model = SVR(kernel='rbf')  # 'rbf' for radial basis function kernel\n",
    "\n",
    "# Lightgbm\n",
    "\n",
    "\n",
    "# Define pipeline steps\n",
    "rf_pipeline = Pipeline([\n",
    "    ('rf', rf)  # Random Forest classifier\n",
    "])\n",
    "\n",
    "svr_pipeline = Pipeline([\n",
    "    ('svr', svr_model)  # Neural Network classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  \\\n",
       "Baseline Support Vector Regression  (SVR())   \n",
       "Final Support Vector Regression     (SVR())   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                   prediction   MAE   MSE  \n",
       "Baseline Support Vector Regression       None  None  None  \n",
       "Final Support Vector Regression          None  None  None  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "evaluation_metrics = pd.DataFrame({\n",
    "    #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "    'Baseline Support Vector Regression': {'model': svr_pipeline, 'X_train': X_baseline_train_NMF, 'X_test' : X_baseline_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Support Vector Regression': {'model': svr_pipeline, 'X_train': X_final_train_NMF, 'X_test' : X_final_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "}).transpose()\n",
    "\n",
    "evaluation_metrics = evaluation_metrics.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Support Vector Regression\n",
      "> Training completed. Duration: 00:06\n",
      "> Predictions completed. Duration: 28544456:22\n",
      "> Evaluation completed. Duration: 00:06\n",
      ">> Total time taken: 00:10\n",
      "\n",
      "\n",
      "Final Support Vector Regression\n",
      "> Training completed. Duration: 00:14\n",
      "> Predictions completed. Duration: 28544456:34\n",
      "> Evaluation completed. Duration: 00:14\n",
      ">> Total time taken: 00:21\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics.iterrows():\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "    # Call model\n",
    "    model = row['model']\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(row['X_train'], y_wr_train)\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate predictions\n",
    "    y_wr_pred = model.predict(row['X_test'])\n",
    "\n",
    "    # save predictions\n",
    "    row['prediction'] = y_wr_pred\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Predictions completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_wr_test, y_wr_pred)\n",
    "    mae = mean_absolute_error(y_wr_test, y_wr_pred)\n",
    "\n",
    "    # Save metrics\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\">> Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[4.2630513149316505, 4.2635265104359625, 4.262...</td>\n",
       "      <td>0.131492</td>\n",
       "      <td>0.04526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.270183758646303, 4.251684087400329, 4.26068...</td>\n",
       "      <td>0.13114</td>\n",
       "      <td>0.044796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  \\\n",
       "Baseline Support Vector Regression  (SVR())   \n",
       "Final Support Vector Regression     (SVR())   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.2630513149316505, 4.2635265104359625, 4.262...   \n",
       "Final Support Vector Regression     [4.270183758646303, 4.251684087400329, 4.26068...   \n",
       "\n",
       "                                         MAE       MSE  \n",
       "Baseline Support Vector Regression  0.131492   0.04526  \n",
       "Final Support Vector Regression      0.13114  0.044796  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest</th>\n",
       "      <td>(RandomForestRegressor())</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Random Forest</th>\n",
       "      <td>(RandomForestRegressor())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model  \\\n",
       "Baseline Random Forest  (RandomForestRegressor())   \n",
       "Final Random Forest     (RandomForestRegressor())   \n",
       "\n",
       "                                                                  X_train  \\\n",
       "Baseline Random Forest             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Random Forest              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                   X_test  \\\n",
       "Baseline Random Forest             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Random Forest              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                       prediction   MAE   MSE  \n",
       "Baseline Random Forest       None  None  None  \n",
       "Final Random Forest          None  None  None  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "evaluation_metrics_RF = pd.DataFrame({\n",
    "    'Baseline Random Forest': {'model': rf_pipeline, 'X_train': X_baseline_train_NMF, 'X_test' : X_baseline_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Random Forest': {'model': rf_pipeline, 'X_train': X_final_train_NMF, 'X_test' : X_final_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "    }).transpose()\n",
    "\n",
    "evaluation_metrics_RF = evaluation_metrics_RF.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest\n",
      "> Training completed. Duration: 02:19\n",
      "> Predictions completed. Duration: 28544456:50\n",
      "> Evaluation completed. Duration: 02:19\n",
      ">> Total time taken: 02:19\n",
      "\n",
      "\n",
      "Final Random Forest\n",
      "> Training completed. Duration: 14:17\n",
      "> Predictions completed. Duration: 28544459:09\n",
      "> Evaluation completed. Duration: 14:17\n",
      ">> Total time taken: 14:17\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics_RF.iterrows():\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "    # Call model\n",
    "    model = row['model']\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(row['X_train'], y_wr_train)\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate predictions\n",
    "    y_wr_pred = model.predict(row['X_test'])\n",
    "\n",
    "    # save predictions\n",
    "    row['prediction'] = y_wr_pred\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Predictions completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_wr_test, y_wr_pred)\n",
    "    mae = mean_absolute_error(y_wr_test, y_wr_pred)\n",
    "\n",
    "    # Save metrics\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\">> Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest</th>\n",
       "      <td>((DecisionTreeRegressor(max_features=1.0, rand...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[4.237389858905316, 4.268050725923043, 4.25435...</td>\n",
       "      <td>0.142518</td>\n",
       "      <td>0.047299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Random Forest</th>\n",
       "      <td>((DecisionTreeRegressor(max_features=1.0, rand...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.311126782469619, 4.17027010139218, 4.307437...</td>\n",
       "      <td>0.132459</td>\n",
       "      <td>0.042102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    model  \\\n",
       "Baseline Random Forest  ((DecisionTreeRegressor(max_features=1.0, rand...   \n",
       "Final Random Forest     ((DecisionTreeRegressor(max_features=1.0, rand...   \n",
       "\n",
       "                                                                  X_train  \\\n",
       "Baseline Random Forest             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Random Forest              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                   X_test  \\\n",
       "Baseline Random Forest             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Random Forest              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                               prediction  \\\n",
       "Baseline Random Forest  [4.237389858905316, 4.268050725923043, 4.25435...   \n",
       "Final Random Forest     [4.311126782469619, 4.17027010139218, 4.307437...   \n",
       "\n",
       "                             MAE       MSE  \n",
       "Baseline Random Forest  0.142518  0.047299  \n",
       "Final Random Forest     0.132459  0.042102  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>None</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>None</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  \\\n",
       "Baseline Neural Network  None   \n",
       "Final Neural Network     None   \n",
       "\n",
       "                                                                   X_train  \\\n",
       "Baseline Neural Network             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                    X_test  \\\n",
       "Baseline Neural Network             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                        prediction   MAE   MSE  \n",
       "Baseline Neural Network       None  None  None  \n",
       "Final Neural Network          None  None  None  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "# evaluation_metrics_NN = pd.DataFrame({\n",
    "#     #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "#     'Baseline Neural Network': {'model': None, 'X_train': X_baseline_train_NN, 'X_test' : X_baseline_test_NN, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "#     'Final Neural Network': {'model': None, 'X_train': X_final_train_NN, 'X_test' : X_final_test_NN, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "# }).transpose()\n",
    "\n",
    "# NOTE: trying to run the NN without the dimension reduction of the NLP output\n",
    "evaluation_metrics_NN = pd.DataFrame({\n",
    "    #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "    'Baseline Neural Network': {'model': None, 'X_train': X_baseline_train_NMF, 'X_test' : X_baseline_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Neural Network': {'model': None, 'X_train': X_final_train_NMF, 'X_test' : X_final_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "}).transpose()\n",
    "\n",
    "evaluation_metrics_NN = evaluation_metrics_NN.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Neural Network\n",
      "> Input shape: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">52,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m52,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │        \u001b[38;5;34m15,934\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">199,549</span> (779.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m199,549\u001b[0m (779.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">199,549</span> (779.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m199,549\u001b[0m (779.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 485.9102 - mae: 10.0932 - mean_squared_error: 485.9110 - val_loss: 0.3300 - val_mae: 0.5500 - val_mean_squared_error: 0.3299\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7809 - mae: 1.1806 - mean_squared_error: 2.7809 - val_loss: 0.2523 - val_mae: 0.4761 - val_mean_squared_error: 0.2520\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8678 - mae: 1.0612 - mean_squared_error: 1.8678 - val_loss: 0.4524 - val_mae: 0.6500 - val_mean_squared_error: 0.4526\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6737 - mae: 0.9969 - mean_squared_error: 1.6737 - val_loss: 0.0480 - val_mae: 0.1489 - val_mean_squared_error: 0.0464\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2832 - mae: 0.8950 - mean_squared_error: 1.2832 - val_loss: 0.3204 - val_mae: 0.5415 - val_mean_squared_error: 0.3203\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2196 - mae: 0.8366 - mean_squared_error: 1.2196 - val_loss: 0.1930 - val_mae: 0.4113 - val_mean_squared_error: 0.1925\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9109 - mae: 0.7552 - mean_squared_error: 0.9109 - val_loss: 0.3101 - val_mae: 0.5321 - val_mean_squared_error: 0.3099\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8232 - mae: 0.7208 - mean_squared_error: 0.8232 - val_loss: 0.0720 - val_mae: 0.2242 - val_mean_squared_error: 0.0708\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7619 - mae: 0.6887 - mean_squared_error: 0.7619 - val_loss: 0.0471 - val_mae: 0.1350 - val_mean_squared_error: 0.0453\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6951 - mae: 0.6619 - mean_squared_error: 0.6951 - val_loss: 0.1057 - val_mae: 0.2901 - val_mean_squared_error: 0.1048\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6337 - mae: 0.6297 - mean_squared_error: 0.6337 - val_loss: 0.2468 - val_mae: 0.4707 - val_mean_squared_error: 0.2464\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5314 - mae: 0.5803 - mean_squared_error: 0.5314 - val_loss: 0.1227 - val_mae: 0.3177 - val_mean_squared_error: 0.1218\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4707 - mae: 0.5431 - mean_squared_error: 0.4707 - val_loss: 0.2700 - val_mae: 0.4942 - val_mean_squared_error: 0.2698\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4203 - mae: 0.5149 - mean_squared_error: 0.4203 - val_loss: 0.1207 - val_mae: 0.3147 - val_mean_squared_error: 0.1199\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3719 - mae: 0.4833 - mean_squared_error: 0.3719 - val_loss: 0.0478 - val_mae: 0.1488 - val_mean_squared_error: 0.0462\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3210 - mae: 0.4480 - mean_squared_error: 0.3210 - val_loss: 0.1059 - val_mae: 0.2907 - val_mean_squared_error: 0.1049\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2824 - mae: 0.4199 - mean_squared_error: 0.2824 - val_loss: 0.1101 - val_mae: 0.2978 - val_mean_squared_error: 0.1092\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2496 - mae: 0.3930 - mean_squared_error: 0.2496 - val_loss: 0.1020 - val_mae: 0.2840 - val_mean_squared_error: 0.1010\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2099 - mae: 0.3591 - mean_squared_error: 0.2099 - val_loss: 0.1760 - val_mae: 0.3909 - val_mean_squared_error: 0.1754\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1751 - mae: 0.3285 - mean_squared_error: 0.1751 - val_loss: 0.1226 - val_mae: 0.3177 - val_mean_squared_error: 0.1217\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1529 - mae: 0.3057 - mean_squared_error: 0.1529 - val_loss: 0.0595 - val_mae: 0.1928 - val_mean_squared_error: 0.0581\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1777 - mae: 0.2907 - mean_squared_error: 0.1777 - val_loss: 0.0943 - val_mae: 0.2702 - val_mean_squared_error: 0.0932\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1149 - mae: 0.2613 - mean_squared_error: 0.1149 - val_loss: 0.0567 - val_mae: 0.1844 - val_mean_squared_error: 0.0552\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0921 - mae: 0.2303 - mean_squared_error: 0.0921 - val_loss: 0.1218 - val_mae: 0.3166 - val_mean_squared_error: 0.1209\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0807 - mae: 0.2164 - mean_squared_error: 0.0807 - val_loss: 0.0558 - val_mae: 0.1815 - val_mean_squared_error: 0.0543\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0713 - mae: 0.1916 - mean_squared_error: 0.0713 - val_loss: 0.1130 - val_mae: 0.3026 - val_mean_squared_error: 0.1121\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0742 - mae: 0.1971 - mean_squared_error: 0.0742 - val_loss: 0.0789 - val_mae: 0.2400 - val_mean_squared_error: 0.0777\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0782 - mae: 0.1827 - mean_squared_error: 0.0782 - val_loss: 0.0504 - val_mae: 0.1623 - val_mean_squared_error: 0.0489\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0519 - mae: 0.1571 - mean_squared_error: 0.0519 - val_loss: 0.0475 - val_mae: 0.1470 - val_mean_squared_error: 0.0458\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0452 - mae: 0.1483 - mean_squared_error: 0.0452 - val_loss: 0.0470 - val_mae: 0.1436 - val_mean_squared_error: 0.0454\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0525 - mae: 0.1441 - mean_squared_error: 0.0525 - val_loss: 0.0466 - val_mae: 0.1369 - val_mean_squared_error: 0.0449\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0837 - mae: 0.1366 - mean_squared_error: 0.0837 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0443 - mae: 0.1370 - mean_squared_error: 0.0443 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0498 - mae: 0.1348 - mean_squared_error: 0.0498 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0550 - mae: 0.1350 - mean_squared_error: 0.0550 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0422 - mae: 0.1343 - mean_squared_error: 0.0422 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0418 - mae: 0.1331 - mean_squared_error: 0.0418 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0865 - mae: 0.1374 - mean_squared_error: 0.0865 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0434 - mae: 0.1347 - mean_squared_error: 0.0434 - val_loss: 0.0467 - val_mae: 0.1337 - val_mean_squared_error: 0.0449\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0419 - mae: 0.1333 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0426 - mae: 0.1348 - mean_squared_error: 0.0426 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1340 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0423 - mae: 0.1335 - mean_squared_error: 0.0423 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0422 - mae: 0.1333 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1384 - val_mean_squared_error: 0.0449\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0419 - mae: 0.1339 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1370 - val_mean_squared_error: 0.0449\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0438 - mae: 0.1356 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0423 - mae: 0.1345 - mean_squared_error: 0.0423 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0434 - mae: 0.1346 - mean_squared_error: 0.0434 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0422 - mae: 0.1342 - mean_squared_error: 0.0421 - val_loss: 0.0466 - val_mae: 0.1360 - val_mean_squared_error: 0.0449\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0435 - mae: 0.1362 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1361 - val_mean_squared_error: 0.0449\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.1335 - mean_squared_error: 0.0415 - val_loss: 0.0466 - val_mae: 0.1367 - val_mean_squared_error: 0.0449\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0421 - mae: 0.1347 - mean_squared_error: 0.0421 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0452 - mae: 0.1377 - mean_squared_error: 0.0452 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0432 - mae: 0.1334 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1364 - val_mean_squared_error: 0.0449\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1367 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0432 - mae: 0.1348 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1341 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0413 - mae: 0.1342 - mean_squared_error: 0.0413 - val_loss: 0.0466 - val_mae: 0.1362 - val_mean_squared_error: 0.0449\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1346 - mean_squared_error: 0.0433 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1349 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0436 - mae: 0.1354 - mean_squared_error: 0.0436 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0437 - mae: 0.1356 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1357 - mean_squared_error: 0.0442 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1352 - mean_squared_error: 0.0445 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0422 - mae: 0.1337 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1363 - mean_squared_error: 0.0442 - val_loss: 0.0467 - val_mae: 0.1340 - val_mean_squared_error: 0.0449\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0456 - mae: 0.1368 - mean_squared_error: 0.0456 - val_loss: 0.0467 - val_mae: 0.1337 - val_mean_squared_error: 0.0449\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0428 - mae: 0.1354 - mean_squared_error: 0.0428 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0422 - mae: 0.1333 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0447 - mae: 0.1372 - mean_squared_error: 0.0447 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0452 - mae: 0.1361 - mean_squared_error: 0.0452 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1346 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1351 - mean_squared_error: 0.0429 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1337 - mean_squared_error: 0.0433 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1343 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1367 - val_mean_squared_error: 0.0449\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0438 - mae: 0.1359 - mean_squared_error: 0.0438 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1191 - mae: 0.1360 - mean_squared_error: 0.1191 - val_loss: 0.0466 - val_mae: 0.1361 - val_mean_squared_error: 0.0449\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0428 - mae: 0.1342 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1362 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.1332 - mean_squared_error: 0.0415 - val_loss: 0.0466 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0436 - mae: 0.1366 - mean_squared_error: 0.0436 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1340 - mean_squared_error: 0.0433 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0436 - mae: 0.1340 - mean_squared_error: 0.0436 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.1329 - mean_squared_error: 0.0415 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1345 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0430 - mae: 0.1354 - mean_squared_error: 0.0430 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1340 - mean_squared_error: 0.0429 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0432 - mae: 0.1350 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1370 - val_mean_squared_error: 0.0449\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0452 - mae: 0.1373 - mean_squared_error: 0.0452 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0430 - mae: 0.1345 - mean_squared_error: 0.0430 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1352 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1343 - mean_squared_error: 0.0433 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6723 - mae: 0.1358 - mean_squared_error: 0.6723 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0550 - mae: 0.1364 - mean_squared_error: 0.0550 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0434 - mae: 0.1341 - mean_squared_error: 0.0434 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0409 - mae: 0.1324 - mean_squared_error: 0.0409 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1333 - mean_squared_error: 0.0427 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1349 - mean_squared_error: 0.0427 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0454 - mae: 0.1355 - mean_squared_error: 0.0454 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0443 - mae: 0.1359 - mean_squared_error: 0.0443 - val_loss: 0.0466 - val_mae: 0.1360 - val_mean_squared_error: 0.0449\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step\n",
      "> Model set up completed. Duration:  14: 17\n",
      "> Compilation completed. Duration:  28544459: 10\n",
      "> Training completed. Duration:  15: 32\n",
      "> Prediction completed. Duration:  28544459: 10\n",
      "> Evaluation completed. Duration:  15: 32\n",
      "Total time taken: 01:15\n",
      "\n",
      "\n",
      "Final Neural Network\n",
      "> Input shape: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">183,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m183,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │        \u001b[38;5;34m15,934\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">330,621</span> (1.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m330,621\u001b[0m (1.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">330,621</span> (1.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m330,621\u001b[0m (1.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 918.3138 - mae: 13.0188 - mean_squared_error: 918.3151 - val_loss: 0.1559 - val_mae: 0.3646 - val_mean_squared_error: 0.1552\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4248 - mae: 1.1497 - mean_squared_error: 2.4248 - val_loss: 0.2535 - val_mae: 0.4775 - val_mean_squared_error: 0.2532\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5565 - mae: 0.9340 - mean_squared_error: 1.5565 - val_loss: 0.8299 - val_mae: 0.8916 - val_mean_squared_error: 0.8310\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1999 - mae: 0.8438 - mean_squared_error: 1.1999 - val_loss: 0.1480 - val_mae: 0.3539 - val_mean_squared_error: 0.1473\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0631 - mae: 0.8000 - mean_squared_error: 1.0631 - val_loss: 0.1631 - val_mae: 0.3739 - val_mean_squared_error: 0.1624\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0013 - mae: 0.7824 - mean_squared_error: 1.0013 - val_loss: 0.5142 - val_mae: 0.6953 - val_mean_squared_error: 0.5146\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9606 - mae: 0.7674 - mean_squared_error: 0.9606 - val_loss: 0.3097 - val_mae: 0.5319 - val_mean_squared_error: 0.3096\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9084 - mae: 0.7513 - mean_squared_error: 0.9084 - val_loss: 0.6284 - val_mae: 0.7721 - val_mean_squared_error: 0.6290\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8590 - mae: 0.7318 - mean_squared_error: 0.8590 - val_loss: 0.7607 - val_mae: 0.8524 - val_mean_squared_error: 0.7616\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8190 - mae: 0.7150 - mean_squared_error: 0.8190 - val_loss: 0.3962 - val_mae: 0.6063 - val_mean_squared_error: 0.3963\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7527 - mae: 0.6873 - mean_squared_error: 0.7527 - val_loss: 0.2788 - val_mae: 0.5028 - val_mean_squared_error: 0.2786\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7100 - mae: 0.6664 - mean_squared_error: 0.7100 - val_loss: 0.4128 - val_mae: 0.6196 - val_mean_squared_error: 0.4129\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6538 - mae: 0.6385 - mean_squared_error: 0.6538 - val_loss: 0.2760 - val_mae: 0.5001 - val_mean_squared_error: 0.2758\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6114 - mae: 0.6171 - mean_squared_error: 0.6114 - val_loss: 0.1566 - val_mae: 0.3658 - val_mean_squared_error: 0.1559\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5738 - mae: 0.5992 - mean_squared_error: 0.5738 - val_loss: 0.4100 - val_mae: 0.6175 - val_mean_squared_error: 0.4101\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5102 - mae: 0.5575 - mean_squared_error: 0.5102 - val_loss: 0.0621 - val_mae: 0.1996 - val_mean_squared_error: 0.0607\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4494 - mae: 0.5294 - mean_squared_error: 0.4494 - val_loss: 0.2836 - val_mae: 0.5076 - val_mean_squared_error: 0.2834\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4056 - mae: 0.5009 - mean_squared_error: 0.4056 - val_loss: 0.3197 - val_mae: 0.5412 - val_mean_squared_error: 0.3196\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3396 - mae: 0.4575 - mean_squared_error: 0.3396 - val_loss: 0.1562 - val_mae: 0.3655 - val_mean_squared_error: 0.1555\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2876 - mae: 0.4248 - mean_squared_error: 0.2876 - val_loss: 0.1456 - val_mae: 0.3511 - val_mean_squared_error: 0.1448\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2487 - mae: 0.3955 - mean_squared_error: 0.2487 - val_loss: 0.1978 - val_mae: 0.4173 - val_mean_squared_error: 0.1973\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2066 - mae: 0.3577 - mean_squared_error: 0.2066 - val_loss: 0.0710 - val_mae: 0.2223 - val_mean_squared_error: 0.0697\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1708 - mae: 0.3246 - mean_squared_error: 0.1708 - val_loss: 0.1017 - val_mae: 0.2837 - val_mean_squared_error: 0.1007\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1414 - mae: 0.2927 - mean_squared_error: 0.1414 - val_loss: 0.0672 - val_mae: 0.2134 - val_mean_squared_error: 0.0659\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1319 - mae: 0.2735 - mean_squared_error: 0.1319 - val_loss: 0.0559 - val_mae: 0.1821 - val_mean_squared_error: 0.0545\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1095 - mae: 0.2507 - mean_squared_error: 0.1095 - val_loss: 0.0746 - val_mae: 0.2306 - val_mean_squared_error: 0.0733\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0882 - mae: 0.2234 - mean_squared_error: 0.0882 - val_loss: 0.0469 - val_mae: 0.1433 - val_mean_squared_error: 0.0452\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0798 - mae: 0.2030 - mean_squared_error: 0.0798 - val_loss: 0.0919 - val_mae: 0.2658 - val_mean_squared_error: 0.0908\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1883 - mae: 0.2104 - mean_squared_error: 0.1883 - val_loss: 0.0476 - val_mae: 0.1478 - val_mean_squared_error: 0.0459\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1762 - mae: 0.1930 - mean_squared_error: 0.1762 - val_loss: 0.0469 - val_mae: 0.1425 - val_mean_squared_error: 0.0452\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0547 - mae: 0.1596 - mean_squared_error: 0.0547 - val_loss: 0.0466 - val_mae: 0.1370 - val_mean_squared_error: 0.0448\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0460 - mae: 0.1477 - mean_squared_error: 0.0460 - val_loss: 0.0493 - val_mae: 0.1571 - val_mean_squared_error: 0.0477\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0542 - mae: 0.1434 - mean_squared_error: 0.0542 - val_loss: 0.0466 - val_mae: 0.1370 - val_mean_squared_error: 0.0449\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0470 - mae: 0.1353 - mean_squared_error: 0.0470 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0751 - mae: 0.1385 - mean_squared_error: 0.0751 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0800 - mae: 0.1368 - mean_squared_error: 0.0800 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0436 - mae: 0.1348 - mean_squared_error: 0.0436 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0422 - mae: 0.1332 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0439 - mae: 0.1352 - mean_squared_error: 0.0439 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0467 - mae: 0.1366 - mean_squared_error: 0.0467 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0444 - mae: 0.1361 - mean_squared_error: 0.0444 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0464 - mae: 0.1370 - mean_squared_error: 0.0464 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0444 - mae: 0.1359 - mean_squared_error: 0.0444 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0432 - mae: 0.1348 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1362 - val_mean_squared_error: 0.0449\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1362 - mean_squared_error: 0.0442 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1344 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1360 - val_mean_squared_error: 0.0449\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0437 - mae: 0.1358 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1360 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mae: 0.1341 - val_mean_squared_error: 0.0449\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1349 - mean_squared_error: 0.0433 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1348 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1358 - mean_squared_error: 0.0445 - val_loss: 0.0466 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0494 - mae: 0.1338 - mean_squared_error: 0.0495 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0443 - mae: 0.1350 - mean_squared_error: 0.0443 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1347 - mean_squared_error: 0.0445 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0439 - mae: 0.1368 - mean_squared_error: 0.0439 - val_loss: 0.0466 - val_mae: 0.1361 - val_mean_squared_error: 0.0449\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0435 - mae: 0.1357 - mean_squared_error: 0.0435 - val_loss: 0.0467 - val_mae: 0.1340 - val_mean_squared_error: 0.0449\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0468 - mae: 0.1371 - mean_squared_error: 0.0468 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1356 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1366 - val_mean_squared_error: 0.0449\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0431 - mae: 0.1356 - mean_squared_error: 0.0431 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1347 - mean_squared_error: 0.0425 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0503 - mae: 0.1380 - mean_squared_error: 0.0503 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0435 - mae: 0.1342 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1364 - val_mean_squared_error: 0.0449\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0444 - mae: 0.1367 - mean_squared_error: 0.0444 - val_loss: 0.0467 - val_mae: 0.1338 - val_mean_squared_error: 0.0449\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0448 - mae: 0.1360 - mean_squared_error: 0.0448 - val_loss: 0.0466 - val_mae: 0.1364 - val_mean_squared_error: 0.0449\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1343 - mean_squared_error: 0.0427 - val_loss: 0.0467 - val_mae: 0.1345 - val_mean_squared_error: 0.0449\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1359 - mean_squared_error: 0.0445 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1346 - mean_squared_error: 0.0425 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1340 - mean_squared_error: 0.0429 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0426 - mae: 0.1340 - mean_squared_error: 0.0426 - val_loss: 0.0467 - val_mae: 0.1345 - val_mean_squared_error: 0.0449\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1342 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0419 - mae: 0.1338 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1368 - val_mean_squared_error: 0.0449\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1345 - mean_squared_error: 0.0427 - val_loss: 0.0466 - val_mae: 0.1374 - val_mean_squared_error: 0.0449\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1348 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1348 - mean_squared_error: 0.0433 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1139 - mae: 0.1361 - mean_squared_error: 0.1139 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1349 - mean_squared_error: 0.0427 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0435 - mae: 0.1350 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.1353 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0455 - mae: 0.1359 - mean_squared_error: 0.0455 - val_loss: 0.0466 - val_mae: 0.1368 - val_mean_squared_error: 0.0449\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0419 - mae: 0.1326 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0423 - mae: 0.1342 - mean_squared_error: 0.0423 - val_loss: 0.0467 - val_mae: 0.1341 - val_mean_squared_error: 0.0449\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0446 - mae: 0.1357 - mean_squared_error: 0.0446 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1348 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0424 - mae: 0.1340 - mean_squared_error: 0.0424 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0431 - mae: 0.1354 - mean_squared_error: 0.0431 - val_loss: 0.0466 - val_mae: 0.1367 - val_mean_squared_error: 0.0449\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0447 - mae: 0.1369 - mean_squared_error: 0.0447 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1350 - mean_squared_error: 0.0442 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0450 - mae: 0.1353 - mean_squared_error: 0.0450 - val_loss: 0.0467 - val_mae: 0.1336 - val_mean_squared_error: 0.0449\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0413 - mae: 0.1315 - mean_squared_error: 0.0413 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1340 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0414 - mae: 0.1325 - mean_squared_error: 0.0414 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0432 - mae: 0.1347 - mean_squared_error: 0.0432 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0413 - mae: 0.1328 - mean_squared_error: 0.0413 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1357 - mean_squared_error: 0.0445 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0467 - mae: 0.1334 - mean_squared_error: 0.0467 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3232 - mae: 0.1380 - mean_squared_error: 0.3232 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0426 - mae: 0.1346 - mean_squared_error: 0.0426 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1360 - mean_squared_error: 0.0433 - val_loss: 0.0466 - val_mae: 0.1369 - val_mean_squared_error: 0.0449\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1372 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0436 - mae: 0.1351 - mean_squared_error: 0.0436 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step\n",
      "> Model set up completed. Duration:  14: 17\n",
      "> Compilation completed. Duration:  28544460: 25\n",
      "> Training completed. Duration:  15: 47\n",
      "> Prediction completed. Duration:  28544460: 25\n",
      "> Evaluation completed. Duration:  15: 47\n",
      "Total time taken: 01:30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run model for different moodels\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics_NN.iterrows():\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "\n",
    "    input_shape = row['X_train'].shape[1]\n",
    "    print(f\"> Input shape: {input_shape}\")\n",
    "\n",
    "    # neurons number\n",
    "    n_neurons = 512\n",
    "\n",
    "### define a model\n",
    "    final_model = keras.Sequential()\n",
    "\n",
    "    # Add input layer\n",
    "    final_model.add(layers.Dense(\n",
    "                n_neurons, # number of neurons\n",
    "                input_dim = input_shape, # number of inputs \n",
    "                activation = 'relu' # activation faunction\n",
    "                ))\n",
    "\n",
    "    # Hidden - Layers\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.3, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "    final_model.add(layers.Dense(\n",
    "        256, \n",
    "        activation = \"relu\"))\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.2, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "    final_model.add(layers.Dense(\n",
    "        62, \n",
    "        activation = \"relu\"))\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.2, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "\n",
    "    # Final layer\n",
    "    final_model.add(layers.Dense(\n",
    "        1, \n",
    "        activation = 'linear'))\n",
    "\n",
    "    final_model.summary()\n",
    "\n",
    "    # Add model to table\n",
    "    row['model'] = final_model\n",
    "    \n",
    "### Compile the model\n",
    "    final_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mean_squared_error'], \n",
    "    metrics = ['mae', 'mean_squared_error']\n",
    "    )\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "\n",
    "### Train the model\n",
    "    epochs_hist = final_model.fit(\n",
    "    row['X_train'], # input\n",
    "    y_wr_train, # output\n",
    "    epochs=100, # number of iterations\n",
    "    batch_size=50, # number of observations taken to train the data - 1030 obs/50 -> there are 17 groups (observations are taken once for epoch) so model is trained 17 times in each epoch\n",
    "    verbose=1,\n",
    "    validation_data = (row['X_test'], y_wr_test),\n",
    "    shuffle = True\n",
    "    #validation_split=0.2,    \n",
    "    )\n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "\n",
    "\n",
    "# ### Predictions\n",
    "    y_pred = final_model.predict(row['X_test'])\n",
    "    # Store predictions\n",
    "    row['prediction'] = y_pred\n",
    "    # Time elapsed\n",
    "    elapsed_time_4 = time.time() - elapsed_time_3\n",
    "\n",
    "\n",
    "# ### Evaluation\n",
    "    mse = mean_squared_error(y_pred, y_wr_test)\n",
    "    mae = mean_absolute_error(y_pred, y_wr_test)\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_5 = time.time() - elapsed_time_4\n",
    "\n",
    "\n",
    "    # Timings\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Model set up completed. Duration: {minutes: 02d}:{seconds: 02d}\")\n",
    "\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Compilation completed. Duration: {minutes: 02d}:{seconds: 02d}\")\n",
    "\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    minutes = int(elapsed_time_4 // 60)\n",
    "    seconds = int(elapsed_time_4 % 60)\n",
    "    print(f\"> Prediction completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    minutes = int(elapsed_time_5 // 60)\n",
    "    seconds = int(elapsed_time_5 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\"Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_1, built=True&gt;</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[[4.2431197], [4.2431197], [4.2431197], [4.243...</td>\n",
       "      <td>0.136032</td>\n",
       "      <td>0.044851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_2, built=True&gt;</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[[4.249064], [4.249064], [4.249064], [4.249064...</td>\n",
       "      <td>0.134362</td>\n",
       "      <td>0.04489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              model  \\\n",
       "Baseline Neural Network  <Sequential name=sequential_1, built=True>   \n",
       "Final Neural Network     <Sequential name=sequential_2, built=True>   \n",
       "\n",
       "                                                                   X_train  \\\n",
       "Baseline Neural Network             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                    X_test  \\\n",
       "Baseline Neural Network             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                prediction  \\\n",
       "Baseline Neural Network  [[4.2431197], [4.2431197], [4.2431197], [4.243...   \n",
       "Final Neural Network     [[4.249064], [4.249064], [4.249064], [4.249064...   \n",
       "\n",
       "                              MAE       MSE  \n",
       "Baseline Neural Network  0.136032  0.044851  \n",
       "Final Neural Network     0.134362   0.04489  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_all = pd.concat([evaluation_metrics, evaluation_metrics_NN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[4.2630513149316505, 4.2635265104359625, 4.262...</td>\n",
       "      <td>0.131492</td>\n",
       "      <td>0.04526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.270183758646303, 4.251684087400329, 4.26068...</td>\n",
       "      <td>0.13114</td>\n",
       "      <td>0.044796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_1, built=True&gt;</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[[4.2431197], [4.2431197], [4.2431197], [4.243...</td>\n",
       "      <td>0.136032</td>\n",
       "      <td>0.044851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_2, built=True&gt;</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[[4.249064], [4.249064], [4.249064], [4.249064...</td>\n",
       "      <td>0.134362</td>\n",
       "      <td>0.04489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         model  \\\n",
       "Baseline Support Vector Regression                                     (SVR())   \n",
       "Final Support Vector Regression                                        (SVR())   \n",
       "Baseline Neural Network             <Sequential name=sequential_1, built=True>   \n",
       "Final Neural Network                <Sequential name=sequential_2, built=True>   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                        NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                        NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.2630513149316505, 4.2635265104359625, 4.262...   \n",
       "Final Support Vector Regression     [4.270183758646303, 4.251684087400329, 4.26068...   \n",
       "Baseline Neural Network             [[4.2431197], [4.2431197], [4.2431197], [4.243...   \n",
       "Final Neural Network                [[4.249064], [4.249064], [4.249064], [4.249064...   \n",
       "\n",
       "                                         MAE       MSE  \n",
       "Baseline Support Vector Regression  0.131492   0.04526  \n",
       "Final Support Vector Regression      0.13114  0.044796  \n",
       "Baseline Neural Network             0.136032  0.044851  \n",
       "Final Neural Network                0.134362   0.04489  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise NN\n",
    "\n",
    "# # Plotting Loss And Mean Square Error For both Training And Test Sets\n",
    "# plt.plot(epochs_hist.history['mse'])\n",
    "# plt.plot(epochs_hist.history['val_mse'])\n",
    "# plt.title('MSE')\n",
    "# plt.ylabel('mae')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(output_folder, '{i} mse chart.png'))\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(epochs_hist.history['loss'])\n",
    "# plt.plot(epochs_hist.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.savefig('4.png')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(output_folder, '{i} summary chart.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other evaluations metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add RMSE and R2 + merge everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE of each model\n",
    "evaluation_metrics_all = pd.concat([evaluation_metrics_all, evaluation_metrics_RF])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[4.2630513149316505, 4.2635265104359625, 4.262...</td>\n",
       "      <td>0.131492</td>\n",
       "      <td>0.04526</td>\n",
       "      <td>0.212743</td>\n",
       "      <td>-0.00911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.270183758646303, 4.251684087400329, 4.26068...</td>\n",
       "      <td>0.13114</td>\n",
       "      <td>0.044796</td>\n",
       "      <td>0.211652</td>\n",
       "      <td>0.001217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_1, built=True&gt;</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[[4.2431197], [4.2431197], [4.2431197], [4.243...</td>\n",
       "      <td>0.136032</td>\n",
       "      <td>0.044851</td>\n",
       "      <td>0.211781</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_2, built=True&gt;</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[[4.249064], [4.249064], [4.249064], [4.249064...</td>\n",
       "      <td>0.134362</td>\n",
       "      <td>0.04489</td>\n",
       "      <td>0.211874</td>\n",
       "      <td>-0.000879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest</th>\n",
       "      <td>((DecisionTreeRegressor(max_features=1.0, rand...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[4.237389858905316, 4.268050725923043, 4.25435...</td>\n",
       "      <td>0.142518</td>\n",
       "      <td>0.047299</td>\n",
       "      <td>0.217483</td>\n",
       "      <td>-0.05458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Random Forest</th>\n",
       "      <td>((DecisionTreeRegressor(max_features=1.0, rand...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.311126782469619, 4.17027010139218, 4.307437...</td>\n",
       "      <td>0.132459</td>\n",
       "      <td>0.042102</td>\n",
       "      <td>0.205188</td>\n",
       "      <td>0.061287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                model  \\\n",
       "Baseline Support Vector Regression                                            (SVR())   \n",
       "Final Support Vector Regression                                               (SVR())   \n",
       "Baseline Neural Network                    <Sequential name=sequential_1, built=True>   \n",
       "Final Neural Network                       <Sequential name=sequential_2, built=True>   \n",
       "Baseline Random Forest              ((DecisionTreeRegressor(max_features=1.0, rand...   \n",
       "Final Random Forest                 ((DecisionTreeRegressor(max_features=1.0, rand...   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                        NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "Baseline Random Forest                         NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Random Forest                          image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                        NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "Baseline Random Forest                         NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Random Forest                          image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.2630513149316505, 4.2635265104359625, 4.262...   \n",
       "Final Support Vector Regression     [4.270183758646303, 4.251684087400329, 4.26068...   \n",
       "Baseline Neural Network             [[4.2431197], [4.2431197], [4.2431197], [4.243...   \n",
       "Final Neural Network                [[4.249064], [4.249064], [4.249064], [4.249064...   \n",
       "Baseline Random Forest              [4.237389858905316, 4.268050725923043, 4.25435...   \n",
       "Final Random Forest                 [4.311126782469619, 4.17027010139218, 4.307437...   \n",
       "\n",
       "                                         MAE       MSE      RMSE        R2  \n",
       "Baseline Support Vector Regression  0.131492   0.04526  0.212743  -0.00911  \n",
       "Final Support Vector Regression      0.13114  0.044796  0.211652  0.001217  \n",
       "Baseline Neural Network             0.136032  0.044851  0.211781 -0.000003  \n",
       "Final Neural Network                0.134362   0.04489  0.211874 -0.000879  \n",
       "Baseline Random Forest              0.142518  0.047299  0.217483  -0.05458  \n",
       "Final Random Forest                 0.132459  0.042102  0.205188  0.061287  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# create RMSE column\n",
    "evaluation_metrics_all['RMSE'] = None\n",
    "evaluation_metrics_all['R2'] = None\n",
    "\n",
    "for i, row in evaluation_metrics_all.iterrows():\n",
    "    rmse = root_mean_squared_error(row['prediction'], y_wr_test)\n",
    "    r2 = r2_score(y_wr_test, row['prediction'])\n",
    "    \n",
    "    evaluation_metrics_all.at[i, 'RMSE'] = rmse\n",
    "    evaluation_metrics_all.at[i, 'R2'] = r2\n",
    "\n",
    "evaluation_metrics_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for each cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>&lt;function mean_absolute_error at 0x1572e9820&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>&lt;function mean_squared_error at 0x1572e9b80&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>&lt;function root_mean_squared_error at 0x1572e9ca0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>&lt;function r2_score at 0x1572f5310&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 metric\n",
       "MAE       <function mean_absolute_error at 0x1572e9820>\n",
       "MSE        <function mean_squared_error at 0x1572e9b80>\n",
       "RMSE  <function root_mean_squared_error at 0x1572e9ca0>\n",
       "R2                   <function r2_score at 0x1572f5310>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dic = {'metric': {'MAE' : mean_absolute_error, \n",
    "                       'MSE': mean_squared_error,\n",
    "                       'RMSE' : root_mean_squared_error, \n",
    "                       'R2' : r2_score}}\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics_dic)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "\n",
    "for i, r in evaluation_metrics_all.iterrows():\n",
    "    #print(r['prediction'])\n",
    "    predictions[i] = np.array(r['prediction'])\n",
    "\n",
    "# Add indices\n",
    "predictions.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = pd.DataFrame(y_wr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add real values in predictions dataset\n",
    "\n",
    "predictions = pd.merge(\n",
    "    predictions,\n",
    "    y_real,\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    "    how = 'inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <th>Final Neural Network</th>\n",
       "      <th>Baseline Random Forest</th>\n",
       "      <th>Final Random Forest</th>\n",
       "      <th>weighted rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143105</th>\n",
       "      <td>4.263051</td>\n",
       "      <td>4.270184</td>\n",
       "      <td>4.24312</td>\n",
       "      <td>4.249064</td>\n",
       "      <td>4.23739</td>\n",
       "      <td>4.311127</td>\n",
       "      <td>4.37443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Baseline Support Vector Regression  \\\n",
       "index_key                                       \n",
       "143105                               4.263051   \n",
       "\n",
       "           Final Support Vector Regression  Baseline Neural Network  \\\n",
       "index_key                                                             \n",
       "143105                            4.270184                  4.24312   \n",
       "\n",
       "           Final Neural Network  Baseline Random Forest  Final Random Forest  \\\n",
       "index_key                                                                      \n",
       "143105                 4.249064                 4.23739             4.311127   \n",
       "\n",
       "           weighted rating  \n",
       "index_key                   \n",
       "143105             4.37443  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Baseline Support Vector Regression',\n",
       " 'Final Support Vector Regression',\n",
       " 'Baseline Neural Network',\n",
       " 'Final Neural Network',\n",
       " 'Baseline Random Forest',\n",
       " 'Final Random Forest']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [col for col in predictions.columns if col != 'weighted rating']\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "subgroup exists\n",
      "model Baseline Support Vector Regression\n",
      "MAE_1_2\n",
      "Baseline Support Vector Regression MAE_1_2\n",
      "                                   MAE_1_2\n",
      "Baseline Support Vector Regression    None\n",
      "Final Support Vector Regression        NaN\n",
      "Baseline Neural Network                NaN\n",
      "Final Neural Network                   NaN\n",
      "Baseline Random Forest                 NaN\n",
      "Final Random Forest                    NaN\n",
      "metric MAE\n",
      "yeee\n",
      "2.3183250228546295\n",
      "  \n",
      "MSE_1_2\n",
      "Baseline Support Vector Regression MSE_1_2\n",
      "                                   MSE_1_2\n",
      "Baseline Support Vector Regression    None\n",
      "Final Support Vector Regression        NaN\n",
      "Baseline Neural Network                NaN\n",
      "Final Neural Network                   NaN\n",
      "Baseline Random Forest                 NaN\n",
      "Final Random Forest                    NaN\n",
      "metric MSE\n",
      "5.374630911593918\n",
      "  \n",
      "RMSE_1_2\n",
      "Baseline Support Vector Regression RMSE_1_2\n",
      "                                   RMSE_1_2\n",
      "Baseline Support Vector Regression     None\n",
      "Final Support Vector Regression         NaN\n",
      "Baseline Neural Network                 NaN\n",
      "Final Neural Network                    NaN\n",
      "Baseline Random Forest                  NaN\n",
      "Final Random Forest                     NaN\n",
      "metric RMSE\n",
      "2.3183250228546295\n",
      "  \n",
      "R2_1_2\n",
      "Baseline Support Vector Regression R2_1_2\n",
      "                                   R2_1_2\n",
      "Baseline Support Vector Regression   None\n",
      "Final Support Vector Regression       NaN\n",
      "Baseline Neural Network               NaN\n",
      "Final Neural Network                  NaN\n",
      "Baseline Random Forest                NaN\n",
      "Final Random Forest                   NaN\n",
      "metric R2\n",
      "nan\n",
      "  \n",
      "model Final Support Vector Regression\n",
      "MAE_1_2\n",
      "Final Support Vector Regression MAE_1_2\n",
      "                                     MAE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "2.301077303645807\n",
      "  \n",
      "MSE_1_2\n",
      "Final Support Vector Regression MSE_1_2\n",
      "                                     MSE_1_2\n",
      "Baseline Support Vector Regression  5.374631\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "5.294956757353857\n",
      "  \n",
      "RMSE_1_2\n",
      "Final Support Vector Regression RMSE_1_2\n",
      "                                    RMSE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "2.301077303645807\n",
      "  \n",
      "R2_1_2\n",
      "Final Support Vector Regression R2_1_2\n",
      "                                   R2_1_2\n",
      "Baseline Support Vector Regression    NaN\n",
      "Final Support Vector Regression      None\n",
      "Baseline Neural Network               NaN\n",
      "Final Neural Network                  NaN\n",
      "Baseline Random Forest                NaN\n",
      "Final Random Forest                   NaN\n",
      "metric R2\n",
      "nan\n",
      "  \n",
      "model Baseline Neural Network\n",
      "MAE_1_2\n",
      "Baseline Neural Network MAE_1_2\n",
      "                                     MAE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression     2.301077\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "2.298762187135495\n",
      "  \n",
      "MSE_1_2\n",
      "Baseline Neural Network MSE_1_2\n",
      "                                     MSE_1_2\n",
      "Baseline Support Vector Regression  5.374631\n",
      "Final Support Vector Regression     5.294957\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "5.284307593003965\n",
      "  \n",
      "RMSE_1_2\n",
      "Baseline Neural Network RMSE_1_2\n",
      "                                    RMSE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression     2.301077\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "2.298762187135495\n",
      "  \n",
      "R2_1_2\n",
      "Baseline Neural Network R2_1_2\n",
      "                                   R2_1_2\n",
      "Baseline Support Vector Regression    NaN\n",
      "Final Support Vector Regression       NaN\n",
      "Baseline Neural Network              None\n",
      "Final Neural Network                  NaN\n",
      "Baseline Random Forest                NaN\n",
      "Final Random Forest                   NaN\n",
      "metric R2\n",
      "nan\n",
      "  \n",
      "model Final Neural Network\n",
      "MAE_1_2\n",
      "Final Neural Network MAE_1_2\n",
      "                                     MAE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression     2.301077\n",
      "Baseline Neural Network             2.298762\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "2.3047064391496552\n",
      "  \n",
      "MSE_1_2\n",
      "Final Neural Network MSE_1_2\n",
      "                                     MSE_1_2\n",
      "Baseline Support Vector Regression  5.374631\n",
      "Final Support Vector Regression     5.294957\n",
      "Baseline Neural Network             5.284308\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "5.311671770657884\n",
      "  \n",
      "RMSE_1_2\n",
      "Final Neural Network RMSE_1_2\n",
      "                                    RMSE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression     2.301077\n",
      "Baseline Neural Network             2.298762\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "2.3047064391496552\n",
      "  \n",
      "R2_1_2\n",
      "Final Neural Network R2_1_2\n",
      "                                   R2_1_2\n",
      "Baseline Support Vector Regression    NaN\n",
      "Final Support Vector Regression       NaN\n",
      "Baseline Neural Network               NaN\n",
      "Final Neural Network                 None\n",
      "Baseline Random Forest                NaN\n",
      "Final Random Forest                   NaN\n",
      "metric R2\n",
      "nan\n",
      "  \n",
      "model Baseline Random Forest\n",
      "MAE_1_2\n",
      "Baseline Random Forest MAE_1_2\n",
      "                                     MAE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression     2.301077\n",
      "Baseline Neural Network             2.298762\n",
      "Final Neural Network                2.304706\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "2.3121164341292757\n",
      "  \n",
      "MSE_1_2\n",
      "Baseline Random Forest MSE_1_2\n",
      "                                     MSE_1_2\n",
      "Baseline Support Vector Regression  5.374631\n",
      "Final Support Vector Regression     5.294957\n",
      "Baseline Neural Network             5.284308\n",
      "Final Neural Network                5.311672\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "5.345882404970677\n",
      "  \n",
      "RMSE_1_2\n",
      "Baseline Random Forest RMSE_1_2\n",
      "                                    RMSE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression     2.301077\n",
      "Baseline Neural Network             2.298762\n",
      "Final Neural Network                2.304706\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "2.3121164341292757\n",
      "  \n",
      "R2_1_2\n",
      "Baseline Random Forest R2_1_2\n",
      "                                   R2_1_2\n",
      "Baseline Support Vector Regression    NaN\n",
      "Final Support Vector Regression       NaN\n",
      "Baseline Neural Network               NaN\n",
      "Final Neural Network                  NaN\n",
      "Baseline Random Forest               None\n",
      "Final Random Forest                   NaN\n",
      "metric R2\n",
      "nan\n",
      "  \n",
      "model Final Random Forest\n",
      "MAE_1_2\n",
      "Final Random Forest MAE_1_2\n",
      "                                     MAE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression     2.301077\n",
      "Baseline Neural Network             2.298762\n",
      "Final Neural Network                2.304706\n",
      "Baseline Random Forest              2.312116\n",
      "Final Random Forest                     None\n",
      "metric MAE\n",
      "yeee\n",
      "2.214662947679973\n",
      "  \n",
      "MSE_1_2\n",
      "Final Random Forest MSE_1_2\n",
      "                                     MSE_1_2\n",
      "Baseline Support Vector Regression  5.374631\n",
      "Final Support Vector Regression     5.294957\n",
      "Baseline Neural Network             5.284308\n",
      "Final Neural Network                5.311672\n",
      "Baseline Random Forest              5.345882\n",
      "Final Random Forest                     None\n",
      "metric MSE\n",
      "4.904731971826546\n",
      "  \n",
      "RMSE_1_2\n",
      "Final Random Forest RMSE_1_2\n",
      "                                    RMSE_1_2\n",
      "Baseline Support Vector Regression  2.318325\n",
      "Final Support Vector Regression     2.301077\n",
      "Baseline Neural Network             2.298762\n",
      "Final Neural Network                2.304706\n",
      "Baseline Random Forest              2.312116\n",
      "Final Random Forest                     None\n",
      "metric RMSE\n",
      "2.214662947679973\n",
      "  \n",
      "R2_1_2\n",
      "Final Random Forest R2_1_2\n",
      "                                   R2_1_2\n",
      "Baseline Support Vector Regression    NaN\n",
      "Final Support Vector Regression       NaN\n",
      "Baseline Neural Network               NaN\n",
      "Final Neural Network                  NaN\n",
      "Baseline Random Forest                NaN\n",
      "Final Random Forest                  None\n",
      "metric R2\n",
      "nan\n",
      "  \n",
      "2\n",
      "3\n",
      "subgroup exists\n",
      "model Baseline Support Vector Regression\n",
      "MAE_2_3\n",
      "Baseline Support Vector Regression MAE_2_3\n",
      "                                   MAE_2_3\n",
      "Baseline Support Vector Regression    None\n",
      "Final Support Vector Regression        NaN\n",
      "Baseline Neural Network                NaN\n",
      "Final Neural Network                   NaN\n",
      "Baseline Random Forest                 NaN\n",
      "Final Random Forest                    NaN\n",
      "metric MAE\n",
      "yeee\n",
      "1.4991951275959878\n",
      "  \n",
      "MSE_2_3\n",
      "Baseline Support Vector Regression MSE_2_3\n",
      "                                   MSE_2_3\n",
      "Baseline Support Vector Regression    None\n",
      "Final Support Vector Regression        NaN\n",
      "Baseline Neural Network                NaN\n",
      "Final Neural Network                   NaN\n",
      "Baseline Random Forest                 NaN\n",
      "Final Random Forest                    NaN\n",
      "metric MSE\n",
      "2.306463064212768\n",
      "  \n",
      "RMSE_2_3\n",
      "Baseline Support Vector Regression RMSE_2_3\n",
      "                                   RMSE_2_3\n",
      "Baseline Support Vector Regression     None\n",
      "Final Support Vector Regression         NaN\n",
      "Baseline Neural Network                 NaN\n",
      "Final Neural Network                    NaN\n",
      "Baseline Random Forest                  NaN\n",
      "Final Random Forest                     NaN\n",
      "metric RMSE\n",
      "1.5187044031715875\n",
      "  \n",
      "R2_2_3\n",
      "Baseline Support Vector Regression R2_2_3\n",
      "                                   R2_2_3\n",
      "Baseline Support Vector Regression   None\n",
      "Final Support Vector Regression       NaN\n",
      "Baseline Neural Network               NaN\n",
      "Final Neural Network                  NaN\n",
      "Baseline Random Forest                NaN\n",
      "Final Random Forest                   NaN\n",
      "metric R2\n",
      "-38.12368797386726\n",
      "  \n",
      "model Final Support Vector Regression\n",
      "MAE_2_3\n",
      "Final Support Vector Regression MAE_2_3\n",
      "                                     MAE_2_3\n",
      "Baseline Support Vector Regression  1.499195\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "1.495166648275538\n",
      "  \n",
      "MSE_2_3\n",
      "Final Support Vector Regression MSE_2_3\n",
      "                                     MSE_2_3\n",
      "Baseline Support Vector Regression  2.306463\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "2.2957519040894026\n",
      "  \n",
      "RMSE_2_3\n",
      "Final Support Vector Regression RMSE_2_3\n",
      "                                    RMSE_2_3\n",
      "Baseline Support Vector Regression  1.518704\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "1.5151738857601138\n",
      "  \n",
      "R2_2_3\n",
      "Final Support Vector Regression R2_2_3\n",
      "                                       R2_2_3\n",
      "Baseline Support Vector Regression -38.123688\n",
      "Final Support Vector Regression          None\n",
      "Baseline Neural Network                   NaN\n",
      "Final Neural Network                      NaN\n",
      "Baseline Random Forest                    NaN\n",
      "Final Random Forest                       NaN\n",
      "metric R2\n",
      "-37.94199848878214\n",
      "  \n",
      "model Baseline Neural Network\n",
      "MAE_2_3\n",
      "Baseline Neural Network MAE_2_3\n",
      "                                     MAE_2_3\n",
      "Baseline Support Vector Regression  1.499195\n",
      "Final Support Vector Regression     1.495167\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "1.4796373609934939\n",
      "  \n",
      "MSE_2_3\n",
      "Baseline Neural Network MSE_2_3\n",
      "                                     MSE_2_3\n",
      "Baseline Support Vector Regression  2.306463\n",
      "Final Support Vector Regression     2.295752\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "2.248279829625635\n",
      "  \n",
      "RMSE_2_3\n",
      "Baseline Neural Network RMSE_2_3\n",
      "                                    RMSE_2_3\n",
      "Baseline Support Vector Regression  1.518704\n",
      "Final Support Vector Regression     1.515174\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "1.4994265002412206\n",
      "  \n",
      "R2_2_3\n",
      "Baseline Neural Network R2_2_3\n",
      "                                       R2_2_3\n",
      "Baseline Support Vector Regression -38.123688\n",
      "Final Support Vector Regression    -37.941998\n",
      "Baseline Neural Network                  None\n",
      "Final Neural Network                      NaN\n",
      "Baseline Random Forest                    NaN\n",
      "Final Random Forest                       NaN\n",
      "metric R2\n",
      "-37.136747081287105\n",
      "  \n",
      "model Final Neural Network\n",
      "MAE_2_3\n",
      "Final Neural Network MAE_2_3\n",
      "                                     MAE_2_3\n",
      "Baseline Support Vector Regression  1.499195\n",
      "Final Support Vector Regression     1.495167\n",
      "Baseline Neural Network             1.479637\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "1.485581613007654\n",
      "  \n",
      "MSE_2_3\n",
      "Final Neural Network MSE_2_3\n",
      "                                     MSE_2_3\n",
      "Baseline Support Vector Regression  2.306463\n",
      "Final Support Vector Regression     2.295752\n",
      "Baseline Neural Network              2.24828\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "2.2659058384842674\n",
      "  \n",
      "RMSE_2_3\n",
      "Final Neural Network RMSE_2_3\n",
      "                                    RMSE_2_3\n",
      "Baseline Support Vector Regression  1.518704\n",
      "Final Support Vector Regression     1.515174\n",
      "Baseline Neural Network             1.499427\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "1.5052926089250114\n",
      "  \n",
      "R2_2_3\n",
      "Final Neural Network R2_2_3\n",
      "                                       R2_2_3\n",
      "Baseline Support Vector Regression -38.123688\n",
      "Final Support Vector Regression    -37.941998\n",
      "Baseline Neural Network            -37.136747\n",
      "Final Neural Network                     None\n",
      "Baseline Random Forest                    NaN\n",
      "Final Random Forest                       NaN\n",
      "metric R2\n",
      "-37.435730612178865\n",
      "  \n",
      "model Baseline Random Forest\n",
      "MAE_2_3\n",
      "Baseline Random Forest MAE_2_3\n",
      "                                     MAE_2_3\n",
      "Baseline Support Vector Regression  1.499195\n",
      "Final Support Vector Regression     1.495167\n",
      "Baseline Neural Network             1.479637\n",
      "Final Neural Network                1.485582\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "1.4949682538186864\n",
      "  \n",
      "MSE_2_3\n",
      "Baseline Random Forest MSE_2_3\n",
      "                                     MSE_2_3\n",
      "Baseline Support Vector Regression  2.306463\n",
      "Final Support Vector Regression     2.295752\n",
      "Baseline Neural Network              2.24828\n",
      "Final Neural Network                2.265906\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "2.2971621009580327\n",
      "  \n",
      "RMSE_2_3\n",
      "Baseline Random Forest RMSE_2_3\n",
      "                                    RMSE_2_3\n",
      "Baseline Support Vector Regression  1.518704\n",
      "Final Support Vector Regression     1.515174\n",
      "Baseline Neural Network             1.499427\n",
      "Final Neural Network                1.505293\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "1.5156391724147382\n",
      "  \n",
      "R2_2_3\n",
      "Baseline Random Forest R2_2_3\n",
      "                                       R2_2_3\n",
      "Baseline Support Vector Regression -38.123688\n",
      "Final Support Vector Regression    -37.941998\n",
      "Baseline Neural Network            -37.136747\n",
      "Final Neural Network               -37.435731\n",
      "Baseline Random Forest                   None\n",
      "Final Random Forest                       NaN\n",
      "metric R2\n",
      "-37.965919141631986\n",
      "  \n",
      "model Final Random Forest\n",
      "MAE_2_3\n",
      "Final Random Forest MAE_2_3\n",
      "                                     MAE_2_3\n",
      "Baseline Support Vector Regression  1.499195\n",
      "Final Support Vector Regression     1.495167\n",
      "Baseline Neural Network             1.479637\n",
      "Final Neural Network                1.485582\n",
      "Baseline Random Forest              1.494968\n",
      "Final Random Forest                     None\n",
      "metric MAE\n",
      "yeee\n",
      "1.4300972582111702\n",
      "  \n",
      "MSE_2_3\n",
      "Final Random Forest MSE_2_3\n",
      "                                     MSE_2_3\n",
      "Baseline Support Vector Regression  2.306463\n",
      "Final Support Vector Regression     2.295752\n",
      "Baseline Neural Network              2.24828\n",
      "Final Neural Network                2.265906\n",
      "Baseline Random Forest              2.297162\n",
      "Final Random Forest                     None\n",
      "metric MSE\n",
      "2.125810934305442\n",
      "  \n",
      "RMSE_2_3\n",
      "Final Random Forest RMSE_2_3\n",
      "                                    RMSE_2_3\n",
      "Baseline Support Vector Regression  1.518704\n",
      "Final Support Vector Regression     1.515174\n",
      "Baseline Neural Network             1.499427\n",
      "Final Neural Network                1.505293\n",
      "Baseline Random Forest              1.515639\n",
      "Final Random Forest                     None\n",
      "metric RMSE\n",
      "1.4580160953519827\n",
      "  \n",
      "R2_2_3\n",
      "Final Random Forest R2_2_3\n",
      "                                       R2_2_3\n",
      "Baseline Support Vector Regression -38.123688\n",
      "Final Support Vector Regression    -37.941998\n",
      "Baseline Neural Network            -37.136747\n",
      "Final Neural Network               -37.435731\n",
      "Baseline Random Forest             -37.965919\n",
      "Final Random Forest                      None\n",
      "metric R2\n",
      "-35.05935207706803\n",
      "  \n",
      "3\n",
      "4\n",
      "subgroup exists\n",
      "model Baseline Support Vector Regression\n",
      "MAE_3_4\n",
      "Baseline Support Vector Regression MAE_3_4\n",
      "                                   MAE_3_4\n",
      "Baseline Support Vector Regression    None\n",
      "Final Support Vector Regression        NaN\n",
      "Baseline Neural Network                NaN\n",
      "Final Neural Network                   NaN\n",
      "Baseline Random Forest                 NaN\n",
      "Final Random Forest                    NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.49268265860890587\n",
      "  \n",
      "MSE_3_4\n",
      "Baseline Support Vector Regression MSE_3_4\n",
      "                                   MSE_3_4\n",
      "Baseline Support Vector Regression    None\n",
      "Final Support Vector Regression        NaN\n",
      "Baseline Neural Network                NaN\n",
      "Final Neural Network                   NaN\n",
      "Baseline Random Forest                 NaN\n",
      "Final Random Forest                    NaN\n",
      "metric MSE\n",
      "0.29628500174419703\n",
      "  \n",
      "RMSE_3_4\n",
      "Baseline Support Vector Regression RMSE_3_4\n",
      "                                   RMSE_3_4\n",
      "Baseline Support Vector Regression     None\n",
      "Final Support Vector Regression         NaN\n",
      "Baseline Neural Network                 NaN\n",
      "Final Neural Network                    NaN\n",
      "Baseline Random Forest                  NaN\n",
      "Final Random Forest                     NaN\n",
      "metric RMSE\n",
      "0.5443206791443781\n",
      "  \n",
      "R2_3_4\n",
      "Baseline Support Vector Regression R2_3_4\n",
      "                                   R2_3_4\n",
      "Baseline Support Vector Regression   None\n",
      "Final Support Vector Regression       NaN\n",
      "Baseline Neural Network               NaN\n",
      "Final Neural Network                  NaN\n",
      "Baseline Random Forest                NaN\n",
      "Final Random Forest                   NaN\n",
      "metric R2\n",
      "-4.531687604104572\n",
      "  \n",
      "model Final Support Vector Regression\n",
      "MAE_3_4\n",
      "Final Support Vector Regression MAE_3_4\n",
      "                                     MAE_3_4\n",
      "Baseline Support Vector Regression  0.492683\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.4881116094714791\n",
      "  \n",
      "MSE_3_4\n",
      "Final Support Vector Regression MSE_3_4\n",
      "                                     MSE_3_4\n",
      "Baseline Support Vector Regression  0.296285\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "0.29161345495184077\n",
      "  \n",
      "RMSE_3_4\n",
      "Final Support Vector Regression RMSE_3_4\n",
      "                                    RMSE_3_4\n",
      "Baseline Support Vector Regression  0.544321\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "0.5400124581450327\n",
      "  \n",
      "R2_3_4\n",
      "Final Support Vector Regression R2_3_4\n",
      "                                      R2_3_4\n",
      "Baseline Support Vector Regression -4.531688\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric R2\n",
      "-4.4444690904061215\n",
      "  \n",
      "model Baseline Neural Network\n",
      "MAE_3_4\n",
      "Baseline Neural Network MAE_3_4\n",
      "                                     MAE_3_4\n",
      "Baseline Support Vector Regression  0.492683\n",
      "Final Support Vector Regression     0.488112\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.47283924622853707\n",
      "  \n",
      "MSE_3_4\n",
      "Baseline Neural Network MSE_3_4\n",
      "                                     MSE_3_4\n",
      "Baseline Support Vector Regression  0.296285\n",
      "Final Support Vector Regression     0.291613\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "0.2771383653028258\n",
      "  \n",
      "RMSE_3_4\n",
      "Baseline Neural Network RMSE_3_4\n",
      "                                    RMSE_3_4\n",
      "Baseline Support Vector Regression  0.544321\n",
      "Final Support Vector Regression     0.540012\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "0.5264393272760175\n",
      "  \n",
      "R2_3_4\n",
      "Baseline Neural Network R2_3_4\n",
      "                                      R2_3_4\n",
      "Baseline Support Vector Regression -4.531688\n",
      "Final Support Vector Regression    -4.444469\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric R2\n",
      "-4.1742168889501405\n",
      "  \n",
      "model Final Neural Network\n",
      "MAE_3_4\n",
      "Final Neural Network MAE_3_4\n",
      "                                     MAE_3_4\n",
      "Baseline Support Vector Regression  0.492683\n",
      "Final Support Vector Regression     0.488112\n",
      "Baseline Neural Network             0.472839\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.4787834982426972\n",
      "  \n",
      "MSE_3_4\n",
      "Final Neural Network MSE_3_4\n",
      "                                     MSE_3_4\n",
      "Baseline Support Vector Regression  0.296285\n",
      "Final Support Vector Regression     0.291613\n",
      "Baseline Neural Network             0.277138\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "0.2827950507183696\n",
      "  \n",
      "RMSE_3_4\n",
      "Final Neural Network RMSE_3_4\n",
      "                                    RMSE_3_4\n",
      "Baseline Support Vector Regression  0.544321\n",
      "Final Support Vector Regression     0.540012\n",
      "Baseline Neural Network             0.526439\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "0.531784778569648\n",
      "  \n",
      "R2_3_4\n",
      "Final Neural Network R2_3_4\n",
      "                                      R2_3_4\n",
      "Baseline Support Vector Regression -4.531688\n",
      "Final Support Vector Regression    -4.444469\n",
      "Baseline Neural Network            -4.174217\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric R2\n",
      "-4.279828095758705\n",
      "  \n",
      "model Baseline Random Forest\n",
      "MAE_3_4\n",
      "Baseline Random Forest MAE_3_4\n",
      "                                     MAE_3_4\n",
      "Baseline Support Vector Regression  0.492683\n",
      "Final Support Vector Regression     0.488112\n",
      "Baseline Neural Network             0.472839\n",
      "Final Neural Network                0.478783\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.46685798063669526\n",
      "  \n",
      "MSE_3_4\n",
      "Baseline Random Forest MSE_3_4\n",
      "                                     MSE_3_4\n",
      "Baseline Support Vector Regression  0.296285\n",
      "Final Support Vector Regression     0.291613\n",
      "Baseline Neural Network             0.277138\n",
      "Final Neural Network                0.282795\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "0.27361038923839043\n",
      "  \n",
      "RMSE_3_4\n",
      "Baseline Random Forest RMSE_3_4\n",
      "                                    RMSE_3_4\n",
      "Baseline Support Vector Regression  0.544321\n",
      "Final Support Vector Regression     0.540012\n",
      "Baseline Neural Network             0.526439\n",
      "Final Neural Network                0.531785\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "0.5230778041920632\n",
      "  \n",
      "R2_3_4\n",
      "Baseline Random Forest R2_3_4\n",
      "                                      R2_3_4\n",
      "Baseline Support Vector Regression -4.531688\n",
      "Final Support Vector Regression    -4.444469\n",
      "Baseline Neural Network            -4.174217\n",
      "Final Neural Network               -4.279828\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric R2\n",
      "-4.108349020687055\n",
      "  \n",
      "model Final Random Forest\n",
      "MAE_3_4\n",
      "Final Random Forest MAE_3_4\n",
      "                                     MAE_3_4\n",
      "Baseline Support Vector Regression  0.492683\n",
      "Final Support Vector Regression     0.488112\n",
      "Baseline Neural Network             0.472839\n",
      "Final Neural Network                0.478783\n",
      "Baseline Random Forest              0.466858\n",
      "Final Random Forest                     None\n",
      "metric MAE\n",
      "yeee\n",
      "0.4264403082553566\n",
      "  \n",
      "MSE_3_4\n",
      "Final Random Forest MSE_3_4\n",
      "                                     MSE_3_4\n",
      "Baseline Support Vector Regression  0.296285\n",
      "Final Support Vector Regression     0.291613\n",
      "Baseline Neural Network             0.277138\n",
      "Final Neural Network                0.282795\n",
      "Baseline Random Forest               0.27361\n",
      "Final Random Forest                     None\n",
      "metric MSE\n",
      "0.2373349624730515\n",
      "  \n",
      "RMSE_3_4\n",
      "Final Random Forest RMSE_3_4\n",
      "                                    RMSE_3_4\n",
      "Baseline Support Vector Regression  0.544321\n",
      "Final Support Vector Regression     0.540012\n",
      "Baseline Neural Network             0.526439\n",
      "Final Neural Network                0.531785\n",
      "Baseline Random Forest              0.523078\n",
      "Final Random Forest                     None\n",
      "metric RMSE\n",
      "0.487170362884537\n",
      "  \n",
      "R2_3_4\n",
      "Final Random Forest R2_3_4\n",
      "                                      R2_3_4\n",
      "Baseline Support Vector Regression -4.531688\n",
      "Final Support Vector Regression    -4.444469\n",
      "Baseline Neural Network            -4.174217\n",
      "Final Neural Network               -4.279828\n",
      "Baseline Random Forest             -4.108349\n",
      "Final Random Forest                     None\n",
      "metric R2\n",
      "-3.431081094905662\n",
      "  \n",
      "4\n",
      "5\n",
      "subgroup exists\n",
      "model Baseline Support Vector Regression\n",
      "MAE_4_5\n",
      "Baseline Support Vector Regression MAE_4_5\n",
      "                                   MAE_4_5\n",
      "Baseline Support Vector Regression    None\n",
      "Final Support Vector Regression        NaN\n",
      "Baseline Neural Network                NaN\n",
      "Final Neural Network                   NaN\n",
      "Baseline Random Forest                 NaN\n",
      "Final Random Forest                    NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.092997117476766\n",
      "  \n",
      "MSE_4_5\n",
      "Baseline Support Vector Regression MSE_4_5\n",
      "                                   MSE_4_5\n",
      "Baseline Support Vector Regression    None\n",
      "Final Support Vector Regression        NaN\n",
      "Baseline Neural Network                NaN\n",
      "Final Neural Network                   NaN\n",
      "Baseline Random Forest                 NaN\n",
      "Final Random Forest                    NaN\n",
      "metric MSE\n",
      "0.015031878309444684\n",
      "  \n",
      "RMSE_4_5\n",
      "Baseline Support Vector Regression RMSE_4_5\n",
      "                                   RMSE_4_5\n",
      "Baseline Support Vector Regression     None\n",
      "Final Support Vector Regression         NaN\n",
      "Baseline Neural Network                 NaN\n",
      "Final Neural Network                    NaN\n",
      "Baseline Random Forest                  NaN\n",
      "Final Random Forest                     NaN\n",
      "metric RMSE\n",
      "0.12260456072040993\n",
      "  \n",
      "R2_4_5\n",
      "Baseline Support Vector Regression R2_4_5\n",
      "                                   R2_4_5\n",
      "Baseline Support Vector Regression   None\n",
      "Final Support Vector Regression       NaN\n",
      "Baseline Neural Network               NaN\n",
      "Final Neural Network                  NaN\n",
      "Baseline Random Forest                NaN\n",
      "Final Random Forest                   NaN\n",
      "metric R2\n",
      "-0.06150084410644863\n",
      "  \n",
      "model Final Support Vector Regression\n",
      "MAE_4_5\n",
      "Final Support Vector Regression MAE_4_5\n",
      "                                     MAE_4_5\n",
      "Baseline Support Vector Regression  0.092997\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.09306806286037048\n",
      "  \n",
      "MSE_4_5\n",
      "Final Support Vector Regression MSE_4_5\n",
      "                                     MSE_4_5\n",
      "Baseline Support Vector Regression  0.015032\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "0.015016571751119081\n",
      "  \n",
      "RMSE_4_5\n",
      "Final Support Vector Regression RMSE_4_5\n",
      "                                    RMSE_4_5\n",
      "Baseline Support Vector Regression  0.122605\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "0.12254212235439323\n",
      "  \n",
      "R2_4_5\n",
      "Final Support Vector Regression R2_4_5\n",
      "                                      R2_4_5\n",
      "Baseline Support Vector Regression -0.061501\n",
      "Final Support Vector Regression         None\n",
      "Baseline Neural Network                  NaN\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric R2\n",
      "-0.060419946280607206\n",
      "  \n",
      "model Baseline Neural Network\n",
      "MAE_4_5\n",
      "Baseline Neural Network MAE_4_5\n",
      "                                     MAE_4_5\n",
      "Baseline Support Vector Regression  0.092997\n",
      "Final Support Vector Regression     0.093068\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.09997042206873996\n",
      "  \n",
      "MSE_4_5\n",
      "Baseline Neural Network MSE_4_5\n",
      "                                     MSE_4_5\n",
      "Baseline Support Vector Regression  0.015032\n",
      "Final Support Vector Regression     0.015017\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "0.01658854336210662\n",
      "  \n",
      "RMSE_4_5\n",
      "Baseline Neural Network RMSE_4_5\n",
      "                                    RMSE_4_5\n",
      "Baseline Support Vector Regression  0.122605\n",
      "Final Support Vector Regression     0.122542\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "0.12879651921580265\n",
      "  \n",
      "R2_4_5\n",
      "Baseline Neural Network R2_4_5\n",
      "                                      R2_4_5\n",
      "Baseline Support Vector Regression -0.061501\n",
      "Final Support Vector Regression     -0.06042\n",
      "Baseline Neural Network                 None\n",
      "Final Neural Network                     NaN\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric R2\n",
      "-0.17142731060487915\n",
      "  \n",
      "model Final Neural Network\n",
      "MAE_4_5\n",
      "Final Neural Network MAE_4_5\n",
      "                                     MAE_4_5\n",
      "Baseline Support Vector Regression  0.092997\n",
      "Final Support Vector Regression     0.093068\n",
      "Baseline Neural Network              0.09997\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.09754118972928702\n",
      "  \n",
      "MSE_4_5\n",
      "Final Neural Network MSE_4_5\n",
      "                                     MSE_4_5\n",
      "Baseline Support Vector Regression  0.015032\n",
      "Final Support Vector Regression     0.015017\n",
      "Baseline Neural Network             0.016589\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "0.016038125644108135\n",
      "  \n",
      "RMSE_4_5\n",
      "Final Neural Network RMSE_4_5\n",
      "                                    RMSE_4_5\n",
      "Baseline Support Vector Regression  0.122605\n",
      "Final Support Vector Regression     0.122542\n",
      "Baseline Neural Network             0.128797\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "0.1266417215774807\n",
      "  \n",
      "R2_4_5\n",
      "Final Neural Network R2_4_5\n",
      "                                      R2_4_5\n",
      "Baseline Support Vector Regression -0.061501\n",
      "Final Support Vector Regression     -0.06042\n",
      "Baseline Neural Network            -0.171427\n",
      "Final Neural Network                    None\n",
      "Baseline Random Forest                   NaN\n",
      "Final Random Forest                      NaN\n",
      "metric R2\n",
      "-0.1325586569184376\n",
      "  \n",
      "model Baseline Random Forest\n",
      "MAE_4_5\n",
      "Baseline Random Forest MAE_4_5\n",
      "                                     MAE_4_5\n",
      "Baseline Support Vector Regression  0.092997\n",
      "Final Support Vector Regression     0.093068\n",
      "Baseline Neural Network              0.09997\n",
      "Final Neural Network                0.097541\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric MAE\n",
      "yeee\n",
      "0.10765387931843763\n",
      "  \n",
      "MSE_4_5\n",
      "Baseline Random Forest MSE_4_5\n",
      "                                     MSE_4_5\n",
      "Baseline Support Vector Regression  0.015032\n",
      "Final Support Vector Regression     0.015017\n",
      "Baseline Neural Network             0.016589\n",
      "Final Neural Network                0.016038\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric MSE\n",
      "0.019511859227985585\n",
      "  \n",
      "RMSE_4_5\n",
      "Baseline Random Forest RMSE_4_5\n",
      "                                    RMSE_4_5\n",
      "Baseline Support Vector Regression  0.122605\n",
      "Final Support Vector Regression     0.122542\n",
      "Baseline Neural Network             0.128797\n",
      "Final Neural Network                0.126642\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric RMSE\n",
      "0.1396848568313172\n",
      "  \n",
      "R2_4_5\n",
      "Baseline Random Forest R2_4_5\n",
      "                                      R2_4_5\n",
      "Baseline Support Vector Regression -0.061501\n",
      "Final Support Vector Regression     -0.06042\n",
      "Baseline Neural Network            -0.171427\n",
      "Final Neural Network               -0.132559\n",
      "Baseline Random Forest                  None\n",
      "Final Random Forest                      NaN\n",
      "metric R2\n",
      "-0.37786207513264847\n",
      "  \n",
      "model Final Random Forest\n",
      "MAE_4_5\n",
      "Final Random Forest MAE_4_5\n",
      "                                     MAE_4_5\n",
      "Baseline Support Vector Regression  0.092997\n",
      "Final Support Vector Regression     0.093068\n",
      "Baseline Neural Network              0.09997\n",
      "Final Neural Network                0.097541\n",
      "Baseline Random Forest              0.107654\n",
      "Final Random Forest                     None\n",
      "metric MAE\n",
      "yeee\n",
      "0.10068630512978591\n",
      "  \n",
      "MSE_4_5\n",
      "Final Random Forest MSE_4_5\n",
      "                                     MSE_4_5\n",
      "Baseline Support Vector Regression  0.015032\n",
      "Final Support Vector Regression     0.015017\n",
      "Baseline Neural Network             0.016589\n",
      "Final Neural Network                0.016038\n",
      "Baseline Random Forest              0.019512\n",
      "Final Random Forest                     None\n",
      "metric MSE\n",
      "0.01777779566266916\n",
      "  \n",
      "RMSE_4_5\n",
      "Final Random Forest RMSE_4_5\n",
      "                                    RMSE_4_5\n",
      "Baseline Support Vector Regression  0.122605\n",
      "Final Support Vector Regression     0.122542\n",
      "Baseline Neural Network             0.128797\n",
      "Final Neural Network                0.126642\n",
      "Baseline Random Forest              0.139685\n",
      "Final Random Forest                     None\n",
      "metric RMSE\n",
      "0.13333340040165914\n",
      "  \n",
      "R2_4_5\n",
      "Final Random Forest R2_4_5\n",
      "                                      R2_4_5\n",
      "Baseline Support Vector Regression -0.061501\n",
      "Final Support Vector Regression     -0.06042\n",
      "Baseline Neural Network            -0.171427\n",
      "Final Neural Network               -0.132559\n",
      "Baseline Random Forest             -0.377862\n",
      "Final Random Forest                     None\n",
      "metric R2\n",
      "-0.25540832049035056\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "# Loop through each range of ratings\n",
    "i = 0\n",
    "for gr in range(1,5):\n",
    "\n",
    "    print(gr)\n",
    "    print(gr+1)\n",
    "\n",
    "    # get dataset splits\n",
    "    y_sub_group = predictions[(predictions['weighted rating'] >= gr) & (predictions['weighted rating'] < gr +1)]\n",
    "    #print(y_sub_group)\n",
    "\n",
    "    if y_sub_group.shape[0] != 0:\n",
    "        # Loop though each model prediction and calculate metric\n",
    "\n",
    "        print('subgroup exists')\n",
    "        for mod in models:\n",
    "            \n",
    "            print('model ' + mod)\n",
    "            # identify prediction\n",
    "            p = y_sub_group[mod]\n",
    "            #print(p)\n",
    "\n",
    "            #calculate metric\n",
    "            for metr in metrics.index:\n",
    "                col_name = metr+\"_\"+str(gr)+\"_\"+str(gr+1)\n",
    "                print(col_name)\n",
    "                print(mod, col_name)\n",
    "                evaluation_metrics_all.at[mod, col_name] = None\n",
    "                print(evaluation_metrics_all[[col_name]])\n",
    "\n",
    "                print('metric ' +metr)\n",
    "                \n",
    "                if metr == 'MAE':\n",
    "                    print('yeee')\n",
    "                    cal_metric = mean_absolute_error(y_sub_group['weighted rating'], p)\n",
    "                    print(cal_metric)\n",
    "\n",
    "                if metr == 'MSE':\n",
    "                    cal_metric = mean_squared_error(y_sub_group['weighted rating'], p)\n",
    "                    print(cal_metric)\n",
    "\n",
    "                if metr == 'RMSE':\n",
    "                    cal_metric = root_mean_squared_error(y_sub_group['weighted rating'], p)\n",
    "                    print(cal_metric)\n",
    "\n",
    "                if metr == 'R2':\n",
    "                    cal_metric = r2_score(y_sub_group['weighted rating'], p)\n",
    "                    print(cal_metric)\n",
    "            \n",
    "                # Add calculated_metric to evaluation_metrics_all\n",
    "                evaluation_metrics_all.at[mod, col_name] = cal_metric\n",
    "\n",
    "                print(\"  \")\n",
    "                i = 1+i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE_1_2</th>\n",
       "      <th>MSE_1_2</th>\n",
       "      <th>...</th>\n",
       "      <th>RMSE_2_3</th>\n",
       "      <th>R2_2_3</th>\n",
       "      <th>MAE_3_4</th>\n",
       "      <th>MSE_3_4</th>\n",
       "      <th>RMSE_3_4</th>\n",
       "      <th>R2_3_4</th>\n",
       "      <th>MAE_4_5</th>\n",
       "      <th>MSE_4_5</th>\n",
       "      <th>RMSE_4_5</th>\n",
       "      <th>R2_4_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[4.2630513149316505, 4.2635265104359625, 4.262...</td>\n",
       "      <td>0.131492</td>\n",
       "      <td>0.04526</td>\n",
       "      <td>0.212743</td>\n",
       "      <td>-0.00911</td>\n",
       "      <td>2.318325</td>\n",
       "      <td>5.374631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.518704</td>\n",
       "      <td>-38.123688</td>\n",
       "      <td>0.492683</td>\n",
       "      <td>0.296285</td>\n",
       "      <td>0.544321</td>\n",
       "      <td>-4.531688</td>\n",
       "      <td>0.092997</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.122605</td>\n",
       "      <td>-0.061501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.270183758646303, 4.251684087400329, 4.26068...</td>\n",
       "      <td>0.13114</td>\n",
       "      <td>0.044796</td>\n",
       "      <td>0.211652</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>2.301077</td>\n",
       "      <td>5.294957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.515174</td>\n",
       "      <td>-37.941998</td>\n",
       "      <td>0.488112</td>\n",
       "      <td>0.291613</td>\n",
       "      <td>0.540012</td>\n",
       "      <td>-4.444469</td>\n",
       "      <td>0.093068</td>\n",
       "      <td>0.015017</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>-0.06042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_1, built=True&gt;</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[[4.2431197], [4.2431197], [4.2431197], [4.243...</td>\n",
       "      <td>0.136032</td>\n",
       "      <td>0.044851</td>\n",
       "      <td>0.211781</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>2.298762</td>\n",
       "      <td>5.284308</td>\n",
       "      <td>...</td>\n",
       "      <td>1.499427</td>\n",
       "      <td>-37.136747</td>\n",
       "      <td>0.472839</td>\n",
       "      <td>0.277138</td>\n",
       "      <td>0.526439</td>\n",
       "      <td>-4.174217</td>\n",
       "      <td>0.09997</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.128797</td>\n",
       "      <td>-0.171427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_2, built=True&gt;</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[[4.249064], [4.249064], [4.249064], [4.249064...</td>\n",
       "      <td>0.134362</td>\n",
       "      <td>0.04489</td>\n",
       "      <td>0.211874</td>\n",
       "      <td>-0.000879</td>\n",
       "      <td>2.304706</td>\n",
       "      <td>5.311672</td>\n",
       "      <td>...</td>\n",
       "      <td>1.505293</td>\n",
       "      <td>-37.435731</td>\n",
       "      <td>0.478783</td>\n",
       "      <td>0.282795</td>\n",
       "      <td>0.531785</td>\n",
       "      <td>-4.279828</td>\n",
       "      <td>0.097541</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>0.126642</td>\n",
       "      <td>-0.132559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest</th>\n",
       "      <td>((DecisionTreeRegressor(max_features=1.0, rand...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>NMF_0     NMF_1     NMF_2     NMF_3...</td>\n",
       "      <td>[4.237389858905316, 4.268050725923043, 4.25435...</td>\n",
       "      <td>0.142518</td>\n",
       "      <td>0.047299</td>\n",
       "      <td>0.217483</td>\n",
       "      <td>-0.05458</td>\n",
       "      <td>2.312116</td>\n",
       "      <td>5.345882</td>\n",
       "      <td>...</td>\n",
       "      <td>1.515639</td>\n",
       "      <td>-37.965919</td>\n",
       "      <td>0.466858</td>\n",
       "      <td>0.27361</td>\n",
       "      <td>0.523078</td>\n",
       "      <td>-4.108349</td>\n",
       "      <td>0.107654</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>0.139685</td>\n",
       "      <td>-0.377862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Random Forest</th>\n",
       "      <td>((DecisionTreeRegressor(max_features=1.0, rand...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.311126782469619, 4.17027010139218, 4.307437...</td>\n",
       "      <td>0.132459</td>\n",
       "      <td>0.042102</td>\n",
       "      <td>0.205188</td>\n",
       "      <td>0.061287</td>\n",
       "      <td>2.214663</td>\n",
       "      <td>4.904732</td>\n",
       "      <td>...</td>\n",
       "      <td>1.458016</td>\n",
       "      <td>-35.059352</td>\n",
       "      <td>0.42644</td>\n",
       "      <td>0.237335</td>\n",
       "      <td>0.48717</td>\n",
       "      <td>-3.431081</td>\n",
       "      <td>0.100686</td>\n",
       "      <td>0.017778</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>-0.255408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                model  \\\n",
       "Baseline Support Vector Regression                                            (SVR())   \n",
       "Final Support Vector Regression                                               (SVR())   \n",
       "Baseline Neural Network                    <Sequential name=sequential_1, built=True>   \n",
       "Final Neural Network                       <Sequential name=sequential_2, built=True>   \n",
       "Baseline Random Forest              ((DecisionTreeRegressor(max_features=1.0, rand...   \n",
       "Final Random Forest                 ((DecisionTreeRegressor(max_features=1.0, rand...   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                        NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "Baseline Random Forest                         NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Random Forest                          image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                        NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "Baseline Random Forest                         NMF_0     NMF_1     NMF_2     NMF_3...   \n",
       "Final Random Forest                          image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.2630513149316505, 4.2635265104359625, 4.262...   \n",
       "Final Support Vector Regression     [4.270183758646303, 4.251684087400329, 4.26068...   \n",
       "Baseline Neural Network             [[4.2431197], [4.2431197], [4.2431197], [4.243...   \n",
       "Final Neural Network                [[4.249064], [4.249064], [4.249064], [4.249064...   \n",
       "Baseline Random Forest              [4.237389858905316, 4.268050725923043, 4.25435...   \n",
       "Final Random Forest                 [4.311126782469619, 4.17027010139218, 4.307437...   \n",
       "\n",
       "                                         MAE       MSE      RMSE        R2  \\\n",
       "Baseline Support Vector Regression  0.131492   0.04526  0.212743  -0.00911   \n",
       "Final Support Vector Regression      0.13114  0.044796  0.211652  0.001217   \n",
       "Baseline Neural Network             0.136032  0.044851  0.211781 -0.000003   \n",
       "Final Neural Network                0.134362   0.04489  0.211874 -0.000879   \n",
       "Baseline Random Forest              0.142518  0.047299  0.217483  -0.05458   \n",
       "Final Random Forest                 0.132459  0.042102  0.205188  0.061287   \n",
       "\n",
       "                                     MAE_1_2   MSE_1_2  ...  RMSE_2_3  \\\n",
       "Baseline Support Vector Regression  2.318325  5.374631  ...  1.518704   \n",
       "Final Support Vector Regression     2.301077  5.294957  ...  1.515174   \n",
       "Baseline Neural Network             2.298762  5.284308  ...  1.499427   \n",
       "Final Neural Network                2.304706  5.311672  ...  1.505293   \n",
       "Baseline Random Forest              2.312116  5.345882  ...  1.515639   \n",
       "Final Random Forest                 2.214663  4.904732  ...  1.458016   \n",
       "\n",
       "                                       R2_2_3   MAE_3_4   MSE_3_4  RMSE_3_4  \\\n",
       "Baseline Support Vector Regression -38.123688  0.492683  0.296285  0.544321   \n",
       "Final Support Vector Regression    -37.941998  0.488112  0.291613  0.540012   \n",
       "Baseline Neural Network            -37.136747  0.472839  0.277138  0.526439   \n",
       "Final Neural Network               -37.435731  0.478783  0.282795  0.531785   \n",
       "Baseline Random Forest             -37.965919  0.466858   0.27361  0.523078   \n",
       "Final Random Forest                -35.059352   0.42644  0.237335   0.48717   \n",
       "\n",
       "                                      R2_3_4   MAE_4_5   MSE_4_5  RMSE_4_5  \\\n",
       "Baseline Support Vector Regression -4.531688  0.092997  0.015032  0.122605   \n",
       "Final Support Vector Regression    -4.444469  0.093068  0.015017  0.122542   \n",
       "Baseline Neural Network            -4.174217   0.09997  0.016589  0.128797   \n",
       "Final Neural Network               -4.279828  0.097541  0.016038  0.126642   \n",
       "Baseline Random Forest             -4.108349  0.107654  0.019512  0.139685   \n",
       "Final Random Forest                -3.431081  0.100686  0.017778  0.133333   \n",
       "\n",
       "                                      R2_4_5  \n",
       "Baseline Support Vector Regression -0.061501  \n",
       "Final Support Vector Regression     -0.06042  \n",
       "Baseline Neural Network            -0.171427  \n",
       "Final Neural Network               -0.132559  \n",
       "Baseline Random Forest             -0.377862  \n",
       "Final Random Forest                -0.255408  \n",
       "\n",
       "[6 rows x 24 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_all.drop(['X_train', 'X_test', 'prediction'], axis = 1).to_excel(\n",
    "    os.path.join(output_folder, 'evaluation_metrics.xlsx'),\n",
    "    sheet_name='evaluation_metrics',\n",
    "    index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted ratings exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted ratings overall distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJgAAAMUCAYAAADwgZ2dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAACljElEQVR4nOzdeVgVZf/H8c9hxxUERRHTNMFURNww01zK9qfU0tRKza3MpU0z0lxz1xaXUktNy55cK7WyxXY1F1KsDJdMJdxARUR2mN8fPMzPE6DggAf0/bouLz1z33POd2buc458mLnHZhiGIQAAAAAAAOAKOTm6AAAAAAAAAJRuBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETABQgv3zzz8KCgrK809wcLBatmyp7t27691339X58+fzfI5t27YpKChIt912m+V6UlJSdOzYsUKt89JLLykoKEivv/66uWzt2rUKCgpSjx49LNdUEJmZmTpy5Ijdsqtdw9Xw1VdfqVOnTgoJCVGLFi306quvXrJ/hw4dFBQUpC1btlylCguuqI9PYcdAXmM9r7HsaI8//riCgoK0atUqR5dS7K6nbbWqqMaqle+PK/m+uBIlaVz89ddfdo8v/g7PyMhwUFUAcPUQMAFAKdGwYUM1adLE/BMYGCgPDw/t2rVLM2bM0AMPPKC9e/cW2+v/+OOPuu+++/TLL78U22sUhz179qhLly5at26do0spVpGRkRo2bJj+/PNPeXl5KSAgQP7+/o4uq0Qo7BgorWMdKCmut/fQqVOnNHToUI0bN87RpQCAQ7k4ugAAQMG8+eabCggIyLU8OjpaEyZM0I8//qiBAwdq1apVqlatmtneqFEjff7553J1dbX0+m+//bb++eefQq/3/PPPa8CAAfL29rb0+ldq2bJlioqK0h133GG3vGPHjgoJCZGnp6dD6ipqGzdulGEYat68uZYuXSpnZ2dHl1RiFHYMXOlYB0oKR3/uXm/voR9++EFfffWVWrRoYbfcz89Pn3/+uSTJxYUfuwBc+/ikA4BSrkaNGnr77bfVu3dv7dy5U9OnT7e7LMLT01N16tRxWH1VqlRRlSpVHPb6+SlfvrzKly/v6DKKzNmzZyVJoaGhhEsFdK2NASBHSf3cvd64uro69PsXAK42LpEDgGuAi4uLeWr+xo0bFR0d7diCcNXlzO/h5ubm4EoAAABwPSJgAoBrRN26dRUSEqKsrCz98MMP5vL8Jmk1DEMrVqxQ9+7dFRoaquDgYHXo0EHh4eE6ePBgrvV//fVXSVJ4eLiCgoK0du1aSf8/meymTZs0a9YsNW/eXKGhoXr88ceVmZl52clmY2Ji9Pzzz6tFixYKDQ1V9+7d85wr53KTzf57wuacyVXXr18vSZo7d66CgoI0Z86cPPtf7OzZs3r99dd17733qlGjRmratKm6d++uFStWKDMzM1f/oKAghYWFKSsrS8uXL9eDDz5oTrT91FNPac+ePXnWfCnHjx/Xq6++qo4dO6phw4Zq0aKF+vTpoy+++CLP7f73dnbo0KFQr7dy5Ur95z//UXBwsNq2basxY8bo5MmT+fb/7LPP1Lt3bzVv3lzBwcHq2LGjJk2adMl1fvrpJz355JMKCwtTw4YN1a5dO7388sv6+++/C1xnSkqKHnvsMQUFBalz5846d+5cvn0LOwYuN9YvJTY2VpMmTVLHjh0VHByssLAw9e/fXz///HOe/RMTE/X666/rvvvuU3BwsEJDQ/Wf//xHr732mnk2WmF9/fXXevjhh9WoUSO1atVKw4cPzzXpcI6MjAytWbNG/fr1U6tWrdSwYUM1a9ZMXbt21ZIlS5SWlpZrnSNHjig8PFzt27dXw4YN1bx5c/Xs2VPLly9Xenp6nq/zxx9/6LnnnlPr1q3VsGFDtWnTRiNHjtShQ4fy3Y4vvvhCPXr0UNOmTRUWFqbnn3/e0mTRhRmrOZ9X3333nfbs2aOnnnpKYWFhatSokTp16qQPPvhAWVlZl33NAwcOKCgoSKGhobn2TWJioho0aKCgoKA8P+uee+45u3GbY+PGjerTp49atGih4OBg3XXXXZo1a1ae74FLfe7u3r1bgwYNUuvWrdW4cWN1795d33zzjTn+X3rppTy36fz585o+fbpuv/12NWzYUG3bttXYsWN1+vRps09B30OOHBc5k4L/9ttvGjVqlEJDQ9WsWTO98MILZp+EhATNmzdPXbt2VYsWLdSgQQO1bNlS/fr109dff233fB06dNDo0aMlSdu3b1dQUJAef/xxSflP8p1zg4ULFy5ow4YNeuSRRxQaGqomTZqod+/e+umnn/KsPSMjQx9++KE6d+6s0NBQtWzZUiNGjNCxY8fMY75t27ZC7xMAKCpcIgcA15DGjRsrMjJSO3fu1GOPPXbJvmPGjNHKlSvl5OSkmjVrqkyZMjp69KjWrl2rzz//XEuWLFGTJk1Uvnx5NWnSRPv371diYqJq1aqlSpUqycfHx+75FixYoMjISNWpU0cXLlyQj4/PZS/Vio2N1SOPPKK4uDjVrVtXqamp2rVrl3bt2qUdO3Zo4sSJV7wv3N3d1aRJEx0+fFhnzpxRtWrVzD+X8tdff6lPnz46deqUXF1dVbduXSUnJ5t1bdy4UW+99Vaecze9+OKLWr9+vby8vFS7dm0dOnRI3333nX7++Wd98MEHaty4cYFq37FjhwYNGqTz58/Lw8NDgYGBio+P19atW7V161Z9++23mjZtmpycnOTj45PndlauXLnA+2r27NnatWuXvLy8FBgYqEOHDmnFihX68ssvtWzZMgUFBZl9MzMz9dxzz+nLL7+UJAUEBKhGjRr666+/tGzZMq1bt07z589XaGio3WtMmTJF7733nqTsy3cCAgJ0+PBhrVmzRp999plmzZqVa46kf8vIyNCzzz6rHTt2qG7dulq0aJEqVqyYb//CjoGCjvV/27NnjwYOHKizZ8/K3d1dtWvX1vnz5/XTTz/pp59+0lNPPaXnnnvO7J+SkqJHH31UUVFR8vDwUK1atSRJf//9t/bv36/PP/9cq1atKtT8OWvWrNGuXbtUtmxZ1a1bV9HR0Vq/fr2++uorLViwQLfccovZNz09XQMHDtSWLVtks9lUs2ZNVa1aVTExMdqzZ4/27NmjHTt26K233jLXOXjwoHr06KGEhARznFy4cEERERGKiIjQ5s2b7fpL0qpVqzR27FhlZmaqYsWKCgwMVExMjD755BNt3LhRr7/+eq4gdPLkyVq6dKkkqWbNmvLw8NAXX3yhbdu2qVy5cgXeH9KVj1Upe06dVatWycnJSTfeeKPOnj2rP//8UxMnTtThw4fNQCE/devWlb+/v44dO6Y9e/aoadOmZltERIQZNuzcuVMPPPCA2ZaVlaUtW7bI2dlZbdq0kZT9y4CXX37ZDGmqVq2qgIAAHTx4UAsXLtRnn32mJUuWqGbNmpfdJ59++qnCw8OVmZmpypUrq06dOoqKitLgwYMvGUonJyfrkUce0V9//aWaNWuqRo0aOnz4sD766CP99NNPWrduncqVK1eg95Cjx0WOiRMnas+ePQoMDFRcXJyqVq0qKTsUevzxx3Xs2DG5u7vrhhtukJOTk44ePaqff/5ZP//8s8LDw9WnTx9J2TfgcHFx0ZEjR1SuXDkFBgYqMDCwQDW8/vrrev/991WuXDnVqlVLR48e1S+//KJt27Zp9uzZuvPOO82+aWlpeuaZZ/Ttt99Kyh5jmZmZWrdunX766SfzcwQAHMoAAJRY0dHRRmBgoBEYGGhER0dftv+SJUuMwMBAo3v37uayX375xQgMDDTatGljLtu/f78RGBhotGzZ0jh48KC5PCkpyXjuueeMwMBA49FHH7V77u7duxuBgYHGmjVr7JaPHDnSrPGjjz4yDMMwsrKyjLNnz9q1v/baa+Y6a9asMddp3bq18fvvv5ttX3/9tREcHGwEBgYaGzduvOR2XCznOS/edsMwjBdeeMEIDAw0Zs+efdn+qampRseOHY3AwEDjiSeeME6dOmW27dmzx2jXrp0RGBhohIeH2z1XzrY0aNDAWLFihZGVlWUYhmGcPn3a6NSpkxEYGGj0798/z7r/7cyZM0bz5s2NwMBAY/jw4cb58+fNtp9++slsmzt3boG281Lat29v1j5x4kQjNTXVMAzDOHfunDFgwAAjMDDQ+M9//mNkZmaa67zxxhtGYGCg0aJFC+Pnn382lyckJBjPP/+8ERgYaLRq1co4ffq02bZq1SojMDDQCA4ONtatW2cuT0lJMaZMmWIEBgYaISEhxl9//WW2/fv4ZGVlmc9/1113GbGxsQXezsKMAcO4/Fi/eCyfO3fOaNOmjbkPk5KSzLYff/zRaNasWa6x/MEHHxiBgYFGz549jfj4eHP58ePHjXvvvdcIDAw03nzzzQJt22OPPWYew2HDhpnjJSUlxQgPDzcCAwONW2+91UhMTDTXWbp0qfneO3DggLk8MzPTWLZsmfl8f/75p9k2dOhQIzAw0Jg8ebKRlpZmLt+xY4fRuHFjIzAw0Ni6dau5fNeuXcbNN99sBAcHGytXrjSXZ2RkGO+8844RFBRkNG7c2O5z7fvvvzcCAwONRo0aGd9++625fP/+/XZj9eLnu5QrGasXf54NHjzYOHPmjLlv3nzzTSMwMNC4+eabjZMnT1729V955RUjMDDQmDNnjt3yqVOnmq9x991327VFRkYagYGBRo8ePcxl7777rhEYGGi0b9/e2LFjh7n83Llz5nY88MADdu/TvMbq0aNHzc/WhQsXmv3j4+ONJ5980qxp5MiR5jo5n7s5++ri14+MjDRCQkKMwMBA47333rPbjvzeQyVhXOS8Z4KCgowffvjBMAzDSE9PNxISEgzDMIzBgwcbgYGBRu/evc3jbxiGcf78eWPEiBFGYGCgERYWZre/V65caQQGBhqPPfaY3Wtd/B2enp5uLr+47rlz55ptFy5cMPr162cEBgYa99xzj91zzZ8/P8/vzF9//dW45ZZbzOf75ZdfCrQfAKA4cIkcAFxDypYtK0mKj4+/ZL/9+/dLkpo0aWI3Aamnp6deeukl3XrrrQoKCpJhGAV+7Ro1auiRRx6RJNlsNnl5eRVovZkzZ6pBgwbm4zvuuEODBw+WJC1ZsqTAr18UNmzYoCNHjqhq1aqaM2eO3VlAwcHBmjt3rmw2mz7++OM857nq2bOnunXrJpvNJkmqVKmSnn76aUlSZGRkgWr44IMPdO7cOTVo0EBTp061++1869atNWnSJEnSokWLlJiYeMXberEWLVpo9OjR5vxNFSpU0GuvvSZvb2/t27dPW7ZskSRduHDBPAtp0qRJuvXWW83nKF++vKZPn6769esrLi5OH374odmWc2bLCy+8oP/85z/mcnd3d7300kvq0KGDkpOT9c477+Rb48SJE7VhwwbVqFFDS5cula+vb5Fsu1UrVqzQyZMn1a5dO40ePdruzLacy34k2Z3ds2/fPknSnXfeaXcGVtWqVTVixAi1b9/+smdN/VutWrU0Y8YMc7y4u7trwoQJuummmxQbG6sNGzaYfbdt2yZnZ2cNHTpUN910k7ncyclJjz/+uHkmzMWX1+V8ZnTu3NnujpTNmjXTwIEDdc8999hdOjZv3jxlZmZq2LBh6tq1q7nc2dlZ/fv31wMPPKCkpCTzrBQpe0xL0uDBg9W+fXtzed26dTVr1qxC7Y8rHas5fH19zfdAzr4ZMmSIKlasqMzMTP3222+XraFt27aSpF9++cVu+bZt21SmTBkFBQXp0KFDOnPmjNn2448/SpLatWsnKftst4ULF8pms+mNN95Qs2bNzL4VKlTQ1KlTdcMNNygqKso8syU/S5YsUWpqqh544AENGDBATk7ZPwZUrFhRb7zxhqpXr37J9ceNG2f3+jmXDUrSrl27LrluDkePi4u1aNHCvOTaxcVF5cuXV2pqqvbs2SObzaaxY8fanUVYrlw58zK6s2fP2l0aeKVyvu9y7jBXpkwZPf/885Ky3385n/Hp6el69913JUnTp0+3+84MDQ3VlClTLNcCAEWBgAkAriE586bkBBz5qVGjhqTsy0AWL16s2NhYs61KlSpavHixXnnllcs+z8VCQkIKXW+tWrUUFhaWa3nODy2RkZE6f/58oZ/3SuXMe9GpUyczrLtYgwYN1LhxY2VlZZk/CF4s5wfKi91www2SVOAwKKeG7t2753mJ4R133CE/Pz9duHBBO3fuLNBzXk5OMHixcuXKqWPHjpJkziMUERGhpKQk+fn56fbbb8+1jrOzszmf0XfffScp+4ekmJgYubq62v1AebGcyzm///77PNvfeOMNLV++XJUrV9bSpUvl5+dXuA0sRjnbee+99+bZfs8998jJyUlRUVE6deqUpP8fE++++66++OILJSUlmf3btWun+fPn69FHHy1UHV26dMk1wbuLi4sZ6F08F9S8efMUGRmpLl265HqetLQ08856ycnJ5vKcmseNG6edO3fazUU2aNAgvfHGG2rVqpWk7FBk69atkqT7778/z3rvu+8+Sf8fqFy4cMGct+fiS8ZyhIaGFupuXFcyVi/WsmXLXPvTyclJAQEBkgr2fr7lllvk5uam3bt3m/syISFBf/75p5o0aWJemrdjxw5znZzjlBMw7dq1S/Hx8apRo4YaNWqU6zVcXV3N92len0kXy3l/5fU+9PDwMD938+Li4pLn/Hc5AeXlfqkhlYxx8e91/83d3V0//vijdu/erRtvvDHP9hwpKSlX9LoXu9R3hvT/4+zXX39VQkKCqlevbne568XPc7mAEACuBuZgAoBryIULFyTpsrdeb9SokTp27Kivv/5a06ZN0/Tp03XzzTerTZs26tChg0JCQgoVLkm6olti16tXL8/lfn5+qlChghISEnT48GEFBwcX+rmvxOHDhyVJN998c7596tevr127dpl9L5ZX8OHh4SFJeU4OfiU12Gw23XzzzTp58mSeNVyJ/I5Dzg+POZNwX1xbfuMjp+4jR47YrVOrVi2VKVPmkuucOXNGCQkJqlChgtm2d+9e8wfMhIQEu4lyS4Kcs3zeffddffTRR3n2cXZ2VlZWlg4fPqwqVaqoa9euWrFihY4ePapnn31Wrq6uatasmW677Tbdcccddj9gFlRBj2EOV1dXnTlzRhEREfr7778VHR2tv/76S3v37jXDkIvPYBw8eLB++eUX7dq1S48++qgqVqyoW265RW3btlWHDh3szlg8cuSIObH1xXNPXSw1NVWSdPToURmGoWPHjik9PV1ly5Y158L5t6CgoHwnLf+3KxmrF8svxMwJGAoy0XeZMmXUvHlzbd68WREREWrdurW2b9+urKwshYWFqUqVKvroo4+0Y8cO3XXXXUpISNCePXtUvXp1cw6fnBsunDlzJs8bEkhSXFycpNzH+GKpqamKiYmRpHznB6pfv36+65cvX94uXLl4G3Oe/3JKwri42KXmqfPw8NDRo0e1a9cuHTlyRNHR0dq/f78OHDhg9inIGLicS31nSP//vZEz+fml5naqV6+eeYwBwFEImADgGpLzn9C8fvP6b2+++aaWL1+uVatWaf/+/dq7d6/27t2rBQsWqHbt2powYYKaN29e4Nf+92/7CyK/wCGnLSEhoUh+S1xQOQFdXmcv5cip+eKzTnJcfOmQo2q4Evkdh5zlOYHDldRWkHUubktKSrILmFJSUlSpUiXVrl1bO3fu1NixY81Ln0qCnDMMci4hu5Scs/EqVqyoVatWaf78+dqwYYNiY2PNCdynTZum1q1b69VXX73shPQXy+8Y5uzbi89GSklJ0cyZM7Vy5Uq7YKBChQpq1aqV9u3bp3/++cfueUJCQrR27Vq9/fbb+vbbb3Xu3Dlt3LhRGzduNM9OCw8Pl5ubm93ZPTnhYH6ysrJ04cIFc99c6jPhcsH5xYr7vVzQy4dvu+02bd68WVu3blXr1q3NO3w1b97cDBdyzkTcsmWLMjMz7c5qydmXiYmJl92Xlzqr6uIzjC43VvJyJZ/v/1YSxsXF8grMJCk6OlqTJk3S999/b3ecq1Wrps6dO2v16tVX9Hp5Keg4y7lT4KX2w6WOHwBcLQRMAHANyZkHI69LKf7N2dlZvXr1Uq9evfTPP/9oy5Yt+vnnn/XTTz/p0KFDGjBggL744otC/ZBbWBf/0PtvOT+M/PuHh/x+sLvUcxVUzn/eL/WDWkF+4LFaQ0JCwlWtIb999+8z4gqyf3LacvoWZp9e3D9HhQoV9N5776ls2bK67777tHXrVn388cfq3LnzJbfpavH09NT58+e1fv36At85SpK8vLz00ksvaeTIkdq7d6+2bNmin376STt27NDPP/+sJ598Up9++mmBzyTML2zM63308ssv67PPPlOZMmX01FNPqUmTJrrpppvk7+8vm82m7t275wqYpOyzoWbNmqW0tDRFRERoy5Yt+u6773TgwAF9+OGHcnZ21ujRo81j6OXlVeBbpufMRZUz5vKScwlwQVzJWC0Ot912m6ZMmWLOw5Qz/1JwcLBcXFwUEBCgffv26fz587kuj5Nkzul11113afbs2Vdcx8XbmJiYqEqVKuXqc6l9XxRKwri4nOTkZD3xxBOKjo5WjRo11LNnTzVs2FA33XSTKlWqpLS0tCINmAoqZxxcaj8U9/EDgIJgDiYAuEbs3r1bhw8flrOzs92tjfOSmJioPXv2mKfTBwQEqFu3bpo9e7a++eYbVatWTcnJydq0aVOx1pzXpSlS9m2iExMT5erqat56OWc+ovx+mLh4HqkrlfNaf/75Z7599u7dK0kFuiV4cdRgGIaioqKKtIb8LrXLeZ2cy6wuri2/oO+PP/6Q9P/ziOScTXfkyJF8Q5Ccdby8vOzOXsp57aCgIAUEBJgTpk+dOtVuYmRHyjkG+V2eZBiGtmzZoiNHjpiXu5w+fVrbt29XUlKSbDabGjRooAEDBmjZsmX68MMPZbPZtG/fPrvLcS4nv/fSv4/hyZMn9fnnn0uSFixYoOeee86cvyUnzMqZK+ribYiOjtb27dslZZ/Ncsstt+iFF17Qhg0bzImP169fLyl7jjcnJyedO3cu3+N05swZ7dy5UydOnJAk+fv7y83NTUlJSTp69Gie6xTmMqgrGavFoXbt2rrhhhv0559/mpdZhYaGmpM6t2jRQllZWWaw6OnpqZYtW+bajktd/nbo0CHt2bPHPMslL+XLlzcvCcvvbLuCnIVnRUkYF5fzzTffKDo6Wl5eXlq1apX69u2rFi1amIHcv98bV0vOPFOX+kwozOcFABQXAiYAuAZkZmZq6tSpkqQHH3zwsnfYmjt3rrp27aqFCxfmavPx8VHdunXN582Rc8ehwtxZ7nL+/PPPPH+oWbFihSQpLCzMnI8i5zfZef1wYhhGvhNEF6bunElsP/300zx/G/zbb79pz549kmR3V6qilFPDihUr8py36auvvlJsbKw8PDzs7uhkxbp163ItO3PmjL7++mtJ/39GRdOmTVWmTBmdOnUqz/AxMzNTK1eulJR9xzsp+wfsgIAApaena9WqVXm+/n//+1+7dfLTt29f1alTR/Hx8eZ4L4jCjt0rGTM52/1vX3zxhZ544gl16tTJDNgGDBigxx9/PM8xGxISYp6tUJg5XjZs2JCr3tTUVH3yySeS/v8YxsTEmP3ymnNn27ZtZvCcM9/VuXPndPfdd6t37955/oCdE4jk1FuuXDk1adJEhmHku19mz56tRx991JyLx9PT03xP5TVO/vrrrwLduS3HlYzV4nLbbbcpMzNTCxYskGEYatGihdmWc5ODDz/8UMePH1fLli3tLt1q1qyZypQpo/379+d5WVlmZqaeeeYZde3aVYsXL75kHR06dJAkc0xcLD09Pc/PgSuV13uoJIyLy8kZ+/7+/nZ3kMvx8ccfm/++eD64nHC2KL8fL9asWTNVqFBBMTExeZ79tXPnznwDOAC4mgiYAKCUi4qKUv/+/bVr1y5VqVLFPJvgUnLueLV69Wq725dL0qZNm7R161bZbDa7H7xyLm84fvx4kdVuGIaeffZZu/8Yf/LJJ1q8eLGcnJw0aNAgc3nt2rXl5eUlwzA0Y8YM80ympKQkTZgwId8zfgpT9/3336+aNWvqxIkTGjZsmDl5riT9/vvvGjZsmKTsW7UXZJ6rK9GzZ095eXnpjz/+0EsvvWR3ic/mzZv1yiuvSJL69+9/xXOP/NvGjRu1cOFCMyCIi4vTkCFDlJiYqFatWpl3WypXrpx69+4tSRo1apS2bNliPkdiYqJefPFF/fnnn/Lx8TH72Ww2PfXUU5KkWbNm2Y23tLQ0TZs2Td999508PDzMM5Ty4+rqqrFjx0rKDgFz7kh1OYUdu4Xp/+ijj8rLy0s///yzJkyYYBdM/vLLL2a9Xbp0MY9Xzvtv6tSp5hlxUvYP+W+++aaSkpJUrVq1Qt0dKzIyUq+++qr5vkhMTNSIESP0zz//6KabbtJdd90lKftsnZwf/i8+5jkh7cWTL+fMz+Tl5aVbb71VWVlZGjFihN3ZgvHx8ealW23atDGXP/3007LZbJo7d65WrVplvk5mZqaWLVtmTojep08fu3WcnJy0ePFiux/ko6Oj9eyzzxbqh/crGavFJSeEzAl28gqYcu4e+e+7ipUrV069evWSJD377LOKiIgw2xITExUeHq79+/fLw8ND3bt3v2Qdffv2lZubmz755BO999575jG5cOGCXn755UueJVVY+b2HHD0uLifnjLF9+/bp22+/NZenpqZqyZIlmj9/vrns4rNpc7b35MmTBb6hQ2F4eHiY43TkyJF233dRUVEaMWJEkb8mAFwJ5mACgFLimWeesZtoNSUlRSdPntTp06clSdWrV9eCBQsue/aSlD1H04ABA/TOO+/ohRde0JQpU1S1alWdPn3a/IFg2LBhdj/gBgUF6ccff9Tbb7+tTZs26bHHHtNDDz1kaZtatGihqKgo3XPPPapbt64SEhLM3yCHh4fbnaHj7OysIUOG6NVXX9XatWv13Xffyd/fX4cPH1ZycrKeeuopu//8X1y3lP2b5z/++EP33HOPXXB1MTc3N82ZM0f9+vXTzz//rHbt2ikwMFDJycnmBOq33HKLRo8ebWm7L8XHx0dvvvmmBg8erHXr1unrr782z9rJmRfn/vvvz3cbrsQdd9yhWbNmadmyZapSpYoOHDigtLQ01a5dW9OmTbPrO2TIEB08eFBff/21nnjiCdWoUUMVK1bUwYMHlZKSIm9vb7355pvy8fEx1+natav27dun999/Xy+88IKmT5+uKlWq6PDhwzp//rw8PT01derUAgUqYWFheuCBB7Ru3TqNHTtW69evz3ey3hyFGQM5/Qs61n19fc3jtXz5cq1duzbX8WrRooVGjhxprtO7d29999132rlzp7p06aKAgADz7IT4+Hi5ublp0qRJhZo0/o477tAHH3ygDRs2KCAgQH///bcuXLigypUra/bs2eZz+fr6qmfPnvrggw+0YMECrVmzRlWrVtXJkycVGxsrT09PhYSEKDIy0u5spXHjxunhhx/WL7/8ovbt26tmzZpydnbWkSNHlJKSIn9/f7344otm/1tvvVUjRozQjBkzNHr0aL3++uvy9/dXTEyMeQbigAEDzOBLyv5cGjlypKZOnaqXXnpJc+bMUcWKFc0ApXnz5tqxY0eB98mVjNXi0LJlS3l4eCglJUWenp52d8WsVq2aatSooejoaEn28y/lGDp0qP766y99/fXX6tmzp2rUqKEKFSro77//VlJSklxcXPTGG29cdr68WrVqacyYMXrllVc0ZcoUvfvuu6patar++usvJScnq0GDBvrjjz/My5GtyO89VBLGxaXcfvvtql+/vvbu3atBgwapZs2aKleunI4cOaLExERVr15dTk5Oio6O1qlTp8w7EQYFBclms+no0aO66667dPPNN2vOnDlFUlOOgQMHaufOndq6das6d+6sunXryjAMHTx4UH5+fvL19VVcXFyRHD8AuFKcwQQApcTvv/+uX3/91fxz4MABOTk5qVWrVhozZow+//xz89K2gnjhhRc0bdo0tWjRQunp6YqKilJaWpo6dOigRYsWafDgwXb9n3rqKd1///3y8PDQ4cOHi+R0/BtuuEErVqxQmzZtFB0drbNnz6ply5ZasmSJ3W+wczz++OOaPXu2mjZtqtTUVB05ckSNGzfW0qVL9cADD+T5Gg8//LB5W/XDhw+bQVF+goKCtG7dOg0YMEABAQE6ePCgzpw5o2bNmmny5MlavHixypUrZ3nbL6Vly5Zat26devbsKR8fH+3bt0/Jyclq3bq15syZo1mzZplzuBSFUaNG6aWXXlLZsmW1f/9+Va5cWQMGDNCqVatUpUoVu74uLi6aM2eOpk+frhYtWig+Pl4HDx6Uv7+/+vfvr08//TTPuw+OHj1aCxYs0G233abU1FTt27dPXl5e6tGjhz7++GPdfffdBa535MiRqlChgo4cOaK5c+detn9hx0Bhx3rLli21fv16PfbYY6pcubL279+v06dPq0GDBho5cqQWLVpkFw67urrqnXfe0bBhw1SvXj3FxcVp//79Klu2rB566CGtW7eu0JdgPvHEE5o2bZqqVKmi/fv3q0yZMurevbs+/vjjXMHdqFGjNHnyZDVs2FCpqanmD+oPP/ywPv74Y/O9/+OPP5rr+Pv7a9WqVXrkkUdUrVo1HTlyREePHlWNGjX01FNP6dNPP811G/l+/frpv//9r+6++245OTnpzz//VEZGhlq1aqXZs2dr+PDhubajT58+WrJkiW699VYlJibqyJEjat26tT766KNCzzl2pWO1qLm7u5tnKoWGhuYKDnPagoKC8gyJcrZjxowZatmypRISErRv3z5z4vuVK1eqffv2Baqla9eueu+999S6dWulpqbqwIEDCgoK0sKFC81QJ+eyZCsu9R5y9Li4FFdXV73//vt66qmnVKdOHZ04cUJ///23qlevrqefflqffPKJuZ8uvsS1Tp06Gjt2rKpXr64TJ05ccu6vK+Xm5qZ33nlHw4cP10033aQjR44oLi5OnTt31sqVK83vpaI4fgBwpWxGcV0sDAAAAKBUmDFjht599109++yzRXqGJK6OVq1a6fTp0/rxxx/l5+fn6HIAXKc4gwkAAAC4xo0YMUJdunTR5s2b82z/+eefJUn16tW7mmWhABISEnTbbbfpiSeeUHp6eq72vXv36vTp0/L29iZcAuBQBEwAAADANe6mm27SH3/8oZkzZ+rEiRPm8uTkZE2ePFlRUVEKCAhQq1atHFgl8lKhQgV5enpqy5Ytmjt3rt0E44cPHzbnP+vataujSgQASVwiBwAAAFzzEhMT9cgjj+jgwYNycXFRzZo15ebmpqNHj+rChQvy8vLSggUL1LhxY0eXijz8+OOPevrpp5Wenq6KFSsqICBAFy5c0NGjR5WVlaVbbrlFCxYsuOxNDwCgOBEwAQAAANeB5ORkrVixQhs2bFB0dLRSU1Pl5+endu3aqU+fPpe9Ex0c69ChQ1q0aJEiIiJ0/PhxeXh4qHbt2nrwwQfVtWtX7iAHwOEImAAAAAAAAGAJczABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEtcHF3AteT06fNiRquSy2aTfHzKc5zgMIxBOBpjEI7GGISjMQbhaIxBONqVjMGcdS6HgKkIGYb4kCgFOE5wNMYgHI0xCEdjDMLRGINwNMYgHK04xiCXyAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAADAQX744Tu1bt3M7s/o0S/a9Tl+/Jg6dmyjX3/dednnMwxDzz03WJ9/vt5ueUJCgsaNG6WOHduoc+d7tWrVR2bb6dNxevrp/rrzzraaNm2SDMMw29asWak335xlcStxPXBxdAEAAAAAAFyvDh8+pFtvbaMXXxxlLnNzc7frM3PmVCUnJ1/2ubKysvTmmzO1Y8c2dex4t13b+PGjlZh4XgsWLNGRI4c1ceJY3XBDTYWF3aLly5fKy8tbCxYs0YsvPqvNm39U69ZtlZ6erlWrPtLcuQuLZmNxTSNgAgAAAADAQY4cOazatW+Sj49vnu1fffWFkpIuXPZ5YmNPacKEV3TsWIzKlStv13bw4AHt3LlNH364RtWrB6h27Zu0a1eEfvstUmFht+jIkSO67bZ2uvHG2qpfv6GOHDms1q3basOGT3XLLbfK1zfv2oCLcYkcAAAAAAAOcvjwIdWocUOebefOxeutt2ZrxIiXL/s8+/ZFqUoVPy1a9IHKlStn17ZrV4Tq1Kmr6tUDzGXPPz9S/fs/JUny8/PTgQP7lJqaqsOHD8nPr6rS09O1cuWHevTR3ha2DtcTAiYAAAAAABzAMAwdPXpE27ZtVffuXdSt24N6++05Sk9PlyTNmfO67rnnftWuXeeyz9W69W165ZUJ8vLyytV27FiM/P399eGH76tr1wfUs+dD+uSTNWZ7jx6Pa8uWn9WxYxt5eVVSu3a367PP1iksrBVnL6HAuEQOAAAAAAAHOHnyhFJSUuTm5qaJE6fo2LFjevPNmUpNTdWtt7bRnj279f77Kyy/TnJyknbu3K7MzExNnDhVf/11UK+9Nk1eXl5q1+521ahxg1atWqeEhHPy9q6kjIwMrVz5oWbPXqBPPlmj5cuXqUqVKho9eryqVfMvgi3HtYiACQAAAAAAB6hatZo+/3yTypevIJvNprp1g2QYWRo9eqQ2b/5Rw4eHy93dw/LrODs7KzMzS2PGvCpPT0/Vq1dfBw8e0KefrlW7drebfby9K0mSPv98vVq0aCnDyNL8+XP0/vsrtWnTV3r99RmaPv11y/Xg2sQlcgAAAAAAOEiFChVls9nMxzVr3ihJOn78mEaPflEdO7ZRx45tJEnDhz+jGTMmF/o1fHx8VaVKFXl6eprLbrihpk6dOpmrb0ZGhlasWK7HHuujvXt/V40aNVW5chW1bHmr9uzZXejXxvWDM5gAAAAAAHCAbdu2avz40Vq79jN5eGSfqXTgwH5VqFBRCxe+Z9e3e/fOeuml0WrePKzQr9OgQbA++GCpEhMTzQnAjxz5W1Wr5r7cbePGDWrePEy+vpVlsznJMAxJUmZmpiSj0K+N6wdnMAEAAAAA4ADBwY3k7u6uqVMn6ujRw9q6dbPeeutNPfpoLwUE1LD7I0m+vpXNy9iSkpJ09uzZAr1Os2YtdMMNNTVp0lgdOXJYmzZ9pfXrP1Hnzg/Z9cvIyNBHH32oxx7rI0kKDAzS33//pcjIXfrss0/VoEGjott4XHMImAAAAAAAcIAyZcpq1qw5io8/q379emnq1Il64IHO6tmz12XX/e9/39eAAZfvJ2XPrzRjxhvKyspS376Pat68NzV06HNq3bqtXb8vv/xcTZs2k69vZUnZc0Q9+eRghYcP1+7du/TMMy8UfiNx3bAZOee7wbK4uPNib5ZcNpvk61ue4wSHYQzC0RiDcDTGIByNMQhHYwzC0a5kDOascznMwQQAAAAAKBJOTjY5Odku3/E65+zMxUTXk6wsQ1lZ136iSMAEAAAAALDMyckmL+8ycnYiPLkcb++yji4BV1FmVpbizyZd8yETARMAAAAAwDInJ5ucnZz0xtf79M+ZJEeXU2K5uDgrIyPT0WXgKgmoVEbPdgySk5ONgAkAAAAAgIL650yS/o674OgySixXV2elpxMw4drDuYsAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEtKRMCUlpam+++/X9u2bcvVdv78ebVp00Zr16695HO89957atOmjUJDQ/Xyyy8rOTnZbEtNTdXLL7+sZs2aqXXr1lq8eLHdutHR0erTp48aN26se++9Vz///HPRbBgAAAAAAMB1wOEBU2pqqp5//nkdOHAgz/YZM2bo1KlTl3yOL7/8UnPnztWECRO0dOlSRUZGasaMGWb79OnT9fvvv2vp0qUaO3as5s6dq40bN0qSDMPQ4MGD5evrqzVr1ujBBx/UkCFDdOzYsaLbSAAAAAAAgGuYQwOmgwcPqlu3bjp69Gie7Tt37tQvv/yiypUrX/J5li1bpt69e6t9+/Zq1KiRxo8frzVr1ig5OVlJSUlatWqVRo0apQYNGqhjx47q37+/li9fLkn65ZdfFB0drQkTJqhOnTp68skn1bhxY61Zs6bItxcAAAAAAOBa5OLIF9++fbvCwsL03HPPqXHjxnZtaWlpeuWVVzRmzBiNGTMm3+fIzMzUb7/9piFDhpjLGjdurPT0dEVFRckwDGVkZCg0NNRsb9q0qebPn6+srCxFRkaqfv36KlOmjF377t27C709NluhV8FVlHN8OE5wFMYgHI0xCEdjDMLRGIMAHKkkfPZcyedgQfs6NGDq2bNnvm3z589X/fr11bp160s+R0JCglJTU1WlShVzmYuLi7y8vHTixAk5OTnJ29tbbm5uZruvr69SU1MVHx+v2NhYu3UlycfHRydOnCj09vj4lC/0Orj6OE5wNMYgHI0xCEdjDMLRGIPFy8XFWa6uzo4uo0Rj/1w/XFyyj7W3d1kHV2KvOD4HHRow5efgwYP66KOPtG7dusv2TUlJkSS7ACnncVpamgzDyLNNyj5LKjk5Od91C+v06fMyjEKvhqvEZst+E3Gc4CiMQTgaYxCOxhiEozEGi5ezs5O8vcsqIyNT6emZji6nxHJ1dWb/XEcyMrKP9dmzF5SZmeXgaq7sczBnncspcQGTYRgaPXq0hg0bJl9f38v2d3d3l6RcgVBaWpo8PT2VmZmZZ5skeXh4yN3dXfHx8bnaPTw8rqB28UVVCnCc4GiMQTgaYxCOxhiEozEGAThCSfrcKY7PQYffRe7fjh07pl27dmnatGkKDQ1VaGiojh07prFjx6p///65+nt5ecnd3V1xcXHmsoyMDMXHx6ty5cry8/PT2bNnlZGRYbbHxsbKw8NDFSpUkJ+fn926khQXF5frsjkAAAAAAADkrcSdweTn56evvvrKbtnjjz+uxx9/XA888ECu/k5OTgoODlZERITCwsIkSbt375aLi4vq1asnKXtOpt27d6tZs2aSpIiICAUHB8vJyUkhISFauHChUlJSzLOWIiIi1LRp0+LcTAAAAAAAgGtGiTuDycXFRTVr1rT74+LiIh8fH/n5+UnKnncpNjbWXKdnz55atGiRvvnmG+3Zs0fjxo1Tt27d5OnpKU9PT3Xq1Enjxo3Tnj179M0332jx4sXq1auXJKlFixaqVq2awsPDdeDAAS1cuFB79uzRww8/7JDtBwAAAAAAKG1K3BlMBfH5558rPDxc+/btkyTdd999iomJ0ZgxY5SWlqY777xTI0aMMPuHh4dr3Lhx6t27t8qVK6ehQ4fqzjvvlCQ5Ozvrrbfe0qhRo9SlSxfVrFlT8+bNk7+/v0O2DQAAAAAAoLSxGUZJmmaq4Pr27avFixc7ugw7cXHcjaIks9kkX9/yHCc4DGMQjsYYhKMxBuFojMHi5eKSfRe54St26e+4C44up8TiLnLXlxt9y2rmI6E6e/aCMjJKxl3kCvs5mLPO5ZS4S+QKYufOnapataqjywAAAAAAAIBK6SVyjRs3ZhJuAAAAAACAEqJUBkwuLqWybAAAAAAAgGtSqbxEDgAAAAAAACUHARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgSYkImNLS0nT//fdr27Zt5rLdu3ere/fuCg0N1V133aVVq1Zd8jk2bNigO+64QyEhIRo8eLDOnDljthmGoZkzZ6ply5Zq0aKFpk+frqysLLP97NmzGjp0qEJDQ9WhQwd9+umnRb+RAAAAAAAA1yiHB0ypqal6/vnndeDAAXNZbGysBgwYoBYtWujjjz/WsGHDNHHiRH3//fd5PseePXs0atQoDRkyRCtWrFBCQoLCw8PN9iVLlmjDhg2aO3euZs+erfXr12vJkiVme3h4uM6fP68VK1Zo0KBBGj16tPbs2VNs2wwAAAAAAHAtcXHkix88eFAvvPCCDMOwW/7NN9/I19dXzz//vCSpVq1a2rZtm9avX6927drlep4PPvhA99xzjzp16iRJmj59utq3b6/o6GjVqFFDy5Yt07Bhw9SsWTNJ0vDhw/Xmm2+qX79+Onr0qL777jtt2rRJAQEBCgwM1O7du/Xhhx+qUaNGxbr9AAAAAAAA1wKHnsG0fft2hYWFacWKFXbL27RpoylTpuTqn5iYmOfzREZGmuGRJFWrVk3+/v6KjIzUyZMndfz4cTVv3txsb9q0qWJiYnTq1ClFRkaqWrVqCggIsGvftWuX1c0DAAAAAAC4Ljj0DKaePXvmuTwgIMAu8Dl9+rQ+++wzDR06NM/+p06dUpUqVeyW+fj46MSJE4qNjZUku3ZfX19JMtvzWvfkyZOF3h6brdCr4CrKOT4cJzgKYxCOxhiEozEG4WiMQQCOVBI+e67kc7CgfR0aMBVESkqKhg4dKl9fXz3yyCP59nFzc7Nb5ubmprS0NKWkpJiPL26TsicXT05OznfdwvLxKV/odXD1cZzgaIxBOBpjEI7GGIQkDRw4UJUqVdLUqVMlSevWrdO8efN0/Phx1a9fXy+//PIlp6xYvny53nnnHSUkJKh169aaMGGCvLy87PrEx8fr3nvv1cqVK81fYCclJWnUqBHavn27WrVqpZkzZ8rDw0OS9P3332vlypV66623imejrxMuLs5ydXV2dBklGvvn+uHikn2svb3LOrgSe8XxXVyiA6YLFy7o6aef1uHDh/Xhhx/K09Mzz37u7u65AqG0tDR5enrahUnu7u7mvyXJ09Mz33VzvmQK4/Tp8/rXdFIoQWy27DcRxwmOwhiEozEG4WiMQeT45psv9cMPP+iee+5XXNx57d69S6NGjdJLL41Ww4aN9PHHq9WvX3+tWbNeZcqUyWP9rzRt2jS98soE1axZU1OmTNSoUa9o/PjJZp+EhAS9+OKzOn36tM6evSAPj/Oy2aQNG9bq1Kk4vfPOUr366lgtWfK+unbtLkl6883ZGjEiXHFx56/avriWODs7ydu7rDIyMpWenunockosV1dn9s91JCMj+1ifPXtBmZlZl+ld/K7kuzhnncspsQFTYmKi+vfvr6NHj2rp0qWqVatWvn39/PwUFxdntywuLk6VK1eWn5+fpOw70+X81iLnsrmc9vzWLSzDEP9ZKgU4TnA0xiAcjTEIR2MMXt8SEs5p3rzZuvnm+pKyx8Lp06fVu3c/3XnnvZKkPn3667///UB//31I9es3zPUcy5cv1aOP9la7drdLkp5++hnNmjVVGRmZcnZ2VmTkbr366lgznLp4zB06dEhNmjTTDTfUUpMmzXX48GEZhrR162b5+Piqbt16jE8AxaIkfbYUx3exQyf5zk9WVpaGDBmif/75R++//77q1q17yf4hISGKiIgwHx8/flzHjx9XSEiI/Pz85O/vb9ceEREhf39/ValSRY0bN1ZMTIxOnDhh1964ceMi3y4AAADgejd37hu66657VatWbXNZhw53qHfvfpKk1NQUrVjxoby9K9n1yXHhQqL279+ntm07mMsaN26i999fKWfn7EtRtm/fqvvu+48mTZqea31/f38dPLhfGRkZOnBgv/z8qkqS3nvvXT3xxIAi3VYAuJ6UyDOYVq9erW3btuntt99WhQoVzDOOXF1d5eXlpbS0NJ07d06VKlWSs7OzevTooccff1yNGzdWcHCwJk2apHbt2qlGjRqSpB49emjmzJmqWjX7y2PWrFnq27evJKlGjRpq3bq1RowYoVGjRum3337Thg0b9MEHHzhm4wEAAIBrVETEDkVG7tKyZR9p5sypudp37tyu558fIsMwNGbMxDwvjzt2LEaSFB9/VoMG9dWxY8fUvHmYnnlmuMqXz76EY8CAQZKk48eP5Vq/a9eu+vjjT9ShQyvVrn2THnywi7Zt2ypvb28FBtYrys0FgOtKiQyYvvzyS2VlZenJJ5+0W96iRQu9//772rVrl3r16qVNmzYpICBAoaGhmjBhgmbPnq1z587p1ltv1cSJE831+vXrp9OnT2vIkCFydnbWww8/rD59+pjt06dP16hRo9StWzdVrlxZkydPvuSEggAAAAAKJzU1VTNmTNbzz4+Uu3ve853Wrl1Hixa9r82bf9LkyeNVrVp1NWwYbNcnKSlZkvTaa9M0aNBQVahQUW++OUsTJ47R9OmvX7aOSpUq6f33V+jMmTOqVMlHkvTee+/o2Wdf1E8/fa9582arTBlPvfTSKwROAFAIJSZg2rdvn/nvRYsWXbJvWFiYOnXqZE7aLUldunRRly5d8uzv7Oys8PBwhYeH59nu4+Oj+fPnX0HVAAAAAApiyZJ3FBR0s8LCbsm3T6VKPqpUyUd16wZp797f9emna3IFTDmXwT32WB+1bt1WkvTSS6P1xBOPKi4uVr6+l59L1WazmeHSjh2/qEKFiqpVq5aee26w3nhjno4fP6ZXXx2rZctWXOnmAsB1p8QETIVx9OhRnTlz5oom4gYAAABw9W3a9JVOnz6tjh3bSPr/Ozt///0mzZ49X05OzgoK+v8zhmrVulGHD/+d63l8fX0lSTfcUMtcdsMNNSVJJ0+eLFDAdLElS97VM88M1+HDh+Xs7KzAwHqqWbOWRo16UYmJiSpXrlyhng8ArlelMmCqUaOG3n77bUeXAQAAAKCA5sxZoIyMDPPx22/PliQNGjRM//3v+zp+/Jhee22u2b5vX1Sel6j5+VWVr29lHTy4Xw0aZN9h7vDhw7LZbOacqwW1c+d2lS9fXkFB9XTgwD4ZRvYtxDMzs28rbpSkWz4BQAlXKgMmm80mF5dSWToAAABwXapatZrd4zJlykqSAgJq6IEHumjgwN5aufK/uuWWW/XVV19o794/NHr0eEnZd5ZLTEyUj4+vbDabunXrqUWLFsjf319eXpU0c+YUtWnTTj4+voWq6b333tWQIc/9r44blJGRoe+/36Tjx4+rZs1a5qThAIDLI6UBAAAA4FBBQfU0efJMLVw4T/Pnz1Xt2nX02mtzVLlyFUnSpk1fa/Lk8fr5552SpB49HlNaWqomThyr5OQk3XrrbRo+PO/5VvPz6687VaZMWdWrd7MkydPTUyNGvKzXXpuuMmXKmuEWAKBgbAbnfRaZuLjzYm+WXDab5OtbnuMEh2EMwtEYg3A0xiAcjTFYvFxcnOTtXVbDV+zS33EXHF1OieXq6qz09ExHl4Gr5Ebfspr5SKjOnr2gjIwsR5dzRZ+DOetcDmcwAQAAAFfIyckmJyebo8tAITk7Ozm6hGsS+xW4vhEwAQAAAFfAyckmL+8ycnbih+rSxtu7rKNLAIBrDgETAAAAcAWcnGxydnLSG1/v0z9nkhxdDgrIxcVZGRlcnlQcQmt669GWtWSzcVYfcD0iYAIAAAAs+OdMEvPNlCLMf1N8qnt7OroEAA7E+bwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWFIiAqa0tDTdf//92rZtm7ksOjpaffr0UePGjXXvvffq559/vuRzbNiwQXfccYdCQkI0ePBgnTlzxmwzDEMzZ85Uy5Yt1aJFC02fPl1ZWVlm+9mzZzV06FCFhoaqQ4cO+vTTT4t+IwEAAAAAAK5RDg+YUlNT9fzzz+vAgQPmMsMwNHjwYPn6+mrNmjV68MEHNWTIEB07dizP59izZ49GjRqlIUOGaMWKFUpISFB4eLjZvmTJEm3YsEFz587V7NmztX79ei1ZssRsDw8P1/nz57VixQoNGjRIo0eP1p49e4pvowEAAAAAAK4hLo588YMHD+qFF16QYRh2y3/55RdFR0fro48+UpkyZVSnTh1t3bpVa9as0dChQ3M9zwcffKB77rlHnTp1kiRNnz5d7du3V3R0tGrUqKFly5Zp2LBhatasmSRp+PDhevPNN9WvXz8dPXpU3333nTZt2qSAgAAFBgZq9+7d+vDDD9WoUaNi3wcAAAAAAAClnUPPYNq+fbvCwsK0YsUKu+WRkZGqX7++ypQpYy5r2rSpdu/enefzREZGmuGRJFWrVk3+/v6KjIzUyZMndfz4cTVv3tzuuWJiYnTq1ClFRkaqWrVqCggIsGvftWtXEW0lAAAAAADAtc2hZzD17Nkzz+WxsbGqUqWK3TIfHx+dOHEiz/6nTp3Kt39sbKwk2bX7+vpKktme17onT54s3MZIstkKvQquopzjw3GCozAG4WiMQTgaYxAAcD0rCd9/V/JdXNC+Dg2Y8pOcnCw3Nze7ZW5ubkpLS8uzf0pKSr79U1JSzMcXt0nZk4sX9rUuxcenfKHXwdXHcYKjMQbhaIxBONq1NgZdXJzl6urs6DJQCByv4uHs7Py/v53Yx5fB/rl+uLhkH2tv77IOrsRecXwXl8iAyd3dXfHx8XbL0tLS5OHhkW//fwdCaWlp8vT0tAuT3N3dzX9LkqenZ77r5vdal3L69Hn9azoplCA2W/abiOMER2EMwtEYg3C0a20MOjs7ydu7rDIyMpWenunoclBArq7OHK9ikpmZ+b+/s9jHl8AYvL5kZGQf67NnLygzM+syvYvflXwX56xzOSUyYPLz89PBgwftlsXFxeW6lO3i/nFxcbn6V65cWX5+fpKyL7vLmWcp57K5nPb81i0sw9A18Z+lax3HCY7GGISjMQbhaIxBAMD1qCR99xXHd7FDJ/nOT0hIiP744w/z8jZJioiIUEhISL79IyIizMfHjx/X8ePHFRISIj8/P/n7+9u1R0REyN/fX1WqVFHjxo0VExNjN79TRESEGjduXPQbBgAAAAAAcA0qkQFTixYtVK1aNYWHh+vAgQNauHCh9uzZo4cfflhS9iVssbGx5imYPXr00KeffqpVq1YpKipKL774otq1a6caNWqY7TNnztS2bdu0bds2zZo1S7169ZIk1ahRQ61bt9aIESMUFRWlVatWacOGDXr00Ucds/EAAAAAAAClTIkMmJydnfXWW28pNjZWXbp00bp16zRv3jz5+/tLknbt2qXWrVvr+PHjkqTQ0FBNmDBB8+bNU48ePVSxYkVNmTLFfL5+/frp3nvv1ZAhQ/TMM8/owQcfVJ8+fcz26dOnq2zZsurWrZvmz5+vyZMnq1GjRld1mwEAAAAAAEqrEjMH0759++we16xZUx988EGefcPCwtSpUydz0m5J6tKli7p06ZJnf2dnZ4WHhys8PDzPdh8fH82fP/8KKwcAAAAAALi+lcgzmC7n6NGjOnPmzBVNxA0AAAAAAICiVSoDpho1aujtt992dBkAAAAAAABQKQ2YbDabXFxKzNV9AAAAAAAA17VSGTABAAAAAACg5CBgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYEmhA6bExETNnDlThw4dUlZWll588UU1btxYPXv2VExMTHHUCAAAAAAAgBKs0AHT+PHj9cMPP8hms2n9+vX66quvNHnyZPn6+mr8+PHFUSMAAAAAAABKMJfCrvDDDz9o2bJluvHGGzVjxgy1b99e9957r+rXr6/OnTsXR40AAAAAAAAowQp9BpNhGHJ1dVVKSoq2bt2qtm3bSpLOnTunMmXKFHmBAAAAAAAAKNkKfQZTy5Yt9corr6hMmTJycnLSHXfcoa1bt2rixInq0KFDcdQIAAAAAACAEqzQZzBNnjxZ9evXl5ubm+bNm6dy5cpp3759atu2rUaPHl0cNQIAAAAAAKAEK/QZTOXLl88VJPXp06eo6gEAAAAAAEApU+gzmCRp3bp16tKli5o1a6bo6GhNmjRJCxcuLOraAAAAAAAAUAoUOmD68MMPNX36dHXp0kXp6emSpIYNG2rRokWaO3dukRcIAAAAAACAkq3QAdP777+vV199VY899picnLJXf/DBBzV9+nStWrWqyAsEAAAAAABAyVbogOnYsWOqU6dOruU1atRQfHx8UdQEAAAAAACAUqTQAVNISIg++eQTu2WGYWjx4sVq1KhRUdUFAAAAAACAUqLQd5EbPXq0Bg4cqO+//15paWkaP368Dh8+rJSUFL3zzjvFUSMAAAAAAABKsEIHTIGBgfryyy+1bt06HTp0SJmZmbr99tv1wAMPqGzZssVRIwAAAAAAAEqwQgdMkuTu7q6uXbsWdS0AAAAAAAAohQoUMHXo0EE2m61AT7hp0yZLBQEAAAAAAKB0KVDANHTo0OKuAwAAAAAAAKVUgQKmzp07m/+eO3eu+vXrJ09PT7s+iYmJmjt3btFWBwAAAAAAgBKvQAHToUOHdPr0aUnSvHnzVK9ePVWsWNGuz/79+/XRRx/ppZdeKvoqAQAAAAAAUGIVKGA6deqU+vTpYz4eMmRIrj6enp7q3bt3kRUGAAAAAACA0qFAAVPLli0VFRUlKXvC79WrV6tSpUrFWhgAAAAAAABKhwIFTBf79ttvi6MOAAAAAAAAlFIFCphuv/12rV69Wt7e3urQoYNsNlu+fTdt2lRkxQEAAAAAAKDkK1DANGTIEJUtW1aSNHTo0GItCAAAAAAAAKVLgQKmzp07m/+OiYlRv3795OnpadcnMTFRc+fOLdrqAAAAAAAAUOIVKGA6dOiQTp8+LUmaN2+e6tWrp4oVK9r12b9/vz766CO99NJLRV8lAAAAAAAASqwCBUynTp1Snz59zMdDhgzJ1cfT01O9e/cussIk6fjx4xo3bpx27NghLy8v9erVy66Oi23ZskWTJ09WdHS0QkJCNGnSJNWoUcNsf++997Ro0SIlJibqnnvu0SuvvGKehZWamqrx48frq6++koeHh/r27au+ffsW6bYAAAAAAABcqwoUMLVs2VJRUVGSpA4dOmj16tWqVKlSsRYmSc8++6z8/f21du1aHTx4UMOHD1f16tXVsWNHu37Hjh3T4MGDNXToULVp00bz5s3T008/rXXr1slms+nLL7/U3LlzNWPGDPn4+Cg8PFwzZszQmDFjJEnTp0/X77//rqVLl+rYsWMaOXKk/P39dffddxf7NgIAAAAAAJR2ToVd4dtvv70q4dK5c+e0e/duDRo0SLVq1dIdd9yhNm3aaOvWrbn6rlq1Sg0bNlTfvn1Vt25dTZkyRTExMdq+fbskadmyZerdu7fat2+vRo0aafz48VqzZo2Sk5OVlJSkVatWadSoUWrQoIE6duyo/v37a/ny5cW+jQAAAAAAANeCQgdMV4uHh4c8PT21du1apaen69ChQ/r1119188035+obGRmpZs2amY89PT3VoEED7d69W5mZmfrtt9/s2hs3bqz09HRFRUUpKipKGRkZCg0NNdubNm2qyMhIZWVlFe9GAgAAAAAAXAMKdImcI7i7u2vMmDGaOHGili1bpszMTHXp0kVdu3bN1Tc2NlZVqlSxW+bj46MTJ04oISFBqampdu0uLi7y8vLSiRMn5OTkJG9vb7m5uZntvr6+Sk1NVXx8fKHO1rLZrmBDcdXkHB+OExyFMQhHYwzC0RiDAIDrWUn4/ruS7+KC9i1QwLR582Y1b97cLoS5Gv766y+1b99eTzzxhA4cOKCJEyfqlltu0QMPPGDXLzk5OVdtbm5uSktLU0pKivk4r3bDMPJsk6S0tLRC1evjU75Q/eEYHCc4GmMQjsYYhKNda2PQxcVZrq7Oji4DhcDxKh7Ozs7/+9uJfXwZ7J/rh4tL9rH29i7r4ErsFcd3cYECpiFDhuiLL75Q1apVdfvtt2v16tXy9vYu8mIutnXrVq1evVo//PCDPDw8FBwcrJMnT+rtt9/OFTC5u7vnCoPS0tJUoUIFubu7m4//3e7p6anMzMw826Tsy/QK4/Tp8zKMQq2Cq8hmy34TcZzgKIxBOBpjEI52rY1BZ2cneXuXVUZGptLTMx1dDgrI1dWZ41VMMjMz//d3Fvv4EhiD15eMjOxjffbsBWVmOn4aniv5Ls5Z53IKFDBVqFBB8+bNU5MmTRQTE6PPPvtM5cqVy7Nvp06dClbhZfz++++qWbOmXchTv359zZ8/P1dfPz8/xcXF2S2Li4vTzTffLC8vL7m7uysuLk516tSRJGVkZCg+Pl6VK1eWYRg6e/asMjIy5OKSvTtiY2Pl4eGhChUqFKpmw9A18Z+lax3HCY7GGISjMQbhaIxBAMD1qCR99xXHd3GBAqYxY8Zozpw52rJli2w2m9599105OeWeH9xmsxVZwFSlShUdOXJEaWlp5iVrhw4dUkBAQK6+ISEhioiIMB8nJydr7969GjJkiJycnBQcHKyIiAiFhYVJknbv3i0XFxfVq1dPUvacTLt37zYnAo+IiFBwcHCe2wgAAAAAAAB7BQqYbr/9dt1+++2SpA4dOmj16tWFmvz6SnTo0EEzZszQ6NGjNWjQIP3999+aP3++nnvuOWVmZurMmTOqWLGi3Nzc9NBDD2nRokVauHCh2rdvr3nz5ikgIMAMlHr27KkxY8YoMDBQVapU0bhx49StWzd5enpKyj7raty4cZo8ebJOnTqlxYsXa8qUKcW6fQAAAAAAANeKQt9F7ttvv5WUPfH3X3/9paysLN14441q1aqVXF1di6yw8uXL67333tOkSZP08MMPq1KlSho0aJAeeeQRxcTE6Pbbb9eyZcsUFhamgIAAzZkzR5MnT9a8efMUGhqqefPmyfa/qc7vu+8+xcTEaMyYMUpLS9Odd96pESNGmK8VHh6ucePGqXfv3ipXrpyGDh2qO++8s8i2BQAAAAAA4FpW6IDp5MmT5hlFN954ozIzM3XkyBH5+/tryZIl8vPzK7LibrrpJi1ZsiTX8oCAAA0ePNicwFuS2rZtq7Zt2+b7XAMHDtTAgQPzbPP09NS0adM0bdo060UDAAAAAABcZwo9ydC4cePk4+Oj77//XmvXrtWnn36q7777Tv7+/po0aVJx1JhLYmKitm7dqvr161+V1wMAAAAAAED+Ch0w/fLLLxoxYoQqVqxoLvP29tbw4cO1efPmIi0uP+XKldOyZcvMyb8BAAAAAADgOIUOmCpWrKhz587lWp6QkFCkczBdztV8LQAAAAAAAOSv0AHTfffdp9GjR2vr1q1KTExUYmKiNm/erFdeeUX33ntvcdQIAAAAAACAEqzQk3w/88wzOn36tPr16yfDMCRJzs7O6tq1q1588cUiLxAAAAAAAAAlW6EDJjc3N02dOlUvv/yyDh8+LDc3N91www0qU6ZMcdQHAAAAAACAEq7QAVOOChUqqFGjRkVZCwAAAAAAAEqhQs/BBAAAAAAAAFyMgAkAAAAAAACWFDpg2rBhg+Lj44uhFAAAAAAAAJRGhQ6Yxo8frzNnzhRHLQAAAAAAACiFCh0whYWFacOGDUpLSyuOegAAAAAAAFDKFPoucqdPn9Zbb72l+fPnq1KlSnJ3d7dr37RpU5EVBwAAAAAAgJKv0AFTt27d1K1bt+KoBQAAAAAAAKVQoQOmzp07m/8+d+6cypcvL5vNJpvNVqSFAQAAAAAAoHQo9BxMhmHo7bffVlhYmG655RbFxMRoxIgRGjNmDPMyAQAAAAAAXIcKHTDNmzdP69at09SpU+Xm5iYp+6ymzZs3a/r06UVeIAAAAAAAAEq2QgdMH3/8sSZMmKD27dubl8XdeuutmjZtmr744osiLxAAAAAAAAAlW6EDptOnT6tKlSq5lleoUEFJSUlFUhQAAAAAAABKj0IHTC1bttSiRYvsliUmJuq1115TWFhYkRUGAAAAAACA0qHQAdO4ceO0d+9e3XrrrUpNTdXTTz+ttm3bKiYmRqNHjy6OGgEAAAAAAFCCuRR2hapVq2r16tXaunWrDh06pIyMDN14441q3bq1nJwKnVcBAAAAAACglCt0wJSjatWqunDhglxdXXXjjTcSLgEAAAAAAFynCh0wHT9+XC+++KJ27NihihUryjAMnT9/Xh06dNCkSZPk5eVVDGUCAAAAAACgpCr0aUejR4+Ws7OzNm3apG3btmn79u364osvdPbsWY0ZM6Y4agQAAAAAAEAJVugzmHbs2KG1a9eqevXq5rJatWppzJgx6t69e5EWBwAAAAAAgJKv0Gcw1alTR/v378+1PDo62i50AgAAAAAAwPWhQGcwffLJJ+a/W7ZsqVGjRmnv3r0KDg6Ws7Oz9u3bp/fee09PPPFEcdUJAAAAAACAEqpAAdPs2bPtHnt7e+vzzz/X559/bi4rX7681qxZo6effrpoKwQAAAAAAECJVqCA6dtvvy3uOgAAAAAAAFBKFXqSb0mKiorSoUOHlJaWlqutU6dOVmsCAAAAAABAKVLogGnmzJl699135ePjI3d3d7s2m81GwAQAAAAAAHCdKXTAtGLFCk2aNEkPPfRQcdQDAAAAAACAUsapsCuUL19ewcHBxVELAAAAAAAASqFCn8E0cuRITZgwQcOGDZO/v7+cnOwzKn9//yIrDgAAAAAAACVfoQOmlJQU/fHHH+rVq5dsNpu53DAM2Ww2/fnnn0VaIAAAAAAAAEq2QgdMM2bMULdu3dStWzd5eHgUR00AAAAAAAAoRQodMKWlpemxxx5TjRo1iqMeAAAAAAAAlDKFnuS7b9++WrBggVJTU4ujHgAAAAAAAJQyhT6DafPmzdq9e7c++eQT+fr6ytnZ2a5906ZNRVYcAAAAAAAASr5CB0xdunRRly5diqMWAAAAAAAAlEKFDpg6d+5cHHUAAAAAAACglCp0wPT444/LZrPl275s2TJLBQEAAAAAAKB0KXTAFBYWZvc4IyND0dHR+uGHHzRo0KAiKwwAAAAAAAClQ6EDpiFDhuS5fO3atfrqq6/Ur18/y0UBAAAAAACg9HAqqidq3ry5tm7dWlRPBwAAAAAAgFKi0GcwHTt2LNeyCxcuaNGiRapevXqRFAUAAAAAAIDSo9ABU4cOHXJN8m0YhqpVq6bJkycXWWEAAAAAAAAoHQodMG3atMnusc1mk6urq3x9fS95dzkAAAAAAABcmwodMHEZHAAAAAAAAC5WoIApr8vi8mKz2fTNN99YLgoAAAAAAAClR4ECpqFDh+bblpSUpMWLFysmJkahoaFFVhgAAAAAAABKhwIFTJ07d85z+aZNmzRnzhwlJSXp1Vdf1cMPP1ykxQEAAAAAAKDkK/QcTJIUExOjV199VT/88IO6dOmi4cOHy8vLq4hLAwAAAAAAQGlQqIApIyNDixYt0ttvv62aNWtq+fLlXBYHAAAAAABwnStwwLRt2zZNmDBBJ0+e1LPPPqtevXrJycmpOGsDAAAAAABAKVCggGn48OH67LPPVL16dY0bN05+fn6KiIjIs2/z5s2LtEAAAAAAAACUbAUKmDZs2CBJ+ueffzR8+PB8+9lsNv35559FUxkAAAAAAABKhQIFTFFRUcVdBwAAAAAAAEopJlECAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwpEQHTGlpaRo/fryaN2+uVq1a6bXXXpNhGHn23bJli+6//36FhISoV69eio6Otmt/77331KZNG4WGhurll19WcnKy2ZaamqqXX35ZzZo1U+vWrbV48eJi3S4AAAAAAIBrSYkOmF599VVt2bJFixYt0qxZs7Ry5UqtWLEiV79jx45p8ODB6tKli1avXq1KlSrp6aefNsOoL7/8UnPnztWECRO0dOlSRUZGasaMGeb606dP1++//66lS5dq7Nixmjt3rjZu3HjVthMAAAAAAKA0K7EBU3x8vNasWaOJEyeqUaNGuuWWW9S3b19FRkbm6rtq1So1bNhQffv2Vd26dTVlyhTFxMRo+/btkqRly5apd+/eat++vRo1aqTx48drzZo1Sk5OVlJSklatWqVRo0apQYMG6tixo/r376/ly5df7U0GAAAAAAAolUpswBQREaFy5cqpRYsW5rKBAwdqypQpufpGRkaqWbNm5mNPT081aNBAu3fvVmZmpn777Te79saNGys9PV1RUVGKiopSRkaGQkNDzfamTZsqMjJSWVlZxbR1AAAAAAAA1w4XRxeQn+joaFWvXl2ffPKJ5s+fr/T0dHXp0kWDBg2Sk5N9LhYbG6sqVarYLfPx8dGJEyeUkJCg1NRUu3YXFxd5eXnpxIkTcnJykre3t9zc3Mx2X19fpaamKj4+XpUqVSpwzTbbFW4sroqc48NxgqMwBuFojEE4GmMQAHA9Kwnff1fyXVzQviU2YEpKStKRI0f00UcfacqUKYqNjdWYMWPk6empvn372vVNTk62C4gkyc3NTWlpaUpJSTEf59VuGEaebVL2JOOF4eNTvlD94RgcJzgaYxCOxhiEo11rY9DFxVmurs6OLgOFwPEqHs7Ozv/724l9fBnsn+uHi0v2sfb2LuvgSuwVx3dxiQ2YXFxclJiYqFmzZql69eqSsifz/u9//5srYHJ3d88VBqWlpalChQpyd3c3H/+73dPTU5mZmXm2SZKHh0ehaj59+rzyuckdSgCbLftNxHGCozAG4WiMQTjatTYGnZ2d5O1dVhkZmUpPz3R0OSggV1dnjlcxyczM/N/fWezjS2AMXl8yMrKP9dmzF5SZ6fhpeK7kuzhnncspsQFT5cqV5e7uboZLknTjjTfq+PHjufr6+fkpLi7ObllcXJxuvvlmeXl5yd3dXXFxcapTp44kKSMjQ/Hx8apcubIMw9DZs2eVkZEhF5fs3REbGysPDw9VqFChUDUbhq6J/yxd6zhOcDTGIByNMQhHYwwCAK5HJem7rzi+i0vsJN8hISFKTU3V33//bS47dOiQXeB0cd+IiAjzcXJysvbu3auQkBA5OTkpODjYrn337t1ycXFRvXr1dPPNN8vFxUW7d+822yMiIhQcHJxrricAAAAAAADkVmITlNq1a6tdu3YKDw9XVFSUfvrpJy1cuFA9evRQZmamYmNjzUvZHnroIf36669auHChDhw4oPDwcAUEBCgsLEyS1LNnTy1atEjffPON9uzZo3Hjxqlbt27y9PSUp6enOnXqpHHjxmnPnj365ptvtHjxYvXq1cuRmw8AAAAAAFBqlNiASZJmzpypG264QT169NDIkSP16KOP6vHHH9fx48fVunVr7dq1S5IUEBCgOXPmaM2aNXr44YcVHx+vefPmyfa/qc7vu+8+PfnkkxozZoz69u2rRo0aacSIEebrhIeHq0GDBurdu7fGjx+voUOH6s4773TINgMAAAAAAJQ2JXYOJkkqX768pk+fnmt5QECABg8ebE7gLUlt27ZV27Zt832ugQMHauDAgXm2eXp6atq0aZo2bZr1ogEAAAAAAK4zJfoMpvwkJiZq69atql+/vqNLAQAAAAAAuO6V6DOY8lOuXDktW7ZMrq6uji4FAAAAAADgulcqz2CSRLgEAAAAAABQQpTagAkAAAAAAAAlAwETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwpNQHTwIED9dJLL+XbvmXLFt1///0KCQlRr169FB0dbdf+3nvvqU2bNgoNDdXLL7+s5ORksy01NVUvv/yymjVrptatW2vx4sXFth0AAAAAAADXmlIRMH322Wf64Ycf8m0/duyYBg8erC5dumj16tWqVKmSnn76aRmGIUn68ssvNXfuXE2YMEFLly5VZGSkZsyYYa4/ffp0/f7771q6dKnGjh2ruXPnauPGjcW+XQAAAAAAANeCEh8wxcfHa/r06QoODs63z6pVq9SwYUP17dtXdevW1ZQpUxQTE6Pt27dLkpYtW6bevXurffv2atSokcaPH681a9YoOTlZSUlJWrVqlUaNGqUGDRqoY8eO6t+/v5YvX361NhEAAAAAAKBUK/EB07Rp0/Tggw/qpptuyrdPZGSkmjVrZj729PRUgwYNtHv3bmVmZuq3336za2/cuLHS09MVFRWlqKgoZWRkKDQ01Gxv2rSpIiMjlZWVVTwbBQAAAAAAcA1xcXQBl7J161bt3LlT69ev17hx4/LtFxsbqypVqtgt8/Hx0YkTJ5SQkKDU1FS7dhcXF3l5eenEiRNycnKSt7e33NzczHZfX1+lpqYqPj5elSpVKnC9NlvBtw1XX87x4TjBURiDcDTGIByNMQgAuJ6VhO+/K/kuLmjfEhswpaamauzYsRozZow8PDwu2Tc5OdkuIJIkNzc3paWlKSUlxXycV7thGHm2SVJaWlqhavbxKV+o/nAMjhMcjTEIR2MMwtGutTHo4uIsV1dnR5eBQuB4FQ9nZ+f//e3EPr4M9s/1w8Ul+1h7e5d1cCX2iuO7uMQGTHPnzlXDhg3Vpk2by/Z1d3fPFQalpaWpQoUKcnd3Nx//u93T01OZmZl5tkm6bLD1b6dPn9f/5hVHCWSzZb+JOE5wFMYgHI0xCEe71sags7OTvL3LKiMjU+npmY4uBwXk6urM8SommZmZ//s7i318CYzB60tGRvaxPnv2gjIzHT8Nz5V8F+esczklNmD67LPPFBcXZ86NlBP6fPnll9q1a5ddXz8/P8XFxdkti4uL08033ywvLy+5u7srLi5OderUkSRlZGQoPj5elStXlmEYOnv2rDIyMuTikr07YmNj5eHhoQoVKhSqZsPQNfGfpWsdxwmOxhiEozEG4WiMQQDA9agkffcVx3dxiQ2Y3n//fWVkZJiPZ86cKUkaPnx4rr4hISGKiIgwHycnJ2vv3r0aMmSInJycFBwcrIiICIWFhUmSdu/eLRcXF9WrV09S9pxMu3fvNicCj4iIUHBwsJycSvwc6AAAAAAAAA5XYgOm6tWr2z0uWzb7esWaNWsqMzNTZ86cUcWKFeXm5qaHHnpIixYt0sKFC9W+fXvNmzdPAQEBZqDUs2dPjRkzRoGBgapSpYrGjRunbt26ydPTU5LUqVMnjRs3TpMnT9apU6e0ePFiTZky5epuMAAAAAAAQClVKk/ROX78uFq3bm1eKhcQEKA5c+ZozZo1evjhhxUfH6958+bJ9r+pzu+77z49+eSTGjNmjPr27atGjRppxIgR5vOFh4erQYMG6t27t8aPH6+hQ4fqzjvvdMi2AQAAAAAAlDYl9gymf5s6dar574CAAA0ePNicwFuS2rZtq7Zt2+a7/sCBAzVw4MA82zw9PTVt2jRNmzat6AoGAAAAAAC4TpTKM5gSExO1detW1a9f39GlAAAAAAAAXPdKzRlMFytXrpyWLVsmV1dXR5cCAAAAAABw3SuVZzBJIlwCAAAAAAAoIUptwAQAAAAAAICSgYAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJaU6IDp5MmTGjZsmFq0aKE2bdpoypQpSk1NzbPv3r171bVrV4WEhOihhx7S77//bte+YcMG3XHHHQoJCdHgwYN15swZs80wDM2cOVMtW7ZUixYtNH36dGVlZRXrtgEAAAAAAFwrSmzAZBiGhg0bpuTkZC1fvlyvv/66vvvuO73xxhu5+iYlJWngwIFq1qyZ1q5dq9DQUD355JNKSkqSJO3Zs0ejRo3SkCFDtGLFCiUkJCg8PNxcf8mSJdqwYYPmzp2r2bNna/369VqyZMnV2lQAAAAAAIBSrcQGTIcOHdLu3bs1ZcoU1a1bV82aNdOwYcO0YcOGXH0///xzubu768UXX1SdOnU0atQolS1bVhs3bpQkffDBB7rnnnvUqVMn1atXT9OnT9cPP/yg6OhoSdKyZcs0bNgwNWvWTC1bttTw4cO1fPnyq7q9AAAAAAAApVWJDZgqV66sd999V76+vnbLExMTc/WNjIxU06ZNZbPZJEk2m01NmjTR7t27zfZmzZqZ/atVqyZ/f39FRkbq5MmTOn78uJo3b262N23aVDExMTp16lQxbBkAAAAAAMC1xcXRBeSnQoUKatOmjfk4KytLH3zwgVq2bJmrb2xsrG666Sa7ZT4+Pjpw4IAk6dSpU6pSpUqu9hMnTig2NlaS7NpzQq0TJ07kWu9S/pdvoYTKOT4cJzgKYxCOxhiEozEGAQDXs5Lw/Xcl38UF7VtiA6Z/mzFjhvbu3avVq1fnaktOTpabm5vdMjc3N6WlpUmSUlJS8m1PSUkxH1/cJslcv6B8fMoXqj8cg+MER2MMwtEYg3C0a20Murg4y9XV2dFloBA4XsXD2dn5f387sY8vg/1z/XBxyT7W3t5lHVyJveL4Li4VAdOMGTO0dOlSvf766woMDMzV7u7unisMSktLk4eHxyXbPT097cIkd3d389+S5OnpWag6T58+L8Mo1Cq4imy27DcRxwmOwhiEozEG4WjX2hh0dnaSt3dZZWRkKj0909HloIBcXZ05XsUkMzPzf39nsY8vgTF4fcnIyD7WZ89eUGam4+9WfyXfxTnrXE6JD5gmTpyo//73v5oxY4buuuuuPPv4+fkpLi7ObllcXJx5eVt+7ZUrV5afn5+k7MvsAgICzH9L2fNAFYZh6Jr4z9K1juMER2MMwtEYg3A0xiAA4HpUkr77iuO7uMRO8i1Jc+fO1UcffaTXXntN9913X779QkJCtGvXLhn/2zuGYejXX39VSEiI2R4REWH2P378uI4fP66QkBD5+fnJ39/frj0iIkL+/v6Fmn8JAAAAAADgelViA6a//vpLb731lgYMGKCmTZsqNjbW/CNln2WUM3/S3XffrYSEBE2aNEkHDx7UpEmTlJycrHvuuUeS1KNHD3366adatWqVoqKi9OKLL6pdu3aqUaOG2T5z5kxt27ZN27Zt06xZs9SrVy/HbDgAAAAAAEApU2Ivkdu0aZMyMzP19ttv6+2337Zr27dvn1q3bq0pU6aoS5cuKleunBYsWKCxY8dq5cqVCgoK0sKFC1WmTBlJUmhoqCZMmKDZs2fr3LlzuvXWWzVx4kTz+fr166fTp09ryJAhcnZ21sMPP6w+ffpczc0FAAAAAAAotUpswDRw4EANHDgw3/bJkyebk3JLUqNGjfTxxx/n279Lly7q0qVLnm3Ozs4KDw9XeHj4lRcMAAAAAABwnSqxl8hdSlZWlj7++GOFhYU5uhQAAAAAAIDrXok9g+lSnJyctGTJErm6ujq6FAAAAAAAgOteqTyDSRLhEgAAAAAAQAlRagMmAAAAAAAAlAwETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAAAAAABYQsAEAAAAAAAASwiYAAAAAAAAYAkBEwAAAAAAACwhYAIAAAAAAIAlBEwAAAAAAACwhIAJAAAAAAAAlhAwAQAAAAAAwBICJgAAAAAAAFhCwAQAAAAAAABLCJgAAAAAAABgCQETAAAAAAAALCFgAgAAAAAAgCUETAAAAAAAALCEgAkAAAAAAACWEDABAAAAAADAEgImAAAAAAAAWELABAAAAAAAAEsImAAAAAAAAGAJARMAAAAAAAAsIWACAAAAAACAJQRMAAAAAAAAsISACQAAAAAAAJYQMAEAAAAAAMASAiYAAICrLC0tTY8/3k2//roz3z7bt/+i3r17qGPHNnrmmad19Ohhs80wDC1atECdO9+ru+9urzFjwnX27Fmzfc2albrvvtvVo0cX/f77b3av2717Z8XFxRXLdgEAgOsXARMAAMBVlJqaqnHjRunvvw/l2+fQob80YsQzatOmrRYtel9BQUEaNmyQkpKSJEmffrpWn322TmPGTNRbb72juLhYTZs2UZJ09uxZzZv3piZOnKZ77/2PXnttqvm8GzZ8qltuaS1fX9/i3UgAAHDdIWACAAC4Sv7++5CefPIJxcT8c8l+n3yyWsHBIerf/yndcEMtDRo0TOXKldNXX30hSfrll83q0KGjQkObqnbtm9SzZy9FROyQJB07FqPy5curSZNmatu2vY4cOSxJSk9P18qV/9Wjj/Yu1m0EAADXJwImAACAq2T37l/VpElTLViw5JL9jh2LUf36DczHNptNtWvfpD/+yL7crUKFitq69WfFxp5SamqKvvnmS9WtGyRJ8vPz0/nzCTpx4oSioqLk51dVkvTZZ5+qZctWnL0EAACKhYujCwAAALhedO78cIH6Varko9jYWLtlp06dVIUKFSRJTzwxQCNHPqfOne+Vs7OzfHx8NX/+YkmSr29lPfxwdz3yyINyc3PTuHGTlZGRoZUr/6vZsxcU7QYBAAD8D2cwAQAAlDAdOnTUd999o82bf1JGRob+r707D6/pWvg4/ss8IxG0hkYMJa0kjhBzq9FrJmY1BCWoWU01T6HacFFRNaS0pfQWkdZQLdrXpe2NFpFQShJa1JSoOXPy/uF13p4bQ/RkIL6f58nT7LX3Xnvtk3WW5Ne19vnqq606duyo0tPTJUnnz/8hOzt7vfvuQoWFLVepUqU1d+4s4/lDhozQli07tXXrLjVs2Fjbtn0pf//6srKy1KhRQ9S5c1t9+unHhXV7AACgCCJgAgAAeMzUq9dAr78+QFOmjFdAQAPt2LFdLVq0lpOTk7KzszV79nS99lpPNWzYWD4+NTVr1lz9/PN+HT16xFhHsWLFZGdn93+zl9apV68++vDD5fL0rKzVq9dp06bPdfz4sUK8SwAAUJQQMAEAADyG+vTpr6+/3qMvvtih995bqtu3b+uZZ8rq6tU/denSRVWp8rzx2DJlnlHx4iV04cL5HPV89dVW1alTV+7upRQbe1j+/vXk4uKiGjV8FBMTXYB3BAAAijICJgAAgMfMzp079N57/5Stra1cXd2UmpqiQ4d+Vq1ateXiUky2trY6fTrBePzVq1d1/fo1lS1b1qSejIwM/etfn6pXr76SJAsLS2VnZ0uSMjMzJGUX1C0BAIAijod8AwAAPAaSkhLl7OwsOzt7VajgoblzZ6lmTYMqVaqipUsXq3TpMqpXr4EsLS3VqlVbLVnynooXL6FixYppyZL39OKLNVS9+gsmde7YsU1+fnXk7l5KkuTl9YK++eYrubu769Chg+revXdh3CoAACiCmMEEAADwGAgMbKHdu3dKkqpX99KYMRO0ZMki9e8fJEkKDX1PlpZ3fnUbPny0Xn75Fc2cOUXDhg2Si4uz3n77n7KwsDDWl5GRoc8++//ZS9KdT5+7cOG8RowYrE6duqpGDe+Cu0EAAFCkWWTfnScNsyUm3hCv5uPLwkJyd3fh54RCQx9EYaMPorAVtT5obW0pV1cnjf3XIZ1KvFXYzUEu2dhYKT09s7CbUSQ1ququN5tV17jPo5Vw+WZhN+exRR98uni6O2l+N4P+/POWMjKyCrs5f+vf4rvnPAxL5AAAQK5ZWlrI0tLi4QcCD2BlVTQm0ReV+wAAIC8QMAEAgFyxtLRQCVdHWVnyRzXM4+rqVNhNAAAAeYyACQAA5IqlpYWsLC21aOevOnvldmE3B08oa2srZWQUjaUhBg9X9axX0eTZVwAAPK0ImAAAwCM5e+U2z5vB31aUnj1SztWhsJsAAMBjgznuAAAAAAAAMAsBEwAAAAAAAMxCwAQAAAAAAACzEDABAPJNamqq5s6dpRYtmigwsLnWr19732N/+GGf+vbtoX/8o7H69HlN+/btMe7Lzs7W2rUfqUuXdmrW7GWNHDlYp04lGPfv2fOtAgObq2PH1tq3798m9Q4Y0EcnThzP+5sDAAAAYETABADIN0uXvqfjx4/pvfeWafToCVq9eqW++25XjuPi4k5q8uRxat26nVavXqd27TpqypS3dPLkCUnSF19s0mefrdWoUeMUHv6Jnn22rMaOHaGUlBRlZmYqNHSOhg4dpYEDh2ju3JnKzs6WJP344z65u7vr+eerF+h9AwAAAE8bAiYgDz3KbI0TJ45rwIA+atq0oYKDe+v48WPGfZmZmfrggzC1a9dc//jHS5o6dYKuXEky7me2Bp4EycnJ2rLlC40cOUbVqlXXyy+/oh49grRp0+c5jt25c4dq1aqjLl1eU/nyFdSpU1fVqlVb3367U5K0fftWvfZaLzVs2FjPPeehsWMn6vr1a4qNjda1a1d17do1vfLKqwoI+IeuXbumq1f/lCStXh2u118fUKD3DQAAADyNCJiAPJTb2RrJyckaN26kfH0N+vDDtapRw0fjx49ScnKyJGnt2o+0e/c3mjVrrlas+EjXr19XSMg0SWK2Bp4YcXEnlJmZIW9vX2OZj09N/fLLUWVlZZkc27JlG73xxrAcddy6dVOSNHToKDVr1tJkX3Z2tm7evKnixUvI3t5ev/56XL/+ekwODg4qVqy4/vOfH+Tm5sb7AQAAACgABExAHnmU2Rq7d38jW1t7DR06UhUremrkyDFydHQ0hlGZmZkaPny0atasJU/PSurSpZtiYqIlidkaeGIkJSWqePESsrGxMZa5uZVUWlqqrl27ZnJsxYqeqlr1eeN2QkK8Dhz4SX5+/pIkX9+aKl26jHH/1q2RyszMlI9PTVlZWWnw4OEaOjRYI0a8oWHD3pSVlZU++ihc/foNzOe7BAAAACBJ1oXdAKCouN9sjU8+Wa2srCxZWv5/nnv06BH5+PjKwsJCkmRhYSFvb18dORKjVq3amvxR/OefV7RlS6QMBj9JMpmtkZ2dxWwNPLZSUlJMwiVJxu309LT7nnf16lVNmTJe3t6+atz45Rz7jx49oiVLFql79yCVLOkuSerUqZtatmwjyUKOjo7av/8/KlGihCpU8NDUqRN09GisXnmlqYYNe9P4vgMAAACQd5jBBOSRR5mtkZSUKHf3UiZlrq5uunz5kknZhx8uV9u2zRQTc1jDhr0pSczWwBPD1tZO6enpJmV3t+3t7e95zpUrSRo58g1lZWUrJORdk2BWko4cidGYMcNUr14DBQe/YbLP0dFJjo6OkqTVq1fq9dcHatOmfykzM1Pr12/S4cPR2rPn27y6PQAAAAB/QcAE5JFHma2RmpoiW1tbkzJbW1ulpZn+Md68eSuFh3+i2rX9NXr0MOPzaDp16qavvvpW27btVmBgxxyzNTp2bK2wsAXGZzMBhaFUqVK6du2qMjIyjGVXriTJzs5Ozs4uOY6/fPmShg4doLS0NC1Zslyurq4m+w8e/FlvvjlUtWrV0YwZb+cIn+766acoFStWTNWqVVds7GHVqVNXdnb2qlWrtnGpKQAAAIC8RcAE5JFHma1xJ0wyDZ3S0tJkb29nUla+fAVVr/6CpkyZqdTUFO3Z851xH7M18LirWrWarKysdfToEWNZTEy0vLxezBEOJScna8yY4bK0tNSSJStyzPBLSIjThAljVLduA82aNVfW1vdf4f3RR+F6/fU7s/ksLS2NDxTPzMwUmSsAAACQPwiYgDzyKLM13N1L68qVJJOyK1eSjM+T+f77vSbL5ezs7FS2bDldvXo1x3WZrYHHlb29vVq2bK3589/WsWNH9e9//4/Wr1+jLl1ek3RnqWhqaook6ZNPVuncubOaPHmGcV9SUqJu3rwza2/evLdVunQZDR/+pq5du2rcf/f8uw4c+ElOTs6qXt1LklS9+gv67rtdSkiI1/ff71WNGt4FdPcAAADA04WACcgjjzJb48UXayg2Nsa4hC07O1uxsYf14ot3/vh9//1F2rFjm/H427dv6cyZ31WxomeO6zJbA4+z4cNHq1o1L40Y8YYWLHhX/fsP0ssvB0iSAgNbaPfunZKkPXu+VWpqqgYO7KvAwBbGr/fem6+kpETFxsbo9OkEderUxmT/3fPvWr16pfr1+/9PUuzcuZscHBw0eHA/GQx+euWVVwvu5gEAAICnCJ8i939SU1M1c+ZMffPNN7K3t1e/fv3Ur1+/wm4WniB/na0xadJ0Xb58WevXr9GkSdMl3ZmR4ezsLDs7e73ySlMtW7ZE7733TwUGdtQXX0QoJSVZAQH/kCR17NhFH364QlWqVFWZMs9qxYr3Va5cBdWr18DkmvebrWEw+On77/dqwADThyADBc3e3l5TpszUlCkzc+zbt+9n4/fr1m16YD1/PfZBlixZYbLt5OSsefPey9W5AAAAAP4+Aqb/ExoaqiNHjujjjz/WH3/8obfeektly5ZVixYtCrtpeIIMHz5a8+fP1YgRb8jJyTnHbI1Jk6arVau2cnJyVmjoQs2fP1dffrlZlStX0bx578nBwUGS1LFjVyUnp2j+/Hd09eqf8vevp3ffXZBjJtTq1Ss1bNgo43bnzt105EiMBg/up4CAZszWMJOlpYUsLfP+I+2trJg8isL1d/sgfRcAAAD3Y5HNx0zp9u3bqlevnlauXKm6detKkpYuXaoff/xRa9asyXU9iYk3WJL0GLOwkNzdXfg5IVcsLS1UwtVRVvf5pDLgaTb2X4d0KvFWYTcDTygbGyulp2cWdjPyRKOq7nqzWXWN+zxaCZdvFnZzkEtFqQ8+bnhP5A598Oni6e6k+d0M+vPPW8rIyCrs5vytv4vvnvMwzGCSdPz4cWVkZMhgMBjL/Pz8tGzZMmVlZd33o7CLmvyarfG44f/AIzesrCxlZWmpRTt/1dkrt/OsXmtrK2Vk8AsFCo85fdDg4aqe9SrKwqLo/1sBAACAR0PAJOny5ctydXWVra2tsczd3V2pqam6evWq3NzcclWPpaWe2JkxFhYWKl7i6Zit4erqVNhNwBPEzspSdtZ5976wtraUlZ7QgQJFgjl90NrqTrBUyd1ZtlaETPh7ilLQXtb1ztJ23hNPlqLUBx83vCdyhz74dCnn6mj8/nH4c/vu/yd8lPwit/9vkYBJUnJyskm4JMm4nZaWlut63NwePmUMwJNlcEDVwm4C8NgZHFClsJsAPFZ4TwCmeE8AOT1uEx3yI794DPKzwmdnZ5cjSLq7bW9vXxhNAgAAAAAAeGIQMEkqU6aM/vzzT2VkZBjLLl++LHt7exUrVqwQWwYAAAAAAPD4I2CS5OXlJWtra0VHRxvLDhw4IG9v76fmAd8AAAAAAAB/F+mJJAcHB7Vv314zZsxQTEyMdu3apVWrVql3796F3TQAAAAAAIDHnkV29pP6uWd5Kzk5WTNmzNA333wjZ2dn9e/fX3379i3sZgEAAAAAADz2CJgAAAAAAABgFpbIAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsCEIuXixYsaMWKE/P391bhxY82dO1epqan3PPaXX35Rly5d5Ovrq06dOunIkSMF3FoURY/SBwcPHqxq1aqZfH333XcF3GIUNb/99pv69+8vg8GgJk2aKDw8/L7HMg4iPzxKH2QcRH4bOHCgJkyYcN/9P/zwg9q0aSNfX1/17t1bZ86cKcDW4WnwsD7Yrl27HOPgiRMnCrCFKIp27tyZo1+NGDHinsfm5ThIwIQiIzs7WyNGjFBycrI+/fRTLVy4UN99950WLVqU49jbt29r4MCBql27tiIiImQwGDRo0CDdvn274BuOIuNR+qAkxcfHa968edq3b5/xq2HDhgXbaBQpWVlZGjhwoFxdXbV582bNnDlTH3zwgbZs2ZLjWMZB5IdH6YMS4yDy17Zt27Rnz5777v/jjz80dOhQdezYURs3bpSbm5uGDBkiPgMJeeVhfTAzM1OnT5/W2rVrTcbBSpUqFWArURTFxcXplVdeMelXs2fPznFcXo+DBEwoMhISEhQdHa25c+eqatWqql27tkaMGKGtW7fmOHb79u2ys7PT+PHjVblyZU2ePFlOTk7asWNHIbQcRcWj9MG0tDSdPXtW3t7eKlWqlPHL1ta2EFqOoiIxMVFeXl6aMWOGKlasqJdffln169fXgQMHchzLOIj88Ch9kHEQ+enq1asKDQ2Vt7f3fY/ZsGGDatSooX79+qlq1aqaO3euzp07p/379xdgS1FU5aYPnj17Vunp6fLx8TEZB62trQuwpSiK4uPj9fzzz5v0q2LFiuU4Lq/HQQImFBmlSpVSeHi43N3dTcpv3ryZ49jDhw/Lz89PFhYWkiQLCwvVqlVL0dHRBdFUFFGP0gcTEhJkYWGhChUqFFTz8BQoXbq0Fi1aJGdnZ2VnZ+vAgQP66aef5O/vn+NYxkHkh0fpg4yDyE/vvvuuAgMDVaVKlfsec/jwYdWuXdu47eDgoBdffJFxEHkiN30wLi5Ozz77rOzs7AqwZXgaxMfHq2LFig89Lq/HQQImFBnFihVT48aNjdtZWVlau3at6tWrl+PYy5cvq3Tp0iZlJUuW1IULF/K9nSi6HqUPJiQkyNnZWePHj1ejRo3UuXPnB06hBh5VQECAevToIYPBoObNm+fYzziI/PawPsg4iPzy448/6ueff9aQIUMeeBzjIPJLbvtgfHy8bGxsNGjQIDVs2FC9evVSTExMAbUSRVV2drZOnTqlffv2qXnz5nr11Vc1f/58paWl5Tg2r8dBAiYUWfPmzdMvv/yiN998M8e+5OTkHFPwbW1t7/mmA/6uB/XBhIQEpaSkqFGjRgoPD9fLL7+swYMHKzY2thBaiqJo8eLFWrZsmY4dO6a5c+fm2M84iPz2sD7IOIj8kJqaqunTp2vatGmyt7d/4LGMg8gPj9IHT506pWvXrqlLly5asWKFKleurD59+uj8+fMF1FoURX/88YdxfFu0aJHeeustbdmyRaGhoTmOzetxkMWdKJLmzZunjz/+WAsXLtTzzz+fY7+dnV2ON01aWtpD/xEAcuthfXDIkCEKCgpS8eLFJUnVq1fX0aNH9fnnnz9wrT6QW3f7UWpqqsaOHavx48eb/ALBOIj89rA+yDiI/LBkyRLVqFHDZEbx/dxvHLzXc0qA3HqUPhgSEqKUlBQ5OztLkmbMmKGDBw/qiy++0BtvvJHfTUURVa5cOUVFRal48eKysLCQl5eXsrKyNG7cOE2cOFFWVlbGY/N6HCRgQpETEhKi9evXa968efecki9JZcqUUWJioklZYmJijumBwN+Rmz5oaWlp/KPqrkqVKikuLq4gmogiKjExUdHR0Xr11VeNZVWqVFF6erpu3rwpNzc3YznjIPLDo/RBxkHkh23btikxMVEGg0GSjH84ff311zp06JDJsfcbB728vAqmsSiSHqUPWltbG8Ml6c7zECtVqqSLFy8WXINRJJUoUcJku3LlykpNTdW1a9dy9fvg3x0HWSKHImXJkiX67LPPtGDBArVu3fq+x/n6+urQoUPGj1/Mzs7WwYMH5evrW1BNRRGV2z44YcIETZw40aTs+PHjfCwtzHL27FkNGzbM5BfTI0eOyM3NzeSXCYlxEPnjUfog4yDyw5o1a7RlyxZFRkYqMjJSAQEBCggIUGRkZI5jfX19TT7hMDk5Wb/88gvjIMzyKH0wKChIS5YsMW5nZWXp119/ZRyEWfbu3au6desqOTnZWHbs2DGVKFHinr8P5uU4SMCEIiM+Pl5Lly7VgAED5Ofnp8uXLxu/pDsPMEtJSZEktWjRQtevX9ecOXMUFxenOXPmKDk5WS1btizMW8AT7lH6YEBAgPGXj99++01LlizRgQMH1KtXr8K8BTzhvL299eKLL2rSpEmKi4vTnj17NG/ePOM0e8ZB5LdH6YOMg8gP5cqVk4eHh/HLyclJTk5O8vDwUGZmpi5fvmycUdKpUycdPHhQK1as0MmTJzVx4kSVL19edevWLeS7wJPsUfpgQECAPvroI+3evVsJCQmaNWuWbty4oQ4dOhTyXeBJZjAYZGdnpylTpighIUF79uxRaGiogoOD830cJGBCkbF7925lZmbqgw8+UKNGjUy+JKlRo0bavn27JMnZ2VnLly/XgQMH1LFjRx0+fFgrVqyQo6NjYd4CnnCP0gebNWum6dOn64MPPlCbNm307bffKjw8XOXLly/MW8ATzsrKSkuXLpWDg4O6deumyZMnKygoSL1795bEOIj89yh9kHEQBe38+fNq1KiRcZlS+fLlFRYWpk2bNqlz5866evWq3n//fVlYWBRyS1FU/Xcf7Nu3r4KDgzV79mwFBgYqLi5Oq1evNlk2BzwqZ2dnffjhh7py5Yo6deqkyZMnq1u3bgoODs73cdAi++7ceAAAAAAAAOBvYAYTAAAAAAAAzELABAAAAAAAALMQMAEAAAAAAMAsBEwAAAAAAAAwCwETAAAAAAAAzELABAAAAAAAALMQMAEAAAAAAMAsBEwAAAAAAAAwCwETAAB46gwcOFATJ040Kdu6dauqVaumsLAwk/KlS5cqMDDwoXWGhYUpKCgoV9efMGGCJkyYcN/9SUlJ+uqrr3JV19+p/1H9+OOPio+PlyRFREQoICAgz+oGAABFAwETAAB46tSuXVuxsbEmZVFRUSpdurSioqJMyqOjo+Xv7//QOvv165cjnPq75s+frz179uRJXXmhb9++SkxMlCS1atVKGzduLOQWAQCAxw0BEwAAeOr4+fkpPj5et27dMpZFRUWpf//+io6OVkpKirH88OHDuQqYnJycVKJEiTxpX3Z2dp7Ukx/s7e3l5uZW2M0AAACPGQImAADw1PH29paNjY2OHj0qSbpw4YL++OMPdenSRS4uLjp48KAk6dSpU7p27Zpq164tSTpx4oSCgoLk4+Oj5s2b69NPPzXW+d9L5Pbt26e2bdvKx8dHwcHBCgkJMVm2dvPmTb355pvy9fVVkyZNtGXLFmM9mzdv1ubNm41L0a5fv65x48apVq1aatSokUJCQkxCsJ9//lnt27eXj4+PRo4cqeTk5Pve+93lc+3atVP9+vV1+vRpxcXFqX///jIYDPL29laPHj2MS+LutqF3794KCwszWSIXFRWlgIAArVu3To0bN1bNmjU1btw4paWlGa/35Zdf6tVXX5Wvr6/GjBmj0aNH59lMLwAA8PggYAIAAE8dW1tb+fr6KiYmRpL0n//8RzVq1JCTk5Pq1KljXCYXHR2tqlWrytXVVSkpKRowYID8/Pz05Zdf6q233tLSpUsVGRmZo/4zZ85o8ODBatmypSIjI+Xt7W0SRknSzp079eKLL2rr1q1q2bKlJk2apBs3bqhfv35q2bKlWrZsaVyKNnnyZN24cUPr16/X0qVLFRsbq1mzZkmSrly5okGDBqlBgwaKjIxUlSpVtGPHjgfe/xdffKFRo0Zp+fLleu655/TGG2+oXLly+uKLL/TZZ58pMzNT8+bNkyRjG8LCwtSvX78cdV26dElff/21wsPDFRYWpm+++cb4mvz888+aNGmSgoODFRERIQcHB23fvj2XPyUAAPAksS7sBgAAABSG2rVrGwOmqKgo1a1bV5Lk7++vrVu3SjJ9/tKWLVtUsmRJjRo1SpJUsWJFnTt3Tp988onat29vUveGDRvk4+OjIUOGSJJGjhypH374weQYg8Gg4OBgSdKQIUO0atUqJSQkyNfXV/b29pIkNzc3/f7779q1a5f2798vFxcXSVJISIjat2+viRMn6quvvpKbm5vGjRsnCwsLDR8+/KHPb/L29jbOQrp9+7Zee+019ejRQ46OjpKkDh06KDw83NgGSSpevLicnJxy1JWenq4pU6aoatWqqlatmho3bqzY2Fh17dpV69evV6tWrfTaa69JkmbMmKF9+/Y9sG0AAODJRMAEAACeSrVr1zbOtImKilJISIikOwHTO++8o7S0NEVHR2vw4MGSpISEBB0/flwGg8FYR2ZmpqysrHLU/euvv8rb29ukrGbNmrp27Zpxu0KFCsbv7wZHqampOeqKj49XVlaWXnrpJZPyrKws/fbbb4qLi1P16tVlYWFh3Oft7f3AZXLlypUzfu/o6Kju3bsrMjJSR44cUUJCgn755Re5u7vf9/z/5uHhYfze2dlZGRkZku68Dt26dTPus7a2Vo0aNXJdLwAAeHIQMAEAgKeSwWDQpUuXFBsbq0uXLqlWrVqSpKpVq8rFxUU//fST4uLijDOYMjIyVL9+fU2bNu2hdVtZWeV4UPd/b98rmLrXw70zMzPl4uKiTZs25dhXpkyZe55nY2PzwIDJzs7O+P2tW7fUuXNnubq6KiAgQG3atFFCQoJWrVp13/P/m62t7T3vIzevAwAAKBp4BhMAAHgqOTo6ysvLS//617/k7e0tBwcHSZKFhYXq1KmjiIgIVaxY0bhEzNPTU6dOnVL58uXl4eEhDw8PRUdHa82aNTnqrlq1qvEB4nf99/aD/HU2kqenp27cuCELCwvjdVNSUhQaGqq0tDRVrVpVv/zyizIzM43nHDt2LNfX2r9/vy5duqRPPvlEwcHBatCggf744488CYKqVKlict+ZmZmP1DYAAPDkIGACAABPrTp16mjbtm3GWUp3+fv7a/fu3apTp46xrF27dkpJSdG0adMUHx+vPXv2aM6cOSpZsmSOert27aro6GitWLFCp06d0rJly/Tzzz+bBEcP4uDgoHPnzunixYuqXLmyGjdurLFjxyomJkZHjx7VxIkTdfv2bRUrVkytW7dWcnKy5syZo4SEBIWHh+vAgQO5fg1KlCih27dva9euXTp79qw2bNigTz/91OST4BwdHXXy5EnduHEj1/VKUq9evbRt2zZt2LBBCQkJevvtt3Xu3Llcvw4AAODJQcAEAACeWn5+frp9+7bxAd93+fv7Kzk52SR4cnZ21sqVK3X69Gm1b99eU6ZMUc+ePTVo0KAc9ZYrV06LFy/Wpk2b1LZtWx06dEhNmzaVjY1NrtoVGBioU6dOqV27dsrOzlZoaKjKly+vvn376vXXX5enp6cWLFgg6c7Dt8PDwxUbG6vAwED98MMPCgwMzPVrYDAYNHToUM2cOVPt2rVTRESEpk2bpqSkJF28eFGSFBQUpNDQUIWFheW63rt1T58+Xe+//746dOigmzdvymAw5Pp1AAAATw6LbBbCAwAA5KkTJ04oIyNDL7zwgrFs4MCB8vb21vDhwwuxZQUrJiZGzs7OqlSpkrGsdevW6t+/vzp27FiILQMAAHmNGUwAAAB57Pfff9frr7+u77//XufOndOGDRv0448/6h//+EdhN61AHTp0SIMGDdLBgwd15swZLVu2TOfPn1fjxo0Lu2kAACCP8SlyAAAAeezVV1/VyZMnNXnyZCUlJcnT01MLFy5U9erVC7tpBapnz546e/ashg8frhs3bsjLy0srV65UqVKlCrtpAAAgj7FEDgAAAAAAAGZhiRwAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALARMAAAAAAADMQsAEAAAAAAAAsxAwAQAAAAAAwCwETAAAAAAAADALAVMemTBhgqpVq3bfr6ioqAee/9VXXykpKSlX1woKClJYWFheNNsshd2O6OhoNWvWTN7e3tqwYYPJvrNnz6patWo6e/Zsvlw7ICBAEREReV5vdHS0XnvtNRkMBjVv3jzHfe3fv1+BgYHy9fVV165ddfz48XvWExERcc9+WL16dUl3fnb32j9x4sQcdYWHhysgICDP7xUAAAAAUHRYF3YDiorJkydrzJgxkqTt27dr1apV2rhxo3F/8eLF73vuuXPnNGrUKO3evTvf21mUrFixQs8995xWrVqlEiVKFHZzzHb58mUNGDBA3bt31zvvvKOjR49q4sSJKlWqlJo0aaIzZ85owIABGjBggNq0aaMPP/xQQ4YM0Y4dO2Rra2tSV6tWrdS4cWPjdkZGhvr06aMmTZpIksLCwpSenm7cf/jwYY0aNUo9evQwqefMmTNasmSJ3Nzc8u/GAQAAAABPPAKmPOLi4iIXFxfj91ZWVipVqlSuzs3Ozs7PphVZN27cUJ06dVS+fPnCbkqe2LVrl9zd3TV69GhJUsWKFRUVFaUtW7aoSZMmWrt2rXx8fDRs2DBJ0qRJk9S2bVslJCQYZybdZW9vL3t7e+P28uXLlZ2drbFjx0qSSSCXmZmphQsXKjg4WN7e3ib1TJ8+XV5eXrp48WJ+3DIAAAAAoIhgiVwBuXDhgkaOHCl/f3/VrVtXs2fPVlpamiSpadOmxv9GREQoOztby5YtU0BAgGrUqKFGjRppyZIlubpOUFCQPvjgA/Xv318+Pj5q3ry59u7da9z/38v1IiIijMufoqKiFBAQoI0bN6phw4aqU6eOVq5cqZ9++kktWrSQwWDQ+PHjlZWVZXJfvXr1kre3d44lW9evX9e4ceNUq1YtNWrUSCEhIUpJSTG51vTp0+Xn56cVK1bkuJesrCyFh4eradOm8vHxUVBQkH799Vfjfe7fv1/vv/++qlWrdt/XY8eOHXrppZdUq1YtTZs2zfiaS9KhQ4fUvXt31axZUwEBAVq/fr3JuREREWrZsqV8fHzUsWNH/fTTT/e8xuHDh2UwGIwz1rZv367mzZvL29tbrVq10q5du0x+PhMmTLhnPY0bN9bcuXNzlN+8eVPSneVxzZo1M5Y7ODho165dOcKl/3b16lWtXLlSY8aMyTHT6e59Xrt2TQMGDDApj4yMVHJysjp37vzA+gEAAAAAIGAqAGlpaerTp4+Sk5O1Zs0aLVq0SP/zP/+j0NBQSTI+Z2fDhg1q1aqVIiMj9fHHH2vOnDnasWOHhg4dqrCwMB09ejRX11u2bJlat26trVu3qnr16po6dapJKPQgly5d0q5du7RmzRq98cYbWrBggd5++2298847WrBggbZv326ylG/z5s1q0aKFIiMjVaFCBQ0bNkyZmZmS7iwbvHHjhtavX6+lS5cqNjZWs2bNMp577tw5paWlKSIiQm3atMnRlvfff1+rVq3SpEmTtHnzZpUrV07BwcG6ffu2wsLCZDAY1K9fP+3bt+++9/P5559r4cKFWrZsmf79739r+fLlkqT4+Hj16dNHderUUUREhIYPH653331XO3fulHQndAkJCdGgQYMUGRmpBg0aaODAgTlm8pw6dUqDBg3S8OHD1blzZyUlJWn8+PEaNGiQduzYoU6dOmn06NG6evWqpDtL0yZPnnzPtpYvX141a9Y0biclJWnbtm2qX7++pDvL1ezt7TVixAg1aNBAvXv3Vlxc3H3v/a7169erdOnSatGiRY592dnZCg8PV+/eveXk5GQsv3LliubPn69Zs2bJwsLiodcAAAAAADzdCJgKwN69e3Xx4kXNmzdP1apVU/369TVt2jStX79et27dMj7fxs3NTfb29nr22Wc1d+5c1a9fX+XLl1f37t1VqlQpnTx5MlfXe/nll9WxY0c999xzGjx4sM6fP6/Lly/n6tz09HS99dZbqlSpknr27KmsrCz17NlTNWvW1CuvvCIvLy8lJCQYj3/11VfVq1cvVa5cWTNnzlRSUpK+//57/f7779q1a5fxnn18fBQSEqLNmzfrxo0bxvODg4Pl4eGhsmXLmrQjOztba9eu1ciRI9W0aVNVrlxZISEhsrKy0pdffqkSJUrIxsZGjo6OD1yKOGnSJPn5+cnf318jR47UZ599JulO8PTCCy9o9OjRqlSpkjp06KBevXopPDxckrRmzRoFBQWpffv2qlSpksaOHavnn39ea9euNdadmJio4OBgde3aVf369ZMkXbx4Uenp6XrmmWdUrlw59evXT0uXLpWdnZ2kO0vT7i6lfJCUlBQNHz5c7u7u6tatmyTp9u3bmj9/vnFm2bPPPqu+ffvq1q1b960nOztbGzZsUK9eve65PyoqShcuXFDXrl1Nyt9++2116NBBVatWfWhbAQAAAADgGUwFID4+XhUrVjR50HetWrWUkZGh33//PUfgUK9ePR0+fFj//Oc/FR8fr2PHjuny5cu5noVUsWJF4/fOzs6S7jzkObcqVKggScZn+JQrV864z97e3mSZmY+Pj8m1PD09lZCQoMzMTGVlZemll14yqTsrK0u//fabcft+z09KSkrS1atX5evrayyzsbFRjRo1FB8fn+t7+Wv7XnjhBSUmJuratWuKj4832SdJBoPBGEDFx8dr6NChJvtr1qxpcu3FixcrIyNDzzzzjLHMy8tLTZo00euvvy5PT081bdpUXbp0kYODQ67bfOvWLQ0ZMkSnT5/WunXrjOdaWVkpICBAQUFBkqSQkBA1adJE3377rdq2bXvPumJjY3Xx4kW1bt36nvu//vprvfTSSybPZNq7d6+io6M1e/bsXLcZAAAAAPB0I2AqAHdnr/zV3WVkd//7Vxs2bNDbb7+tLl26qFmzZnrrrbfUu3fvXF/PxsYmR9n9HiR+r+tbW5t2C0vL+090s7KyMtnOysqSjY2NMjMz5eLiok2bNuU4p0yZMjp8+LCke782Dyq/G1zl1l/bfvc1sLGxuWf9WVlZxtfjfj+zv167SZMm8vf316JFi9SiRQu5ubnJwsJCy5cvV0xMjHbv3q2dO3dq3bp1Wrdunby8vB7a3ps3byo4OFi///67Pv74Y5OwsFSpUvL09DRu29raqly5cjp//vx969u7d69q1659308x3Lt3r/Gh4Xdt375dFy5cMC7Ny8jIUHp6ugwGg1auXKnatWs/9D4AAAAAAE8XlsgVAE9PT50+fdr4HB5Jio6OlrW1tZ577rkcz7hZv369hg4dqkmTJql9+/ZydXVVUlJSnnzanI2NjcmSqjNnzphV34kTJ4zfX79+XadPn1alSpXk6empGzduyMLCQh4eHvLw8FBKSopCQ0NNZkDdj4uLi9zd3RUdHW0sS09P19GjR01ClkdpX0xMjJ555hk5OjrK09PTGHLddejQIWPd99p/+PBhk2sHBASoZ8+eKlOmjObNmyfpzsynd999Vz4+PnrzzTe1bds2PfvssyYPWr+frKwsDRs2TGfPntWaNWtyLE+rWbOm8SHn0p1ne505c+aBn6IXExOjWrVq3XPflStXdObMGfn5+ZmUjx07Vtu2bVNkZKQiIyM1YsQIlS5dWpGRkapRo8ZD7wMAAAAA8PRhBlMBaNiwoSpUqKDx48drzJgx+vPPPxUSEqI2bdqoWLFixuVrx48fl6urq1xdXfXjjz+qadOmunXrlhYuXKj09PRcBTMP4+3trbVr16pSpUqKj49XRETEPT9ZLLe2bNkig8GgWrVqadGiRfLw8FC9evVkYWGhxo0ba+zYsZoyZYqsrKw0depUFS9eXMWKFctV3X379tXixYtVunRpeXh4aOXKlUpNTVWrVq1y3b6QkBDNnj1bN2/e1OLFi9W/f39JUo8ePfTJJ59owYIF6tChg6Kjo7Vu3TpNnTrVeO3JkyercuXK8vX11aZNm3T8+HG98847JvVbWVlpypQp6tOnj7p27ary5ctr/fr1cnFxUdu2bRUXF6dz587phRdekHTnE92srKzu+RymjRs3KioqSh988IGKFStmfG6WjY2NSpQooT59+qhnz57y8/NTgwYNFB4eLjs7OzVp0kSSdOPGDWVmZposdzt58qTatWt3z9fm5MmTsrOzyxFQlSxZUiVLljTZtra2loeHR65fdwAAAADA04WAqQBYWVlp6dKlCgkJUdeuXeXk5KS2bdtq9OjRku483Ltdu3YaNWqUxo4dq0mTJmnSpEkKDAxUyZIl1bJlSzk4OOjYsWNmt2Xq1KmaMmWK2rRpI29vb40YMULLli372/UFBQVp48aNCgkJkcFg0JIlS4wzskJDQzV79mz17dtX1tbWaty4saZMmZLruvv166ebN29q6tSpunnzpgwGg9asWWN8KHpudO/eXYMHD1Z6erq6du2qPn36SJLKli2r5cuXKzQ0VKtWrVLZsmU1YcIEderUSZLUqlUrJSYmavHixbp8+bK8vLy0atUqVa5cOcc16tatq2bNmmnWrFnauHGjwsLCNH/+fC1btkwlS5bU6NGj1ahRI0nS8OHDVa5cuRxBlXTneUhZWVkaNGiQSbm/v7/WrFkjX19fLVq0SPPnz9fcuXNVo0YNhYeHy9HRUZI0Z84cnTt3TmvWrDGem5iYeN9ALykpScWKFeNT4gAAAAAAZrPIzot1VwAKXVpamtmBIQAAAAAAfwfPYAKKiFWrVqlZs2aF3QwAAAAAwFOIGUxAEZGenn7PTxAEAAAAACC/ETABAAAAAADALCyRAwAAAAAAgFkImAAAAAAAAGAWAiYAAAAAAACYhYAJAAAAAAAAZiFgAgAAAAAAgFkImAAAAAAAAGAWAiYAAAAAAACYhYAJAAAAAAAAZvlf2NJ40KT93c0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# plot weighthed rating overall\n",
    "\n",
    "tot_books = y.shape[0]\n",
    "\n",
    "# Set up number of bins\n",
    "n_bins = 5\n",
    "# Set up bin edges\n",
    "bin_edges = np.linspace(0, 5, 6)  # Ensure bins start from 0 to 5 with 5 bins\n",
    "\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(\n",
    "    figsize = (14, 8))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Draw histogram using Seaborn\n",
    "ax = sns.histplot(y['weighted rating'], bins=n_bins, kde=False)\n",
    "\n",
    "# Compute the counts for each bin\n",
    "counts, _ = np.histogram(y['weighted rating'], bins=n_bins)\n",
    "counts = counts / len(y)  # Convert counts to percentages\n",
    "\n",
    "# Add text on each column showing the % of observations in each bin\n",
    "for rect, count in zip(ax.patches, counts):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 1, f'{count:.2%}', ha='center', va='bottom')\n",
    "\n",
    "# Add title and footnotes\n",
    "plt.title(\"Distribution of book titles based on weighted rating\", fontsize=17)\n",
    "plt.xlabel(\"Weighted rating\")\n",
    "plt.ylabel(\"Number of titles\")\n",
    "\n",
    "# Add footnote\n",
    "plt.text(0, -0.2, f\"Total number of books: {tot_books:,}\", fontsize=10, transform=ax.transAxes)\n",
    "\n",
    "# Get the current axis\n",
    "ax = plt.gca()\n",
    "\n",
    "# Apply thousand separator to y-axis ticks\n",
    "ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "\n",
    "\n",
    "plt.savefig(\n",
    "    os.path.join(\n",
    "        output_folder,\n",
    "        'Charts',\n",
    "        '03 Weighted ratings dis.png'\n",
    "    )\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted ratings train-test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Calculate percentage of observations in each bin for train and test sets\n",
    "train_percentages = [count / len(y_wr_train) * 100 for count in np.histogram(y_wr_train, bins=n_bins)[0]]\n",
    "test_percentages = [count / len(y_wr_test) * 100 for count in np.histogram(y_wr_test, bins=n_bins)[0]]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(\n",
    "    1, \n",
    "    2, \n",
    "    figsize=(15, 6))\n",
    "\n",
    "# Histogram of weighted ratings in train set\n",
    "sns.histplot(\n",
    "    x=y_wr_train, \n",
    "    ax=axes[0],\n",
    "    bins=n_bins, kde=False)\n",
    "axes[0].set_title('Weighted ratings in train set')\n",
    "axes[0].set_ylabel('Weighted rating')\n",
    "axes[0].yaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))  # Add thousands separator\n",
    "\n",
    "# Annotate bars with percentage of observations for train set\n",
    "for i, p in enumerate(axes[0].patches):\n",
    "    axes[0].annotate(f'{train_percentages[i]:.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                     ha='center', va='center', fontsize=8, color='black', xytext=(0, 5),\n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Histogram of weighted ratings in test set\n",
    "sns.histplot(\n",
    "    x=y_wr_test, \n",
    "    ax=axes[1],\n",
    "    bins=n_bins, kde=False)\n",
    "axes[1].set_title('Weighted ratings in test set')\n",
    "axes[1].set_ylabel('Weighted rating')\n",
    "axes[1].yaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))  # Add thousands separator\n",
    "\n",
    "# Annotate bars with percentage of observations for test set\n",
    "for i, p in enumerate(axes[1].patches):\n",
    "    axes[1].annotate(f'{test_percentages[i]:.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                     ha='center', va='center', fontsize=8, color='black', xytext=(0, 5),\n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Set common title for the figure\n",
    "plt.suptitle(\"Weighted ratings distribution\", fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(\n",
    "        output_folder,\n",
    "        'Charts',\n",
    "        '03 Weighted rating dis train test.png'\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
