{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folders\n",
    "from EDA_functions import folders_set_up\n",
    "import os\n",
    "\n",
    "# Work with datarames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charts\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# X, Y preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# Neural Network\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers import Dense,Dropout\n",
    "# from keras.optimizers import Adam, SGD\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Evaluate models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Time\n",
    "import time\n",
    "\n",
    "#from scipy.sparse import spmatrixc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers import Dense,Dropout\n",
    "# from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM\n",
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders\n",
    "Run the code below if you have the following structure:\n",
    "- Group-project: GitHub folder\n",
    "- 01 Input\n",
    "- 02 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_folder, input_folder, output_folder = folders_set_up.generate_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'description', 'authors', 'image', 'previewLink',\n",
       "       'publisher', 'infoLink', 'categories', 'reviews number',\n",
       "       'average rating', 'median rating', 'min review date', 'max review date',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Title-level dataset with embeddings\n",
    "title_embeddings_df = pd.read_pickle(\n",
    "    os.path.join(output_folder, 'English_fiction_pre_PCA_3_with_av_pool_embeddings')\n",
    ")\n",
    "\n",
    "title_embeddings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                     int64\n",
       "Title                    object\n",
       "description              object\n",
       "authors                  object\n",
       "image                    object\n",
       "previewLink              object\n",
       "publisher                object\n",
       "infoLink                 object\n",
       "categories               object\n",
       "reviews number            int64\n",
       "average rating          float64\n",
       "median rating           float64\n",
       "min review date          object\n",
       "max review date          object\n",
       "weighted rating         float64\n",
       "date                     object\n",
       "year                    float64\n",
       "description_language     object\n",
       "Embedding                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_columns = ['min review date', 'max review date', 'date']\n",
    "\n",
    "for date in dates_columns:\n",
    "    # get date from strings with time\n",
    "    title_embeddings_df[date] = title_embeddings_df[date].str.split().str[0]\n",
    "    # convert in datetime\n",
    "    title_embeddings_df[date] = pd.to_datetime(title_embeddings_df[date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min review date    0\n",
       "max review date    0\n",
       "date               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df[dates_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8228249664151188\n",
      "4.886083503427672\n"
     ]
    }
   ],
   "source": [
    "# what is the max and minimu of the ratings?\n",
    "print(title_embeddings_df['weighted rating'].min())\n",
    "print(title_embeddings_df['weighted rating'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we work on a subset of data for now to make the ML run faster\n",
    "#title_embeddings_df = title_embeddings_df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image embeddings\n",
    "These need may need to be transformed in from arrays to columns if the model we use is not NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>median rating</th>\n",
       "      <th>min review date</th>\n",
       "      <th>max review date</th>\n",
       "      <th>weighted rating</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>description_language</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>index_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-02-14</td>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>3.938400</td>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.5179044, -0.7533603, -1.1291503, -0.4418345...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>4.306145</td>\n",
       "      <td>2001-03-06</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.706188, -0.4773652, -0.17887038, 0.07989502...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-22</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>4.256189</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.294651, -0.24902871, -0.6188333, -0.7722471...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-04-16</td>\n",
       "      <td>2000-05-14</td>\n",
       "      <td>4.336313</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.37794992, -0.6178984, -0.81393754, -0.66795...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-08-07</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>4.701517</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.34032565, -2.1706967, -0.21470371, -0.10447...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26769</th>\n",
       "      <td>212361</td>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>4.137453</td>\n",
       "      <td>2009-03-17</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[1.1648176, 0.56768346, -0.22511423, -0.185316...</td>\n",
       "      <td>212361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26770</th>\n",
       "      <td>212365</td>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-05-17</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>4.466716</td>\n",
       "      <td>1998-01-27</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.023786038, -1.9050528, -0.38564998, 0.14921...</td>\n",
       "      <td>212365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26771</th>\n",
       "      <td>212394</td>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>4.260690</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.2700834, -0.11750376, -2.0253444, -1.039558...</td>\n",
       "      <td>212394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26772</th>\n",
       "      <td>212399</td>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-07-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>4.504800</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.6904726, -0.96442795, 0.093034565, -1.69420...</td>\n",
       "      <td>212399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773</th>\n",
       "      <td>212402</td>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2002-11-11</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>3.989408</td>\n",
       "      <td>2003-08-12</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.47349918, -0.8046489, -0.88566315, -0.04958...</td>\n",
       "      <td>212402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26774 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                              Title  \\\n",
       "0           3                      Whispers of the Wicked Saints   \n",
       "1          24           The Forbidden Stories of Marta Veneranda   \n",
       "2          42                            Tess and the Highlander   \n",
       "3          49  Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "4          73                 Night World: Daughters Of Darkness   \n",
       "...       ...                                                ...   \n",
       "26769  212361                                       Calder Pride   \n",
       "26770  212365                                      The Road Back   \n",
       "26771  212394                                       Final things   \n",
       "26772  212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "26773  212402                                  The Autograph Man   \n",
       "\n",
       "                                             description  \\\n",
       "0      Julia Thomas finds her life spinning out of co...   \n",
       "1      Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "2      In 1543, on a windswept isle off of Scotland, ...   \n",
       "3      Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "4      \"There’s something strange about the new girls...   \n",
       "...                                                  ...   \n",
       "26769  The Long-Awaited Addition to the Beloved Calde...   \n",
       "26770  The sequel to the masterpiece All Quiet on the...   \n",
       "26771  Grace's father believes in science and builds ...   \n",
       "26772  During a school trip to Ellis Island, Dominick...   \n",
       "26773  Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                        authors  \\\n",
       "0           ['Veronica Haddon']   \n",
       "1       ['Sonia Rivera-Valdes']   \n",
       "2            ['May Mcgoldrick']   \n",
       "3        ['Elizabeth Sinclair']   \n",
       "4                ['L.J. Smith']   \n",
       "...                         ...   \n",
       "26769          ['Janet Dailey']   \n",
       "26770  ['Erich Maria Remarque']   \n",
       "26771          ['Jenny Offill']   \n",
       "26772       ['Elvira Woodruff']   \n",
       "26773           ['Zadie Smith']   \n",
       "\n",
       "                                                   image  \\\n",
       "0      http://books.google.com/books/content?id=aRSIg...   \n",
       "1      http://books.google.com/books/content?id=A7aYb...   \n",
       "2      http://books.google.com/books/content?id=VmCRS...   \n",
       "3      http://books.google.com/books/content?id=Z6uzJ...   \n",
       "4      http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                  ...   \n",
       "26769  http://books.google.com/books/content?id=nlsgd...   \n",
       "26770  http://books.google.com/books/content?id=obZdA...   \n",
       "26771  http://books.google.com/books/content?id=UbSFB...   \n",
       "26772  http://books.google.com/books/content?id=J7M-N...   \n",
       "26773  http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                             previewLink  \\\n",
       "0      http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "1      http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "2      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "3      http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "4      http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                  ...   \n",
       "26769  http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "26770  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "26771  http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "26772  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "26773  http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                               publisher  \\\n",
       "0                                              iUniverse   \n",
       "1                                    Seven Stories Press   \n",
       "2                                         Harper Collins   \n",
       "3      Harlequin Treasury-Harlequin American Romance 90s   \n",
       "4                                     Simon and Schuster   \n",
       "...                                                  ...   \n",
       "26769                                     Harper Collins   \n",
       "26770                      Random House Trade Paperbacks   \n",
       "26771                                            Vintage   \n",
       "26772                              Scholastic Paperbacks   \n",
       "26773                                            Vintage   \n",
       "\n",
       "                                                infoLink  \\\n",
       "0      http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "1      http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "2      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "3      http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "4      http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                  ...   \n",
       "26769  https://play.google.com/store/books/details?id...   \n",
       "26770  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "26771  https://play.google.com/store/books/details?id...   \n",
       "26772  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "26773  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                 categories  reviews number  average rating  median rating  \\\n",
       "0               ['fiction']              32        3.718750            5.0   \n",
       "1               ['fiction']               1        5.000000            5.0   \n",
       "2      ['juvenile fiction']              17        4.235294            5.0   \n",
       "3               ['fiction']               2        5.000000            5.0   \n",
       "4      ['juvenile fiction']             134        4.768657            5.0   \n",
       "...                     ...             ...             ...            ...   \n",
       "26769           ['fiction']              28        4.035714            5.0   \n",
       "26770           ['fiction']              17        4.705882            5.0   \n",
       "26771           ['fiction']               1        4.000000            4.0   \n",
       "26772  ['juvenile fiction']              28        4.678571            5.0   \n",
       "26773           ['fiction']               4        2.500000            2.5   \n",
       "\n",
       "      min review date max review date  weighted rating       date    year  \\\n",
       "0          2005-02-14      2006-07-01         3.938400 2005-02-01  2005.0   \n",
       "1          2005-01-24      2005-01-24         4.306145 2001-03-06  2001.0   \n",
       "2          2002-10-22      2011-05-25         4.256189 2002-11-01  2002.0   \n",
       "3          1998-04-16      2000-05-14         4.336313 1997-01-01  1997.0   \n",
       "4          1996-08-07      2012-09-18         4.701517 2016-12-06  2016.0   \n",
       "...               ...             ...              ...        ...     ...   \n",
       "26769      1999-09-30      2012-04-04         4.137453 2009-03-17  2009.0   \n",
       "26770      1997-05-17      2012-01-23         4.466716 1998-01-27  1998.0   \n",
       "26771      2012-01-26      2012-01-26         4.260690 2015-03-17  2015.0   \n",
       "26772      1998-07-10      2011-12-31         4.504800 2000-06-01  2000.0   \n",
       "26773      2002-11-11      2005-11-14         3.989408 2003-08-12  2003.0   \n",
       "\n",
       "      description_language                                          Embedding  \\\n",
       "0                  English  [0.5179044, -0.7533603, -1.1291503, -0.4418345...   \n",
       "1                  English  [0.706188, -0.4773652, -0.17887038, 0.07989502...   \n",
       "2                  English  [2.294651, -0.24902871, -0.6188333, -0.7722471...   \n",
       "3                  English  [0.37794992, -0.6178984, -0.81393754, -0.66795...   \n",
       "4                  English  [0.34032565, -2.1706967, -0.21470371, -0.10447...   \n",
       "...                    ...                                                ...   \n",
       "26769              English  [1.1648176, 0.56768346, -0.22511423, -0.185316...   \n",
       "26770              English  [0.023786038, -1.9050528, -0.38564998, 0.14921...   \n",
       "26771              English  [2.2700834, -0.11750376, -2.0253444, -1.039558...   \n",
       "26772              English  [2.6904726, -0.96442795, 0.093034565, -1.69420...   \n",
       "26773              English  [0.47349918, -0.8046489, -0.88566315, -0.04958...   \n",
       "\n",
       "       index_key  \n",
       "0              3  \n",
       "1             24  \n",
       "2             42  \n",
       "3             49  \n",
       "4             73  \n",
       "...          ...  \n",
       "26769     212361  \n",
       "26770     212365  \n",
       "26771     212394  \n",
       "26772     212399  \n",
       "26773     212402  \n",
       "\n",
       "[26774 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df['index_key'] = title_embeddings_df['index']\n",
    "title_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings_df = title_embeddings_df.set_index('index_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>median rating</th>\n",
       "      <th>min review date</th>\n",
       "      <th>max review date</th>\n",
       "      <th>weighted rating</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>description_language</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-02-14</td>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>3.938400</td>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.5179044, -0.7533603, -1.1291503, -0.4418345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>4.306145</td>\n",
       "      <td>2001-03-06</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.706188, -0.4773652, -0.17887038, 0.07989502...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-22</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>4.256189</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.294651, -0.24902871, -0.6188333, -0.7722471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-04-16</td>\n",
       "      <td>2000-05-14</td>\n",
       "      <td>4.336313</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.37794992, -0.6178984, -0.81393754, -0.66795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-08-07</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>4.701517</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.34032565, -2.1706967, -0.21470371, -0.10447...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>212361</td>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>4.137453</td>\n",
       "      <td>2009-03-17</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[1.1648176, 0.56768346, -0.22511423, -0.185316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>212365</td>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-05-17</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>4.466716</td>\n",
       "      <td>1998-01-27</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.023786038, -1.9050528, -0.38564998, 0.14921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>212394</td>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>4.260690</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.2700834, -0.11750376, -2.0253444, -1.039558...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>212399</td>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-07-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>4.504800</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.6904726, -0.96442795, 0.093034565, -1.69420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>212402</td>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2002-11-11</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>3.989408</td>\n",
       "      <td>2003-08-12</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.47349918, -0.8046489, -0.88566315, -0.04958...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26774 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index                                              Title  \\\n",
       "index_key                                                              \n",
       "3               3                      Whispers of the Wicked Saints   \n",
       "24             24           The Forbidden Stories of Marta Veneranda   \n",
       "42             42                            Tess and the Highlander   \n",
       "49             49  Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "73             73                 Night World: Daughters Of Darkness   \n",
       "...           ...                                                ...   \n",
       "212361     212361                                       Calder Pride   \n",
       "212365     212365                                      The Road Back   \n",
       "212394     212394                                       Final things   \n",
       "212399     212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "212402     212402                                  The Autograph Man   \n",
       "\n",
       "                                                 description  \\\n",
       "index_key                                                      \n",
       "3          Julia Thomas finds her life spinning out of co...   \n",
       "24         Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "42         In 1543, on a windswept isle off of Scotland, ...   \n",
       "49         Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "73         \"There’s something strange about the new girls...   \n",
       "...                                                      ...   \n",
       "212361     The Long-Awaited Addition to the Beloved Calde...   \n",
       "212365     The sequel to the masterpiece All Quiet on the...   \n",
       "212394     Grace's father believes in science and builds ...   \n",
       "212399     During a school trip to Ellis Island, Dominick...   \n",
       "212402     Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                            authors  \\\n",
       "index_key                             \n",
       "3               ['Veronica Haddon']   \n",
       "24          ['Sonia Rivera-Valdes']   \n",
       "42               ['May Mcgoldrick']   \n",
       "49           ['Elizabeth Sinclair']   \n",
       "73                   ['L.J. Smith']   \n",
       "...                             ...   \n",
       "212361             ['Janet Dailey']   \n",
       "212365     ['Erich Maria Remarque']   \n",
       "212394             ['Jenny Offill']   \n",
       "212399          ['Elvira Woodruff']   \n",
       "212402              ['Zadie Smith']   \n",
       "\n",
       "                                                       image  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.com/books/content?id=aRSIg...   \n",
       "24         http://books.google.com/books/content?id=A7aYb...   \n",
       "42         http://books.google.com/books/content?id=VmCRS...   \n",
       "49         http://books.google.com/books/content?id=Z6uzJ...   \n",
       "73         http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                      ...   \n",
       "212361     http://books.google.com/books/content?id=nlsgd...   \n",
       "212365     http://books.google.com/books/content?id=obZdA...   \n",
       "212394     http://books.google.com/books/content?id=UbSFB...   \n",
       "212399     http://books.google.com/books/content?id=J7M-N...   \n",
       "212402     http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                                 previewLink  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24         http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "42         http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49         http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "73         http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                      ...   \n",
       "212361     http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "212365     http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394     http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "212399     http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402     http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                                   publisher  \\\n",
       "index_key                                                      \n",
       "3                                                  iUniverse   \n",
       "24                                       Seven Stories Press   \n",
       "42                                            Harper Collins   \n",
       "49         Harlequin Treasury-Harlequin American Romance 90s   \n",
       "73                                        Simon and Schuster   \n",
       "...                                                      ...   \n",
       "212361                                        Harper Collins   \n",
       "212365                         Random House Trade Paperbacks   \n",
       "212394                                               Vintage   \n",
       "212399                                 Scholastic Paperbacks   \n",
       "212402                                               Vintage   \n",
       "\n",
       "                                                    infoLink  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24         http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "42         http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49         http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "73         http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                      ...   \n",
       "212361     https://play.google.com/store/books/details?id...   \n",
       "212365     http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394     https://play.google.com/store/books/details?id...   \n",
       "212399     http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402     https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                     categories  reviews number  average rating  \\\n",
       "index_key                                                         \n",
       "3                   ['fiction']              32        3.718750   \n",
       "24                  ['fiction']               1        5.000000   \n",
       "42         ['juvenile fiction']              17        4.235294   \n",
       "49                  ['fiction']               2        5.000000   \n",
       "73         ['juvenile fiction']             134        4.768657   \n",
       "...                         ...             ...             ...   \n",
       "212361              ['fiction']              28        4.035714   \n",
       "212365              ['fiction']              17        4.705882   \n",
       "212394              ['fiction']               1        4.000000   \n",
       "212399     ['juvenile fiction']              28        4.678571   \n",
       "212402              ['fiction']               4        2.500000   \n",
       "\n",
       "           median rating min review date max review date  weighted rating  \\\n",
       "index_key                                                                   \n",
       "3                    5.0      2005-02-14      2006-07-01         3.938400   \n",
       "24                   5.0      2005-01-24      2005-01-24         4.306145   \n",
       "42                   5.0      2002-10-22      2011-05-25         4.256189   \n",
       "49                   5.0      1998-04-16      2000-05-14         4.336313   \n",
       "73                   5.0      1996-08-07      2012-09-18         4.701517   \n",
       "...                  ...             ...             ...              ...   \n",
       "212361               5.0      1999-09-30      2012-04-04         4.137453   \n",
       "212365               5.0      1997-05-17      2012-01-23         4.466716   \n",
       "212394               4.0      2012-01-26      2012-01-26         4.260690   \n",
       "212399               5.0      1998-07-10      2011-12-31         4.504800   \n",
       "212402               2.5      2002-11-11      2005-11-14         3.989408   \n",
       "\n",
       "                date    year description_language  \\\n",
       "index_key                                           \n",
       "3         2005-02-01  2005.0              English   \n",
       "24        2001-03-06  2001.0              English   \n",
       "42        2002-11-01  2002.0              English   \n",
       "49        1997-01-01  1997.0              English   \n",
       "73        2016-12-06  2016.0              English   \n",
       "...              ...     ...                  ...   \n",
       "212361    2009-03-17  2009.0              English   \n",
       "212365    1998-01-27  1998.0              English   \n",
       "212394    2015-03-17  2015.0              English   \n",
       "212399    2000-06-01  2000.0              English   \n",
       "212402    2003-08-12  2003.0              English   \n",
       "\n",
       "                                                   Embedding  \n",
       "index_key                                                     \n",
       "3          [0.5179044, -0.7533603, -1.1291503, -0.4418345...  \n",
       "24         [0.706188, -0.4773652, -0.17887038, 0.07989502...  \n",
       "42         [2.294651, -0.24902871, -0.6188333, -0.7722471...  \n",
       "49         [0.37794992, -0.6178984, -0.81393754, -0.66795...  \n",
       "73         [0.34032565, -2.1706967, -0.21470371, -0.10447...  \n",
       "...                                                      ...  \n",
       "212361     [1.1648176, 0.56768346, -0.22511423, -0.185316...  \n",
       "212365     [0.023786038, -1.9050528, -0.38564998, 0.14921...  \n",
       "212394     [2.2700834, -0.11750376, -2.0253444, -1.039558...  \n",
       "212399     [2.6904726, -0.96442795, 0.093034565, -1.69420...  \n",
       "212402     [0.47349918, -0.8046489, -0.88566315, -0.04958...  \n",
       "\n",
       "[26774 rows x 19 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "Most of the cleaning is done in '02 Consolidate books dataset':\n",
    "- English description\n",
    "- category containing the word 'fiction'\n",
    "- non-missing date\n",
    "- non-missing author\n",
    "- non-missing publisher\n",
    "- non-missing cover image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'description', 'authors', 'image', 'previewLink',\n",
       "       'publisher', 'infoLink', 'categories', 'reviews number',\n",
       "       'average rating', 'median rating', 'min review date', 'max review date',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y including all X features and all all teh possible target variables\n",
    "# NOTE: we will have to add the description PCA in X_features\n",
    "X_columns = ['year', 'Embedding', 'index', 'Title']\n",
    "\n",
    "X = title_embeddings_df[X_columns]\n",
    "y = title_embeddings_df[['average rating', 'weighted rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'Embedding', 'index', 'Title'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['average rating', 'weighted rating'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "\n",
    "# Need to create train test split for different combinations of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size= 0.2, \n",
    "    random_state= 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store indices of train test split for the NLP of description\n",
    "train_indices = X_train[['Title', 'index']]\n",
    "test_indices = X_test[['Title', 'index']]\n",
    "\n",
    "train_indices.to_csv(\n",
    "    os.path.join(output_folder, 'train_indices.csv')\n",
    ")\n",
    "\n",
    "\n",
    "test_indices.to_csv(\n",
    "    os.path.join(output_folder, 'test_indices.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y train with average rating\n",
    "y_avg_r_train = y_train['average rating']\n",
    "y_avg_r_test = y_test['average rating']\n",
    "\n",
    "# Y train with weighted rating\n",
    "y_wr_train = y_train['weighted rating']\n",
    "y_wr_test = y_test['weighted rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings X\n",
    "Transform the arrays into columns so that they can feed into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images_train = X_train['Embedding'].apply(pd.Series)\n",
    "X_images_test = X_test['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "X_images_train = X_images_train.add_prefix('image_')\n",
    "X_images_test = X_images_test.add_prefix('image_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_0</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "      <th>image_6</th>\n",
       "      <th>image_7</th>\n",
       "      <th>image_8</th>\n",
       "      <th>image_9</th>\n",
       "      <th>...</th>\n",
       "      <th>image_246</th>\n",
       "      <th>image_247</th>\n",
       "      <th>image_248</th>\n",
       "      <th>image_249</th>\n",
       "      <th>image_250</th>\n",
       "      <th>image_251</th>\n",
       "      <th>image_252</th>\n",
       "      <th>image_253</th>\n",
       "      <th>image_254</th>\n",
       "      <th>image_255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19757</th>\n",
       "      <td>1.310540</td>\n",
       "      <td>0.253238</td>\n",
       "      <td>-0.208362</td>\n",
       "      <td>-0.584103</td>\n",
       "      <td>-0.794551</td>\n",
       "      <td>-1.803357</td>\n",
       "      <td>-2.700018</td>\n",
       "      <td>-0.848385</td>\n",
       "      <td>0.949902</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475167</td>\n",
       "      <td>-0.116055</td>\n",
       "      <td>0.735540</td>\n",
       "      <td>-2.354414</td>\n",
       "      <td>0.956939</td>\n",
       "      <td>-1.065875</td>\n",
       "      <td>-0.428229</td>\n",
       "      <td>-0.285047</td>\n",
       "      <td>1.098027</td>\n",
       "      <td>-1.029737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111405</th>\n",
       "      <td>1.078856</td>\n",
       "      <td>-0.691140</td>\n",
       "      <td>-0.908770</td>\n",
       "      <td>-0.527087</td>\n",
       "      <td>-1.044688</td>\n",
       "      <td>-0.904328</td>\n",
       "      <td>0.210946</td>\n",
       "      <td>-1.238919</td>\n",
       "      <td>2.290273</td>\n",
       "      <td>-0.155667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833435</td>\n",
       "      <td>-1.053398</td>\n",
       "      <td>-2.031961</td>\n",
       "      <td>-2.716383</td>\n",
       "      <td>0.817275</td>\n",
       "      <td>-0.434370</td>\n",
       "      <td>-1.456125</td>\n",
       "      <td>0.112614</td>\n",
       "      <td>0.199815</td>\n",
       "      <td>-2.946273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12269</th>\n",
       "      <td>0.350093</td>\n",
       "      <td>-0.135313</td>\n",
       "      <td>0.512653</td>\n",
       "      <td>0.466054</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>0.014968</td>\n",
       "      <td>-2.224684</td>\n",
       "      <td>-0.740723</td>\n",
       "      <td>0.951188</td>\n",
       "      <td>0.978717</td>\n",
       "      <td>...</td>\n",
       "      <td>1.401945</td>\n",
       "      <td>-0.270900</td>\n",
       "      <td>-1.967142</td>\n",
       "      <td>-0.814089</td>\n",
       "      <td>0.170715</td>\n",
       "      <td>0.335253</td>\n",
       "      <td>-0.030882</td>\n",
       "      <td>-0.557203</td>\n",
       "      <td>0.037506</td>\n",
       "      <td>-1.951925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186303</th>\n",
       "      <td>4.560193</td>\n",
       "      <td>0.550376</td>\n",
       "      <td>-0.331547</td>\n",
       "      <td>-2.215812</td>\n",
       "      <td>0.023501</td>\n",
       "      <td>-0.370375</td>\n",
       "      <td>-0.838666</td>\n",
       "      <td>-0.905349</td>\n",
       "      <td>2.655245</td>\n",
       "      <td>0.461321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073267</td>\n",
       "      <td>0.354717</td>\n",
       "      <td>-1.745163</td>\n",
       "      <td>-2.610591</td>\n",
       "      <td>-0.239567</td>\n",
       "      <td>-1.714204</td>\n",
       "      <td>-0.914772</td>\n",
       "      <td>-0.354720</td>\n",
       "      <td>0.098914</td>\n",
       "      <td>-3.272672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134045</th>\n",
       "      <td>2.236698</td>\n",
       "      <td>-0.498289</td>\n",
       "      <td>-1.946595</td>\n",
       "      <td>-0.618339</td>\n",
       "      <td>-2.150359</td>\n",
       "      <td>1.257699</td>\n",
       "      <td>-1.547900</td>\n",
       "      <td>-1.492419</td>\n",
       "      <td>2.133226</td>\n",
       "      <td>1.513557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192120</td>\n",
       "      <td>-0.731990</td>\n",
       "      <td>-3.023546</td>\n",
       "      <td>-2.410438</td>\n",
       "      <td>-0.701821</td>\n",
       "      <td>-0.403381</td>\n",
       "      <td>0.522945</td>\n",
       "      <td>0.256027</td>\n",
       "      <td>1.826598</td>\n",
       "      <td>-3.091393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167206</th>\n",
       "      <td>0.998582</td>\n",
       "      <td>-1.018484</td>\n",
       "      <td>-0.603367</td>\n",
       "      <td>-0.467136</td>\n",
       "      <td>0.090738</td>\n",
       "      <td>-1.786356</td>\n",
       "      <td>-1.359131</td>\n",
       "      <td>-0.057187</td>\n",
       "      <td>-0.699324</td>\n",
       "      <td>-0.277699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967072</td>\n",
       "      <td>-0.407894</td>\n",
       "      <td>0.675619</td>\n",
       "      <td>-2.690143</td>\n",
       "      <td>0.811195</td>\n",
       "      <td>0.162421</td>\n",
       "      <td>-0.319673</td>\n",
       "      <td>-0.655105</td>\n",
       "      <td>1.837679</td>\n",
       "      <td>-2.292945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40262</th>\n",
       "      <td>0.993674</td>\n",
       "      <td>-0.236895</td>\n",
       "      <td>-1.161556</td>\n",
       "      <td>-0.831213</td>\n",
       "      <td>-0.535987</td>\n",
       "      <td>1.596271</td>\n",
       "      <td>-1.988977</td>\n",
       "      <td>-0.020737</td>\n",
       "      <td>0.298276</td>\n",
       "      <td>0.706597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235213</td>\n",
       "      <td>-0.146208</td>\n",
       "      <td>-1.484007</td>\n",
       "      <td>-1.698481</td>\n",
       "      <td>-0.935188</td>\n",
       "      <td>-0.480799</td>\n",
       "      <td>0.807755</td>\n",
       "      <td>0.346888</td>\n",
       "      <td>1.523457</td>\n",
       "      <td>-1.739759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>0.622462</td>\n",
       "      <td>-0.793748</td>\n",
       "      <td>-0.837395</td>\n",
       "      <td>-0.745482</td>\n",
       "      <td>-0.137351</td>\n",
       "      <td>0.551706</td>\n",
       "      <td>0.313828</td>\n",
       "      <td>-0.853430</td>\n",
       "      <td>0.280148</td>\n",
       "      <td>-0.483826</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114822</td>\n",
       "      <td>-0.845614</td>\n",
       "      <td>-1.312257</td>\n",
       "      <td>-0.366303</td>\n",
       "      <td>-0.078197</td>\n",
       "      <td>1.005683</td>\n",
       "      <td>0.627290</td>\n",
       "      <td>-1.296975</td>\n",
       "      <td>1.453560</td>\n",
       "      <td>-0.719741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124694</th>\n",
       "      <td>1.551901</td>\n",
       "      <td>0.323544</td>\n",
       "      <td>-1.019795</td>\n",
       "      <td>-0.265791</td>\n",
       "      <td>-0.342785</td>\n",
       "      <td>0.346762</td>\n",
       "      <td>0.363122</td>\n",
       "      <td>-0.480910</td>\n",
       "      <td>0.268287</td>\n",
       "      <td>-0.495149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396447</td>\n",
       "      <td>0.472859</td>\n",
       "      <td>-1.687484</td>\n",
       "      <td>-2.504541</td>\n",
       "      <td>-0.655285</td>\n",
       "      <td>0.751038</td>\n",
       "      <td>0.302664</td>\n",
       "      <td>-0.154664</td>\n",
       "      <td>1.174814</td>\n",
       "      <td>-2.710322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185394</th>\n",
       "      <td>1.494337</td>\n",
       "      <td>-0.428976</td>\n",
       "      <td>-0.336699</td>\n",
       "      <td>-0.481797</td>\n",
       "      <td>-1.894806</td>\n",
       "      <td>0.845233</td>\n",
       "      <td>-1.619887</td>\n",
       "      <td>-0.521737</td>\n",
       "      <td>-0.534672</td>\n",
       "      <td>-2.321126</td>\n",
       "      <td>...</td>\n",
       "      <td>1.549637</td>\n",
       "      <td>-0.056938</td>\n",
       "      <td>-0.942204</td>\n",
       "      <td>-1.869788</td>\n",
       "      <td>0.764314</td>\n",
       "      <td>-1.347982</td>\n",
       "      <td>-0.955746</td>\n",
       "      <td>-0.672004</td>\n",
       "      <td>1.069264</td>\n",
       "      <td>-1.114935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_0   image_1   image_2   image_3   image_4   image_5  \\\n",
       "index_key                                                               \n",
       "19757      1.310540  0.253238 -0.208362 -0.584103 -0.794551 -1.803357   \n",
       "111405     1.078856 -0.691140 -0.908770 -0.527087 -1.044688 -0.904328   \n",
       "12269      0.350093 -0.135313  0.512653  0.466054  0.326599  0.014968   \n",
       "186303     4.560193  0.550376 -0.331547 -2.215812  0.023501 -0.370375   \n",
       "134045     2.236698 -0.498289 -1.946595 -0.618339 -2.150359  1.257699   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "167206     0.998582 -1.018484 -0.603367 -0.467136  0.090738 -1.786356   \n",
       "40262      0.993674 -0.236895 -1.161556 -0.831213 -0.535987  1.596271   \n",
       "7002       0.622462 -0.793748 -0.837395 -0.745482 -0.137351  0.551706   \n",
       "124694     1.551901  0.323544 -1.019795 -0.265791 -0.342785  0.346762   \n",
       "185394     1.494337 -0.428976 -0.336699 -0.481797 -1.894806  0.845233   \n",
       "\n",
       "            image_6   image_7   image_8   image_9  ...  image_246  image_247  \\\n",
       "index_key                                          ...                         \n",
       "19757     -2.700018 -0.848385  0.949902  0.030431  ...  -0.475167  -0.116055   \n",
       "111405     0.210946 -1.238919  2.290273 -0.155667  ...   0.833435  -1.053398   \n",
       "12269     -2.224684 -0.740723  0.951188  0.978717  ...   1.401945  -0.270900   \n",
       "186303    -0.838666 -0.905349  2.655245  0.461321  ...   1.073267   0.354717   \n",
       "134045    -1.547900 -1.492419  2.133226  1.513557  ...   0.192120  -0.731990   \n",
       "...             ...       ...       ...       ...  ...        ...        ...   \n",
       "167206    -1.359131 -0.057187 -0.699324 -0.277699  ...   0.967072  -0.407894   \n",
       "40262     -1.988977 -0.020737  0.298276  0.706597  ...   1.235213  -0.146208   \n",
       "7002       0.313828 -0.853430  0.280148 -0.483826  ...   1.114822  -0.845614   \n",
       "124694     0.363122 -0.480910  0.268287 -0.495149  ...   1.396447   0.472859   \n",
       "185394    -1.619887 -0.521737 -0.534672 -2.321126  ...   1.549637  -0.056938   \n",
       "\n",
       "           image_248  image_249  image_250  image_251  image_252  image_253  \\\n",
       "index_key                                                                     \n",
       "19757       0.735540  -2.354414   0.956939  -1.065875  -0.428229  -0.285047   \n",
       "111405     -2.031961  -2.716383   0.817275  -0.434370  -1.456125   0.112614   \n",
       "12269      -1.967142  -0.814089   0.170715   0.335253  -0.030882  -0.557203   \n",
       "186303     -1.745163  -2.610591  -0.239567  -1.714204  -0.914772  -0.354720   \n",
       "134045     -3.023546  -2.410438  -0.701821  -0.403381   0.522945   0.256027   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "167206      0.675619  -2.690143   0.811195   0.162421  -0.319673  -0.655105   \n",
       "40262      -1.484007  -1.698481  -0.935188  -0.480799   0.807755   0.346888   \n",
       "7002       -1.312257  -0.366303  -0.078197   1.005683   0.627290  -1.296975   \n",
       "124694     -1.687484  -2.504541  -0.655285   0.751038   0.302664  -0.154664   \n",
       "185394     -0.942204  -1.869788   0.764314  -1.347982  -0.955746  -0.672004   \n",
       "\n",
       "           image_254  image_255  \n",
       "index_key                        \n",
       "19757       1.098027  -1.029737  \n",
       "111405      0.199815  -2.946273  \n",
       "12269       0.037506  -1.951925  \n",
       "186303      0.098914  -3.272672  \n",
       "134045      1.826598  -3.091393  \n",
       "...              ...        ...  \n",
       "167206      1.837679  -2.292945  \n",
       "40262       1.523457  -1.739759  \n",
       "7002        1.453560  -0.719741  \n",
       "124694      1.174814  -2.710322  \n",
       "185394      1.069264  -1.114935  \n",
       "\n",
       "[21419 rows x 256 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description from NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description NLP test\n",
    "NMF_df_test = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_test_NMF_topics.csv')\n",
    ")\n",
    "\n",
    "# Set indices as in train test split\n",
    "NMF_df_test = NMF_df_test.set_index('index')\n",
    "\n",
    "NMF_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description NLP train\n",
    "NMF_df_train = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_train_NMF_topics.csv')\n",
    ")\n",
    "\n",
    "NMF_df_train = NMF_df_train.set_index('index')\n",
    "\n",
    "NMF_df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to keep\n",
    "# columns_to_keep\n",
    "\n",
    "NMF_df_train = NMF_df_train[columns_to_keep]\n",
    "NMF_df_test = NMF_df_test[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description from tSDV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'tSVD2991', 'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996',\n",
       "       'tSVD2997', 'tSVD2998', 'tSVD2999', 'tSVD3000'],\n",
       "      dtype='object', length=3018)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP test\n",
    "NLP_df_test = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_test_tSVD_3000.csv')\n",
    ")\n",
    "\n",
    "# Set indices as in train test split\n",
    "NLP_df_test = NLP_df_test.set_index('index')\n",
    "\n",
    "NLP_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>From Potter's Field</td>\n",
       "      <td>The sixth book in the Kay Scarpetta series, fr...</td>\n",
       "      <td>['Patricia Cornwell']</td>\n",
       "      <td>http://books.google.com/books/content?id=prefg...</td>\n",
       "      <td>http://books.google.nl/books?id=prefgSxnGOwC&amp;p...</td>\n",
       "      <td>Hachette UK</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>157</td>\n",
       "      <td>3.783439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.011035</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>-0.009888</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.021092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Riverworld and Other Stories</td>\n",
       "      <td>Three stories of a world shared by resurrected...</td>\n",
       "      <td>['Philip José Farmer']</td>\n",
       "      <td>http://books.google.com/books/content?id=TP4oD...</td>\n",
       "      <td>http://books.google.nl/books?id=TP4oDwAAQBAJ&amp;p...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>7</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.007662</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>-0.007470</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>0.012055</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>0.011430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Kenny Doin' Just Fine</td>\n",
       "      <td>KENNY DOIN' JUST FINE Miriam Greenfield, a pro...</td>\n",
       "      <td>['Sadie Wernick Hurwitz']</td>\n",
       "      <td>http://books.google.com/books/content?id=D6Wgi...</td>\n",
       "      <td>http://books.google.nl/books?id=D6WgitXrr8sC&amp;p...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=D6WgitXrr8sC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>-0.004538</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>-0.015920</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>-0.003591</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>-0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Harry on the Rocks</td>\n",
       "      <td>Harry and his boat become stranded on an islan...</td>\n",
       "      <td>['Susan Meddaugh']</td>\n",
       "      <td>http://books.google.com/books/content?id=u5r79...</td>\n",
       "      <td>http://books.google.nl/books?id=u5r79DAUeIYC&amp;q...</td>\n",
       "      <td>Houghton Mifflin Harcourt</td>\n",
       "      <td>http://books.google.nl/books?id=u5r79DAUeIYC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.003397</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.011397</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>-0.004257</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>-0.002277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>The National Review Treasury of Classic Childr...</td>\n",
       "      <td>A collection of over forty stories, tales, poe...</td>\n",
       "      <td>['William F. Buckley, Jr.']</td>\n",
       "      <td>http://books.google.com/books/content?id=NZm7P...</td>\n",
       "      <td>http://books.google.nl/books?id=NZm7PAAACAAJ&amp;d...</td>\n",
       "      <td>Isi Books</td>\n",
       "      <td>http://books.google.nl/books?id=NZm7PAAACAAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002185</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>-0.000960</td>\n",
       "      <td>-0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212041</th>\n",
       "      <td>Man For Maggie Moore (Montana Matchmakers) (Ha...</td>\n",
       "      <td>You don't know what love is until you have los...</td>\n",
       "      <td>['Steven Labree']</td>\n",
       "      <td>http://books.google.com/books/content?id=NZpeJ...</td>\n",
       "      <td>http://books.google.com/books?id=NZpeJhtmGo8C&amp;...</td>\n",
       "      <td>Steven LaBree</td>\n",
       "      <td>http://books.google.com/books?id=NZpeJhtmGo8C&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>3</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>-0.010747</td>\n",
       "      <td>-0.012375</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>-0.006146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212144</th>\n",
       "      <td>Prancing Tiger</td>\n",
       "      <td>To clear the name of his ex-girlfriend's son, ...</td>\n",
       "      <td>['Philip Singerman']</td>\n",
       "      <td>http://books.google.com/books/content?id=68R7S...</td>\n",
       "      <td>http://books.google.com/books?id=68R7SppHYHcC&amp;...</td>\n",
       "      <td>William Morrow</td>\n",
       "      <td>http://books.google.com/books?id=68R7SppHYHcC&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011748</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>-0.006230</td>\n",
       "      <td>-0.008297</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>-0.002252</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>-0.001916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212256</th>\n",
       "      <td>Nude Men: A Novel</td>\n",
       "      <td>The internationally acclaimed debut of a novel...</td>\n",
       "      <td>['Amanda Filipacchi']</td>\n",
       "      <td>http://books.google.com/books/content?id=uM-1A...</td>\n",
       "      <td>http://books.google.com/books?id=uM-1AwAAQBAJ&amp;...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>23</td>\n",
       "      <td>3.739130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010690</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>-0.001923</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>-0.002761</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>0.018299</td>\n",
       "      <td>-0.007997</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.007875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212260</th>\n",
       "      <td>The Tale of Digby</td>\n",
       "      <td>In Digby, Willy Wink finds himself in the midd...</td>\n",
       "      <td>['Timothy Lee Bonnette, Jr.']</td>\n",
       "      <td>http://books.google.com/books/content?id=pcgBA...</td>\n",
       "      <td>http://books.google.com/books?id=pcgBAAAACAAJ&amp;...</td>\n",
       "      <td>Publishamerica Incorporated</td>\n",
       "      <td>http://books.google.com/books?id=pcgBAAAACAAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>0.008438</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>-0.006659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212333</th>\n",
       "      <td>Blueprint For Revenge</td>\n",
       "      <td>Johanna is devastated when her beloved grandmo...</td>\n",
       "      <td>['Martine Jardin']</td>\n",
       "      <td>http://books.google.com/books/content?id=I96XB...</td>\n",
       "      <td>http://books.google.com/books?id=I96XBQAAQBAJ&amp;...</td>\n",
       "      <td>Devine Destinies</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001040</td>\n",
       "      <td>-0.004460</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.008113</td>\n",
       "      <td>-0.002739</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>-0.008496</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5355 rows × 3018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "index                                                       \n",
       "115                                   From Potter's Field   \n",
       "209                          Riverworld and Other Stories   \n",
       "330                                 Kenny Doin' Just Fine   \n",
       "333                                    Harry on the Rocks   \n",
       "371     The National Review Treasury of Classic Childr...   \n",
       "...                                                   ...   \n",
       "212041  Man For Maggie Moore (Montana Matchmakers) (Ha...   \n",
       "212144                                     Prancing Tiger   \n",
       "212256                                  Nude Men: A Novel   \n",
       "212260                                  The Tale of Digby   \n",
       "212333                              Blueprint For Revenge   \n",
       "\n",
       "                                              description  \\\n",
       "index                                                       \n",
       "115     The sixth book in the Kay Scarpetta series, fr...   \n",
       "209     Three stories of a world shared by resurrected...   \n",
       "330     KENNY DOIN' JUST FINE Miriam Greenfield, a pro...   \n",
       "333     Harry and his boat become stranded on an islan...   \n",
       "371     A collection of over forty stories, tales, poe...   \n",
       "...                                                   ...   \n",
       "212041  You don't know what love is until you have los...   \n",
       "212144  To clear the name of his ex-girlfriend's son, ...   \n",
       "212256  The internationally acclaimed debut of a novel...   \n",
       "212260  In Digby, Willy Wink finds himself in the midd...   \n",
       "212333  Johanna is devastated when her beloved grandmo...   \n",
       "\n",
       "                              authors  \\\n",
       "index                                   \n",
       "115             ['Patricia Cornwell']   \n",
       "209            ['Philip José Farmer']   \n",
       "330         ['Sadie Wernick Hurwitz']   \n",
       "333                ['Susan Meddaugh']   \n",
       "371       ['William F. Buckley, Jr.']   \n",
       "...                               ...   \n",
       "212041              ['Steven Labree']   \n",
       "212144           ['Philip Singerman']   \n",
       "212256          ['Amanda Filipacchi']   \n",
       "212260  ['Timothy Lee Bonnette, Jr.']   \n",
       "212333             ['Martine Jardin']   \n",
       "\n",
       "                                                    image  \\\n",
       "index                                                       \n",
       "115     http://books.google.com/books/content?id=prefg...   \n",
       "209     http://books.google.com/books/content?id=TP4oD...   \n",
       "330     http://books.google.com/books/content?id=D6Wgi...   \n",
       "333     http://books.google.com/books/content?id=u5r79...   \n",
       "371     http://books.google.com/books/content?id=NZm7P...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books/content?id=NZpeJ...   \n",
       "212144  http://books.google.com/books/content?id=68R7S...   \n",
       "212256  http://books.google.com/books/content?id=uM-1A...   \n",
       "212260  http://books.google.com/books/content?id=pcgBA...   \n",
       "212333  http://books.google.com/books/content?id=I96XB...   \n",
       "\n",
       "                                              previewLink  \\\n",
       "index                                                       \n",
       "115     http://books.google.nl/books?id=prefgSxnGOwC&p...   \n",
       "209     http://books.google.nl/books?id=TP4oDwAAQBAJ&p...   \n",
       "330     http://books.google.nl/books?id=D6WgitXrr8sC&p...   \n",
       "333     http://books.google.nl/books?id=u5r79DAUeIYC&q...   \n",
       "371     http://books.google.nl/books?id=NZm7PAAACAAJ&d...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books?id=NZpeJhtmGo8C&...   \n",
       "212144  http://books.google.com/books?id=68R7SppHYHcC&...   \n",
       "212256  http://books.google.com/books?id=uM-1AwAAQBAJ&...   \n",
       "212260  http://books.google.com/books?id=pcgBAAAACAAJ&...   \n",
       "212333  http://books.google.com/books?id=I96XBQAAQBAJ&...   \n",
       "\n",
       "                          publisher  \\\n",
       "index                                 \n",
       "115                     Hachette UK   \n",
       "209                 Open Road Media   \n",
       "330                       iUniverse   \n",
       "333       Houghton Mifflin Harcourt   \n",
       "371                       Isi Books   \n",
       "...                             ...   \n",
       "212041                Steven LaBree   \n",
       "212144               William Morrow   \n",
       "212256              Open Road Media   \n",
       "212260  Publishamerica Incorporated   \n",
       "212333             Devine Destinies   \n",
       "\n",
       "                                                 infoLink  \\\n",
       "index                                                       \n",
       "115     https://play.google.com/store/books/details?id...   \n",
       "209     https://play.google.com/store/books/details?id...   \n",
       "330     http://books.google.nl/books?id=D6WgitXrr8sC&d...   \n",
       "333     http://books.google.nl/books?id=u5r79DAUeIYC&d...   \n",
       "371     http://books.google.nl/books?id=NZm7PAAACAAJ&d...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books?id=NZpeJhtmGo8C&...   \n",
       "212144  http://books.google.com/books?id=68R7SppHYHcC&...   \n",
       "212256  https://play.google.com/store/books/details?id...   \n",
       "212260  http://books.google.com/books?id=pcgBAAAACAAJ&...   \n",
       "212333  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                  categories  reviews number  average rating  ...  tSVD2991  \\\n",
       "index                                                         ...             \n",
       "115              ['fiction']             157        3.783439  ...  0.004673   \n",
       "209              ['fiction']               7        4.285714  ...  0.000682   \n",
       "330              ['fiction']               1        5.000000  ... -0.005573   \n",
       "333     ['juvenile fiction']               2        5.000000  ...  0.003240   \n",
       "371     ['juvenile fiction']               3        5.000000  ... -0.002185   \n",
       "...                      ...             ...             ...  ...       ...   \n",
       "212041           ['fiction']               3        4.666667  ...  0.004356   \n",
       "212144           ['fiction']               4        4.250000  ...  0.011748   \n",
       "212256           ['fiction']              23        3.739130  ... -0.010690   \n",
       "212260           ['fiction']               2        4.000000  ...  0.002488   \n",
       "212333           ['fiction']               1        5.000000  ... -0.001040   \n",
       "\n",
       "        tSVD2992  tSVD2993  tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  \\\n",
       "index                                                                          \n",
       "115     0.011035  0.000885  0.011355  0.003110 -0.009888  0.001707 -0.000568   \n",
       "209    -0.000072 -0.007662  0.010220  0.001084 -0.007470 -0.006064  0.012055   \n",
       "330    -0.004538  0.000019  0.002786 -0.015920  0.004716 -0.000231 -0.003591   \n",
       "333    -0.001592 -0.003397  0.001563  0.008503  0.011397 -0.003413 -0.004257   \n",
       "371    -0.001413  0.003879  0.004376  0.000110  0.002358  0.005980  0.005749   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212041  0.004014  0.000388 -0.010747 -0.012375 -0.001525  0.006544  0.013144   \n",
       "212144 -0.000504  0.006213 -0.006230 -0.008297 -0.009560 -0.002252  0.006993   \n",
       "212256  0.012784 -0.001923  0.008254 -0.002761 -0.003366  0.018299 -0.007997   \n",
       "212260  0.002354  0.004941 -0.000441  0.008438  0.012563  0.002361  0.008794   \n",
       "212333 -0.004460 -0.001457 -0.008113 -0.002739 -0.001419  0.011348 -0.008496   \n",
       "\n",
       "        tSVD2999  tSVD3000  \n",
       "index                       \n",
       "115     0.000850  0.021092  \n",
       "209    -0.007764  0.011430  \n",
       "330     0.000867 -0.001421  \n",
       "333     0.006653 -0.002277  \n",
       "371    -0.000960 -0.003924  \n",
       "...          ...       ...  \n",
       "212041  0.003183 -0.006146  \n",
       "212144  0.007963 -0.001916  \n",
       "212256  0.004508  0.007875  \n",
       "212260  0.000451 -0.006659  \n",
       "212333  0.005376  0.000832  \n",
       "\n",
       "[5355 rows x 3018 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'tSVD2991', 'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996',\n",
       "       'tSVD2997', 'tSVD2998', 'tSVD2999', 'tSVD3000'],\n",
       "      dtype='object', length=3018)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP train\n",
    "NLP_df_train = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_train_tSVD_3000.csv')\n",
    ")\n",
    "\n",
    "NLP_df_train = NLP_df_train.set_index('index')\n",
    "\n",
    "NLP_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "index                                                       \n",
       "3                           Whispers of the Wicked Saints   \n",
       "24               The Forbidden Stories of Marta Veneranda   \n",
       "42                                Tess and the Highlander   \n",
       "49      Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "73                     Night World: Daughters Of Darkness   \n",
       "...                                                   ...   \n",
       "212361                                       Calder Pride   \n",
       "212365                                      The Road Back   \n",
       "212394                                       Final things   \n",
       "212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "212402                                  The Autograph Man   \n",
       "\n",
       "                                              description  \\\n",
       "index                                                       \n",
       "3       Julia Thomas finds her life spinning out of co...   \n",
       "24      Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "42      In 1543, on a windswept isle off of Scotland, ...   \n",
       "49      Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "73      \"There’s something strange about the new girls...   \n",
       "...                                                   ...   \n",
       "212361  The Long-Awaited Addition to the Beloved Calde...   \n",
       "212365  The sequel to the masterpiece All Quiet on the...   \n",
       "212394  Grace's father believes in science and builds ...   \n",
       "212399  During a school trip to Ellis Island, Dominick...   \n",
       "212402  Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                         authors  \\\n",
       "index                              \n",
       "3            ['Veronica Haddon']   \n",
       "24       ['Sonia Rivera-Valdes']   \n",
       "42            ['May Mcgoldrick']   \n",
       "49        ['Elizabeth Sinclair']   \n",
       "73                ['L.J. Smith']   \n",
       "...                          ...   \n",
       "212361          ['Janet Dailey']   \n",
       "212365  ['Erich Maria Remarque']   \n",
       "212394          ['Jenny Offill']   \n",
       "212399       ['Elvira Woodruff']   \n",
       "212402           ['Zadie Smith']   \n",
       "\n",
       "                                                    image  \\\n",
       "index                                                       \n",
       "3       http://books.google.com/books/content?id=aRSIg...   \n",
       "24      http://books.google.com/books/content?id=A7aYb...   \n",
       "42      http://books.google.com/books/content?id=VmCRS...   \n",
       "49      http://books.google.com/books/content?id=Z6uzJ...   \n",
       "73      http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                   ...   \n",
       "212361  http://books.google.com/books/content?id=nlsgd...   \n",
       "212365  http://books.google.com/books/content?id=obZdA...   \n",
       "212394  http://books.google.com/books/content?id=UbSFB...   \n",
       "212399  http://books.google.com/books/content?id=J7M-N...   \n",
       "212402  http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                              previewLink  \\\n",
       "index                                                       \n",
       "3       http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24      http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "42      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49      http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "73      http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                   ...   \n",
       "212361  http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "212365  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394  http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "212399  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402  http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                                publisher  \\\n",
       "index                                                       \n",
       "3                                               iUniverse   \n",
       "24                                    Seven Stories Press   \n",
       "42                                         Harper Collins   \n",
       "49      Harlequin Treasury-Harlequin American Romance 90s   \n",
       "73                                     Simon and Schuster   \n",
       "...                                                   ...   \n",
       "212361                                     Harper Collins   \n",
       "212365                      Random House Trade Paperbacks   \n",
       "212394                                            Vintage   \n",
       "212399                              Scholastic Paperbacks   \n",
       "212402                                            Vintage   \n",
       "\n",
       "                                                 infoLink  \\\n",
       "index                                                       \n",
       "3       http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24      http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "42      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49      http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "73      http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                   ...   \n",
       "212361  https://play.google.com/store/books/details?id...   \n",
       "212365  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394  https://play.google.com/store/books/details?id...   \n",
       "212399  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                  categories  reviews number  average rating  ...  tSVD2991  \\\n",
       "index                                                         ...             \n",
       "3                ['fiction']              32        3.718750  ...  0.001714   \n",
       "24               ['fiction']               1        5.000000  ...  0.000900   \n",
       "42      ['juvenile fiction']              17        4.235294  ... -0.002241   \n",
       "49               ['fiction']               2        5.000000  ...  0.004947   \n",
       "73      ['juvenile fiction']             134        4.768657  ...  0.000070   \n",
       "...                      ...             ...             ...  ...       ...   \n",
       "212361           ['fiction']              28        4.035714  ... -0.003849   \n",
       "212365           ['fiction']              17        4.705882  ...  0.014579   \n",
       "212394           ['fiction']               1        4.000000  ...  0.007651   \n",
       "212399  ['juvenile fiction']              28        4.678571  ... -0.012930   \n",
       "212402           ['fiction']               4        2.500000  ... -0.007902   \n",
       "\n",
       "        tSVD2992  tSVD2993  tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  \\\n",
       "index                                                                          \n",
       "3      -0.012096  0.012755  0.004345  0.016297  0.022422  0.026395 -0.001808   \n",
       "24      0.002294 -0.002052 -0.013243 -0.002329 -0.005818 -0.000012  0.007939   \n",
       "42     -0.007230 -0.005164  0.000416 -0.007837 -0.002487 -0.004066  0.010140   \n",
       "49     -0.003206  0.011330 -0.004898 -0.002988  0.001996 -0.008123 -0.000639   \n",
       "73      0.003473 -0.013758  0.000954 -0.011287 -0.008795  0.003780  0.002349   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361 -0.000571  0.001393 -0.001693 -0.012675 -0.000349  0.002634 -0.004115   \n",
       "212365 -0.010071 -0.008840  0.011693  0.006862 -0.001960  0.004995 -0.007247   \n",
       "212394  0.005601  0.000670 -0.010949 -0.012086  0.002205 -0.004838 -0.008230   \n",
       "212399  0.005578 -0.000719 -0.004401  0.003777 -0.001121 -0.006025 -0.007464   \n",
       "212402  0.017672  0.005686  0.007956 -0.003892 -0.007589 -0.002541  0.005038   \n",
       "\n",
       "        tSVD2999  tSVD3000  \n",
       "index                       \n",
       "3      -0.028102  0.011717  \n",
       "24     -0.024247 -0.006868  \n",
       "42     -0.007082  0.001236  \n",
       "49     -0.016424 -0.004971  \n",
       "73     -0.013346  0.001237  \n",
       "...          ...       ...  \n",
       "212361  0.004868 -0.006720  \n",
       "212365 -0.013905 -0.001202  \n",
       "212394  0.000084 -0.002611  \n",
       "212399 -0.008325  0.005250  \n",
       "212402 -0.010622 -0.009332  \n",
       "\n",
       "[21419 rows x 3018 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [col for col in NLP_df_test.columns if col.startswith('tSVD')]\n",
    "\n",
    "NLP_df_train = NLP_df_train[columns_to_keep]\n",
    "NLP_df_test = NLP_df_test[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "index                                                                          \n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2991  tSVD2992  tSVD2993  \\\n",
       "index                                 ...                                 \n",
       "3       0.006327  0.040408  0.035931  ...  0.001714 -0.012096  0.012755   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.000900  0.002294 -0.002052   \n",
       "42      0.042483  0.019502  0.016915  ... -0.002241 -0.007230 -0.005164   \n",
       "49     -0.027805 -0.038459 -0.016390  ...  0.004947 -0.003206  0.011330   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.000070  0.003473 -0.013758   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.003849 -0.000571  0.001393   \n",
       "212365 -0.028389  0.026683  0.003763  ...  0.014579 -0.010071 -0.008840   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.007651  0.005601  0.000670   \n",
       "212399 -0.028692 -0.017410  0.020350  ... -0.012930  0.005578 -0.000719   \n",
       "212402 -0.004967 -0.007246 -0.009244  ... -0.007902  0.017672  0.005686   \n",
       "\n",
       "        tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000  \n",
       "index                                                                         \n",
       "3       0.004345  0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  \n",
       "24     -0.013243 -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  \n",
       "42      0.000416 -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  \n",
       "49     -0.004898 -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  \n",
       "73      0.000954 -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "212361 -0.001693 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  \n",
       "212365  0.011693  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  \n",
       "212394 -0.010949 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  \n",
       "212399 -0.004401  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  \n",
       "212402  0.007956 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  \n",
       "\n",
       "[21419 rows x 3000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description dimension reduction NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "Inputs to choose:\n",
    "- number of layers:\n",
    "    - Description NN\n",
    "        - input\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - final layer\n",
    "    - Description and image embeddings NN\n",
    "        - input\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - noise\n",
    "        - final layer\n",
    "    Too many?   \n",
    "- add dense layers to avoid overfitting?\n",
    "- activation functions\n",
    "    - ReLu (Rectified linear activation function): piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. Simple but effective.\n",
    "- Use linear in the last layer to obtain a continuous variable\n",
    "- optimizer: \n",
    "    - Adam; works with momentums of first and second order. \n",
    "    - sdg: variant of Gradient Descent (Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. It's easy and works well but there is the risk that the model gets stuck in local minima)\n",
    "- loss function\n",
    "    - MSE?\n",
    "- number of epochs\n",
    "- which metric to use to evaluate the model?\n",
    "    - MSE\n",
    "    - MAE\n",
    "\n",
    "- Use gridsearch to optimise hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,536,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,667,840</span> (6.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,667,840\u001b[0m (6.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,667,840</span> (6.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,667,840\u001b[0m (6.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get number of inputs - second element of shape (i.e. number of columns in X)\n",
    "input_shape = NLP_df_train.shape[1]\n",
    "\n",
    "# neurons number\n",
    "n_neurons = 512\n",
    "\n",
    "# define a model\n",
    "baseline_model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "baseline_model.add(layers.Dense(\n",
    "            n_neurons, # number of neurons\n",
    "            input_dim = input_shape, # number of inputs \n",
    "            activation = 'relu' # activation faunction\n",
    "            ))\n",
    "\n",
    "# Hidden - Layers\n",
    "baseline_model.add(layers.Dense(\n",
    "                    256, \n",
    "                    activation = \"linear\"))\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mean_squared_error'], \n",
    "    metrics = ['mae', 'mean_squared_error']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 5.8223 - mae: 1.6919 - mean_squared_error: 5.8223 - val_loss: 0.0681 - val_mae: 0.1823 - val_mean_squared_error: 0.0665\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0593 - mae: 0.1786 - mean_squared_error: 0.0593 - val_loss: 0.0664 - val_mae: 0.1834 - val_mean_squared_error: 0.0650\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0617 - mae: 0.1828 - mean_squared_error: 0.0617 - val_loss: 0.0631 - val_mae: 0.1746 - val_mean_squared_error: 0.0618\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0621 - mae: 0.1822 - mean_squared_error: 0.0621 - val_loss: 0.0620 - val_mae: 0.1706 - val_mean_squared_error: 0.0604\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0577 - mae: 0.1736 - mean_squared_error: 0.0577 - val_loss: 0.0596 - val_mae: 0.1713 - val_mean_squared_error: 0.0586\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0564 - mae: 0.1722 - mean_squared_error: 0.0564 - val_loss: 0.0587 - val_mae: 0.1703 - val_mean_squared_error: 0.0576\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0560 - mae: 0.1717 - mean_squared_error: 0.0560 - val_loss: 0.0594 - val_mae: 0.1744 - val_mean_squared_error: 0.0583\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0581 - mae: 0.1718 - mean_squared_error: 0.0581 - val_loss: 0.0582 - val_mae: 0.1597 - val_mean_squared_error: 0.0570\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0524 - mae: 0.1649 - mean_squared_error: 0.0524 - val_loss: 0.0595 - val_mae: 0.1639 - val_mean_squared_error: 0.0577\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0528 - mae: 0.1646 - mean_squared_error: 0.0528 - val_loss: 0.0558 - val_mae: 0.1592 - val_mean_squared_error: 0.0542\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0519 - mae: 0.1625 - mean_squared_error: 0.0519 - val_loss: 0.0555 - val_mae: 0.1592 - val_mean_squared_error: 0.0542\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0499 - mae: 0.1590 - mean_squared_error: 0.0499 - val_loss: 0.0557 - val_mae: 0.1558 - val_mean_squared_error: 0.0541\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0498 - mae: 0.1577 - mean_squared_error: 0.0498 - val_loss: 0.0552 - val_mae: 0.1563 - val_mean_squared_error: 0.0536\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0481 - mae: 0.1554 - mean_squared_error: 0.0481 - val_loss: 0.0548 - val_mae: 0.1600 - val_mean_squared_error: 0.0531\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0488 - mae: 0.1573 - mean_squared_error: 0.0488 - val_loss: 0.0561 - val_mae: 0.1534 - val_mean_squared_error: 0.0546\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0480 - mae: 0.1544 - mean_squared_error: 0.0480 - val_loss: 0.0534 - val_mae: 0.1580 - val_mean_squared_error: 0.0521\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0478 - mae: 0.1541 - mean_squared_error: 0.0478 - val_loss: 0.0553 - val_mae: 0.1528 - val_mean_squared_error: 0.0539\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0463 - mae: 0.1523 - mean_squared_error: 0.0463 - val_loss: 0.0538 - val_mae: 0.1544 - val_mean_squared_error: 0.0521\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0475 - mae: 0.1532 - mean_squared_error: 0.0475 - val_loss: 0.0597 - val_mae: 0.1565 - val_mean_squared_error: 0.0579\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0471 - mae: 0.1528 - mean_squared_error: 0.0471 - val_loss: 0.0535 - val_mae: 0.1573 - val_mean_squared_error: 0.0520\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0497 - mae: 0.1572 - mean_squared_error: 0.0497 - val_loss: 0.0523 - val_mae: 0.1540 - val_mean_squared_error: 0.0508\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0458 - mae: 0.1514 - mean_squared_error: 0.0458 - val_loss: 0.0527 - val_mae: 0.1591 - val_mean_squared_error: 0.0515\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0444 - mae: 0.1499 - mean_squared_error: 0.0444 - val_loss: 0.0576 - val_mae: 0.1776 - val_mean_squared_error: 0.0564\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0457 - mae: 0.1510 - mean_squared_error: 0.0457 - val_loss: 0.0543 - val_mae: 0.1660 - val_mean_squared_error: 0.0530\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0455 - mae: 0.1503 - mean_squared_error: 0.0455 - val_loss: 0.0537 - val_mae: 0.1500 - val_mean_squared_error: 0.0523\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0459 - mae: 0.1522 - mean_squared_error: 0.0459 - val_loss: 0.0539 - val_mae: 0.1627 - val_mean_squared_error: 0.0525\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0451 - mae: 0.1494 - mean_squared_error: 0.0451 - val_loss: 0.0534 - val_mae: 0.1617 - val_mean_squared_error: 0.0521\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0454 - mae: 0.1494 - mean_squared_error: 0.0454 - val_loss: 0.0543 - val_mae: 0.1481 - val_mean_squared_error: 0.0526\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0431 - mae: 0.1461 - mean_squared_error: 0.0431 - val_loss: 0.0540 - val_mae: 0.1491 - val_mean_squared_error: 0.0525\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0452 - mae: 0.1506 - mean_squared_error: 0.0452 - val_loss: 0.0551 - val_mae: 0.1488 - val_mean_squared_error: 0.0535\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0441 - mae: 0.1487 - mean_squared_error: 0.0441 - val_loss: 0.0526 - val_mae: 0.1484 - val_mean_squared_error: 0.0512\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0437 - mae: 0.1470 - mean_squared_error: 0.0437 - val_loss: 0.0520 - val_mae: 0.1491 - val_mean_squared_error: 0.0505\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0428 - mae: 0.1457 - mean_squared_error: 0.0428 - val_loss: 0.0545 - val_mae: 0.1671 - val_mean_squared_error: 0.0534\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0457 - mae: 0.1502 - mean_squared_error: 0.0457 - val_loss: 0.0552 - val_mae: 0.1714 - val_mean_squared_error: 0.0540\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0440 - mae: 0.1482 - mean_squared_error: 0.0440 - val_loss: 0.0546 - val_mae: 0.1489 - val_mean_squared_error: 0.0529\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0415 - mae: 0.1430 - mean_squared_error: 0.0415 - val_loss: 0.0520 - val_mae: 0.1542 - val_mean_squared_error: 0.0506\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0433 - mae: 0.1466 - mean_squared_error: 0.0433 - val_loss: 0.0528 - val_mae: 0.1474 - val_mean_squared_error: 0.0512\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0386 - mae: 0.1380 - mean_squared_error: 0.0386 - val_loss: 0.0565 - val_mae: 0.1506 - val_mean_squared_error: 0.0547\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0374 - mae: 0.1374 - mean_squared_error: 0.0374 - val_loss: 0.0539 - val_mae: 0.1632 - val_mean_squared_error: 0.0526\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0344 - mae: 0.1326 - mean_squared_error: 0.0344 - val_loss: 0.0602 - val_mae: 0.1579 - val_mean_squared_error: 0.0586\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0334 - mae: 0.1304 - mean_squared_error: 0.0334 - val_loss: 0.0664 - val_mae: 0.1695 - val_mean_squared_error: 0.0645\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0317 - mae: 0.1289 - mean_squared_error: 0.0317 - val_loss: 0.0730 - val_mae: 0.1833 - val_mean_squared_error: 0.0712\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0274 - mae: 0.1210 - mean_squared_error: 0.0274 - val_loss: 0.0567 - val_mae: 0.1646 - val_mean_squared_error: 0.0554\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0242 - mae: 0.1137 - mean_squared_error: 0.0242 - val_loss: 0.0653 - val_mae: 0.1702 - val_mean_squared_error: 0.0636\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0204 - mae: 0.1041 - mean_squared_error: 0.0204 - val_loss: 0.0622 - val_mae: 0.1660 - val_mean_squared_error: 0.0605\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0178 - mae: 0.0958 - mean_squared_error: 0.0178 - val_loss: 0.0699 - val_mae: 0.1786 - val_mean_squared_error: 0.0681\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0151 - mae: 0.0897 - mean_squared_error: 0.0151 - val_loss: 0.0625 - val_mae: 0.1693 - val_mean_squared_error: 0.0611\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0126 - mae: 0.0810 - mean_squared_error: 0.0126 - val_loss: 0.0762 - val_mae: 0.1895 - val_mean_squared_error: 0.0746\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0118 - mae: 0.0769 - mean_squared_error: 0.0118 - val_loss: 0.0761 - val_mae: 0.1897 - val_mean_squared_error: 0.0743\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0102 - mae: 0.0706 - mean_squared_error: 0.0102 - val_loss: 0.0722 - val_mae: 0.1842 - val_mean_squared_error: 0.0706\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0099 - mae: 0.0675 - mean_squared_error: 0.0099 - val_loss: 0.0661 - val_mae: 0.1752 - val_mean_squared_error: 0.0646\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0096 - mae: 0.0648 - mean_squared_error: 0.0096 - val_loss: 0.0694 - val_mae: 0.1801 - val_mean_squared_error: 0.0680\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0085 - mae: 0.0613 - mean_squared_error: 0.0085 - val_loss: 0.0749 - val_mae: 0.1882 - val_mean_squared_error: 0.0735\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0091 - mae: 0.0628 - mean_squared_error: 0.0091 - val_loss: 0.0784 - val_mae: 0.1946 - val_mean_squared_error: 0.0768\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0086 - mae: 0.0606 - mean_squared_error: 0.0086 - val_loss: 0.0714 - val_mae: 0.1842 - val_mean_squared_error: 0.0700\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0087 - mae: 0.0598 - mean_squared_error: 0.0087 - val_loss: 0.0769 - val_mae: 0.1912 - val_mean_squared_error: 0.0754\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0083 - mae: 0.0579 - mean_squared_error: 0.0083 - val_loss: 0.0731 - val_mae: 0.1861 - val_mean_squared_error: 0.0715\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0074 - mae: 0.0547 - mean_squared_error: 0.0074 - val_loss: 0.0793 - val_mae: 0.1961 - val_mean_squared_error: 0.0778\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0072 - mae: 0.0543 - mean_squared_error: 0.0072 - val_loss: 0.0719 - val_mae: 0.1852 - val_mean_squared_error: 0.0707\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0074 - mae: 0.0555 - mean_squared_error: 0.0074 - val_loss: 0.0707 - val_mae: 0.1835 - val_mean_squared_error: 0.0694\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0075 - mae: 0.0539 - mean_squared_error: 0.0075 - val_loss: 0.0726 - val_mae: 0.1863 - val_mean_squared_error: 0.0713\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0073 - mae: 0.0517 - mean_squared_error: 0.0073 - val_loss: 0.0848 - val_mae: 0.2047 - val_mean_squared_error: 0.0832\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0069 - mae: 0.0524 - mean_squared_error: 0.0069 - val_loss: 0.0699 - val_mae: 0.1822 - val_mean_squared_error: 0.0686\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0078 - mae: 0.0534 - mean_squared_error: 0.0078 - val_loss: 0.0836 - val_mae: 0.2022 - val_mean_squared_error: 0.0819\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0072 - mae: 0.0519 - mean_squared_error: 0.0072 - val_loss: 0.0746 - val_mae: 0.1887 - val_mean_squared_error: 0.0732\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0069 - mae: 0.0500 - mean_squared_error: 0.0069 - val_loss: 0.0712 - val_mae: 0.1830 - val_mean_squared_error: 0.0699\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0487 - mean_squared_error: 0.0066 - val_loss: 0.0778 - val_mae: 0.1939 - val_mean_squared_error: 0.0764\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0491 - mean_squared_error: 0.0066 - val_loss: 0.0879 - val_mae: 0.2101 - val_mean_squared_error: 0.0863\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0505 - mean_squared_error: 0.0066 - val_loss: 0.0964 - val_mae: 0.2241 - val_mean_squared_error: 0.0947\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0075 - mae: 0.0525 - mean_squared_error: 0.0075 - val_loss: 0.0766 - val_mae: 0.1911 - val_mean_squared_error: 0.0752\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0062 - mae: 0.0485 - mean_squared_error: 0.0062 - val_loss: 0.0727 - val_mae: 0.1861 - val_mean_squared_error: 0.0715\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0064 - mae: 0.0481 - mean_squared_error: 0.0064 - val_loss: 0.0667 - val_mae: 0.1784 - val_mean_squared_error: 0.0657\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0065 - mae: 0.0476 - mean_squared_error: 0.0065 - val_loss: 0.0745 - val_mae: 0.1885 - val_mean_squared_error: 0.0732\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0060 - mae: 0.0468 - mean_squared_error: 0.0060 - val_loss: 0.0778 - val_mae: 0.1934 - val_mean_squared_error: 0.0764\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0061 - mae: 0.0461 - mean_squared_error: 0.0061 - val_loss: 0.0661 - val_mae: 0.1786 - val_mean_squared_error: 0.0649\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0063 - mae: 0.0497 - mean_squared_error: 0.0063 - val_loss: 0.0726 - val_mae: 0.1853 - val_mean_squared_error: 0.0711\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0068 - mae: 0.0479 - mean_squared_error: 0.0068 - val_loss: 0.0687 - val_mae: 0.1804 - val_mean_squared_error: 0.0674\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0059 - mae: 0.0450 - mean_squared_error: 0.0059 - val_loss: 0.0779 - val_mae: 0.1938 - val_mean_squared_error: 0.0765\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0059 - mae: 0.0473 - mean_squared_error: 0.0059 - val_loss: 0.0703 - val_mae: 0.1820 - val_mean_squared_error: 0.0689\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0061 - mae: 0.0476 - mean_squared_error: 0.0061 - val_loss: 0.0725 - val_mae: 0.1850 - val_mean_squared_error: 0.0711\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0056 - mae: 0.0444 - mean_squared_error: 0.0056 - val_loss: 0.0850 - val_mae: 0.2057 - val_mean_squared_error: 0.0836\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0061 - mae: 0.0447 - mean_squared_error: 0.0061 - val_loss: 0.0719 - val_mae: 0.1849 - val_mean_squared_error: 0.0706\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0060 - mae: 0.0453 - mean_squared_error: 0.0060 - val_loss: 0.0732 - val_mae: 0.1858 - val_mean_squared_error: 0.0717\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0060 - mae: 0.0457 - mean_squared_error: 0.0060 - val_loss: 0.0736 - val_mae: 0.1879 - val_mean_squared_error: 0.0723\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0064 - mae: 0.0471 - mean_squared_error: 0.0064 - val_loss: 0.0723 - val_mae: 0.1855 - val_mean_squared_error: 0.0710\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0060 - mae: 0.0453 - mean_squared_error: 0.0060 - val_loss: 0.0685 - val_mae: 0.1794 - val_mean_squared_error: 0.0674\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0443 - mean_squared_error: 0.0066 - val_loss: 0.0705 - val_mae: 0.1828 - val_mean_squared_error: 0.0694\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0056 - mae: 0.0472 - mean_squared_error: 0.0056 - val_loss: 0.0855 - val_mae: 0.2065 - val_mean_squared_error: 0.0840\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0063 - mae: 0.0483 - mean_squared_error: 0.0063 - val_loss: 0.0665 - val_mae: 0.1773 - val_mean_squared_error: 0.0653\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0056 - mae: 0.0434 - mean_squared_error: 0.0056 - val_loss: 0.0721 - val_mae: 0.1850 - val_mean_squared_error: 0.0708\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0064 - mae: 0.0487 - mean_squared_error: 0.0064 - val_loss: 0.0656 - val_mae: 0.1774 - val_mean_squared_error: 0.0644\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0060 - mae: 0.0472 - mean_squared_error: 0.0060 - val_loss: 0.0678 - val_mae: 0.1783 - val_mean_squared_error: 0.0666\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0498 - mean_squared_error: 0.0066 - val_loss: 0.0721 - val_mae: 0.1848 - val_mean_squared_error: 0.0708\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0055 - mae: 0.0422 - mean_squared_error: 0.0055 - val_loss: 0.0688 - val_mae: 0.1800 - val_mean_squared_error: 0.0676\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0053 - mae: 0.0418 - mean_squared_error: 0.0053 - val_loss: 0.0688 - val_mae: 0.1796 - val_mean_squared_error: 0.0675\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0051 - mae: 0.0441 - mean_squared_error: 0.0051 - val_loss: 0.0685 - val_mae: 0.1795 - val_mean_squared_error: 0.0673\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0056 - mae: 0.0436 - mean_squared_error: 0.0056 - val_loss: 0.0714 - val_mae: 0.1839 - val_mean_squared_error: 0.0702\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0055 - mae: 0.0432 - mean_squared_error: 0.0055 - val_loss: 0.0782 - val_mae: 0.1945 - val_mean_squared_error: 0.0768\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0052 - mae: 0.0417 - mean_squared_error: 0.0052 - val_loss: 0.0700 - val_mae: 0.1820 - val_mean_squared_error: 0.0687\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0052 - mae: 0.0417 - mean_squared_error: 0.0052 - val_loss: 0.0652 - val_mae: 0.1760 - val_mean_squared_error: 0.0640\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs_hist = baseline_model.fit(\n",
    "    NLP_df_train, # input\n",
    "    y_wr_train, # output\n",
    "    epochs=100, # number of iterations\n",
    "    batch_size=50, # number of observations taken to train the data\n",
    "    verbose=1,\n",
    "    validation_data = (NLP_df_test, y_wr_test),\n",
    "    shuffle = True\n",
    "    #validation_split=0.2,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate intermediate description features with lower dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m670/670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict baseline X train and X test \n",
    "\n",
    "NLP_intermediate_train = baseline_model.predict(NLP_df_train)\n",
    "NLP_intermediate_test = baseline_model.predict(NLP_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.391864 , 4.3906555, 4.395023 , ..., 4.3978972, 4.394945 ,\n",
       "        4.394764 ],\n",
       "       [4.1803966, 4.181886 , 4.192978 , ..., 4.1850934, 4.183947 ,\n",
       "        4.185663 ],\n",
       "       [4.1231346, 4.120899 , 4.129018 , ..., 4.122696 , 4.123878 ,\n",
       "        4.12049  ],\n",
       "       ...,\n",
       "       [4.169956 , 4.1723447, 4.1878123, ..., 4.177408 , 4.1834483,\n",
       "        4.1883984],\n",
       "       [4.2660465, 4.2685523, 4.272393 , ..., 4.266396 , 4.2657576,\n",
       "        4.2643247],\n",
       "       [4.153614 , 4.1443214, 4.1541653, ..., 4.153152 , 4.1550703,\n",
       "        4.153667 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_intermediate_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NLP_intermediate_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store these into a dataframe\n",
    "NLP_intermediate_train_df = pd.DataFrame(NLP_intermediate_train, index=NLP_df_train.index)\n",
    "NLP_intermediate_test_df = pd.DataFrame(NLP_intermediate_test, index=NLP_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.391864</td>\n",
       "      <td>4.390656</td>\n",
       "      <td>4.395023</td>\n",
       "      <td>4.388587</td>\n",
       "      <td>4.389089</td>\n",
       "      <td>4.393358</td>\n",
       "      <td>4.391413</td>\n",
       "      <td>4.386438</td>\n",
       "      <td>4.399346</td>\n",
       "      <td>4.395594</td>\n",
       "      <td>...</td>\n",
       "      <td>4.398738</td>\n",
       "      <td>4.389634</td>\n",
       "      <td>4.393667</td>\n",
       "      <td>4.393713</td>\n",
       "      <td>4.393002</td>\n",
       "      <td>4.389347</td>\n",
       "      <td>4.391222</td>\n",
       "      <td>4.397897</td>\n",
       "      <td>4.394945</td>\n",
       "      <td>4.394764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.180397</td>\n",
       "      <td>4.181886</td>\n",
       "      <td>4.192978</td>\n",
       "      <td>4.185059</td>\n",
       "      <td>4.189871</td>\n",
       "      <td>4.193472</td>\n",
       "      <td>4.185056</td>\n",
       "      <td>4.182319</td>\n",
       "      <td>4.184555</td>\n",
       "      <td>4.191511</td>\n",
       "      <td>...</td>\n",
       "      <td>4.187167</td>\n",
       "      <td>4.184680</td>\n",
       "      <td>4.186547</td>\n",
       "      <td>4.180562</td>\n",
       "      <td>4.187067</td>\n",
       "      <td>4.186366</td>\n",
       "      <td>4.184957</td>\n",
       "      <td>4.185093</td>\n",
       "      <td>4.183947</td>\n",
       "      <td>4.185663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.123135</td>\n",
       "      <td>4.120899</td>\n",
       "      <td>4.129018</td>\n",
       "      <td>4.123671</td>\n",
       "      <td>4.123624</td>\n",
       "      <td>4.127313</td>\n",
       "      <td>4.124439</td>\n",
       "      <td>4.119629</td>\n",
       "      <td>4.123821</td>\n",
       "      <td>4.130284</td>\n",
       "      <td>...</td>\n",
       "      <td>4.121098</td>\n",
       "      <td>4.125817</td>\n",
       "      <td>4.124756</td>\n",
       "      <td>4.118839</td>\n",
       "      <td>4.124855</td>\n",
       "      <td>4.121467</td>\n",
       "      <td>4.118801</td>\n",
       "      <td>4.122696</td>\n",
       "      <td>4.123878</td>\n",
       "      <td>4.120490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.432966</td>\n",
       "      <td>4.429181</td>\n",
       "      <td>4.441802</td>\n",
       "      <td>4.434208</td>\n",
       "      <td>4.440212</td>\n",
       "      <td>4.434598</td>\n",
       "      <td>4.431140</td>\n",
       "      <td>4.431800</td>\n",
       "      <td>4.440544</td>\n",
       "      <td>4.436188</td>\n",
       "      <td>...</td>\n",
       "      <td>4.439245</td>\n",
       "      <td>4.433405</td>\n",
       "      <td>4.437764</td>\n",
       "      <td>4.436251</td>\n",
       "      <td>4.438423</td>\n",
       "      <td>4.430849</td>\n",
       "      <td>4.430588</td>\n",
       "      <td>4.434324</td>\n",
       "      <td>4.437403</td>\n",
       "      <td>4.434786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4.183240</td>\n",
       "      <td>4.178967</td>\n",
       "      <td>4.178316</td>\n",
       "      <td>4.173810</td>\n",
       "      <td>4.181118</td>\n",
       "      <td>4.178365</td>\n",
       "      <td>4.179305</td>\n",
       "      <td>4.186319</td>\n",
       "      <td>4.179751</td>\n",
       "      <td>4.176682</td>\n",
       "      <td>...</td>\n",
       "      <td>4.176789</td>\n",
       "      <td>4.177713</td>\n",
       "      <td>4.175012</td>\n",
       "      <td>4.180591</td>\n",
       "      <td>4.178446</td>\n",
       "      <td>4.176745</td>\n",
       "      <td>4.176698</td>\n",
       "      <td>4.183792</td>\n",
       "      <td>4.179440</td>\n",
       "      <td>4.174779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>4.140194</td>\n",
       "      <td>4.128915</td>\n",
       "      <td>4.137585</td>\n",
       "      <td>4.133186</td>\n",
       "      <td>4.141717</td>\n",
       "      <td>4.143141</td>\n",
       "      <td>4.142426</td>\n",
       "      <td>4.135183</td>\n",
       "      <td>4.147612</td>\n",
       "      <td>4.141233</td>\n",
       "      <td>...</td>\n",
       "      <td>4.142248</td>\n",
       "      <td>4.141637</td>\n",
       "      <td>4.126824</td>\n",
       "      <td>4.139755</td>\n",
       "      <td>4.135477</td>\n",
       "      <td>4.141424</td>\n",
       "      <td>4.136750</td>\n",
       "      <td>4.144142</td>\n",
       "      <td>4.141432</td>\n",
       "      <td>4.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>4.142812</td>\n",
       "      <td>4.138370</td>\n",
       "      <td>4.134733</td>\n",
       "      <td>4.131379</td>\n",
       "      <td>4.137354</td>\n",
       "      <td>4.154920</td>\n",
       "      <td>4.134166</td>\n",
       "      <td>4.126167</td>\n",
       "      <td>4.146227</td>\n",
       "      <td>4.136079</td>\n",
       "      <td>...</td>\n",
       "      <td>4.118249</td>\n",
       "      <td>4.145643</td>\n",
       "      <td>4.141264</td>\n",
       "      <td>4.114353</td>\n",
       "      <td>4.123492</td>\n",
       "      <td>4.146420</td>\n",
       "      <td>4.134927</td>\n",
       "      <td>4.154398</td>\n",
       "      <td>4.127565</td>\n",
       "      <td>4.140688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>4.169956</td>\n",
       "      <td>4.172345</td>\n",
       "      <td>4.187812</td>\n",
       "      <td>4.173820</td>\n",
       "      <td>4.183793</td>\n",
       "      <td>4.185894</td>\n",
       "      <td>4.194084</td>\n",
       "      <td>4.185398</td>\n",
       "      <td>4.180892</td>\n",
       "      <td>4.182940</td>\n",
       "      <td>...</td>\n",
       "      <td>4.193676</td>\n",
       "      <td>4.174583</td>\n",
       "      <td>4.180787</td>\n",
       "      <td>4.185509</td>\n",
       "      <td>4.178794</td>\n",
       "      <td>4.177565</td>\n",
       "      <td>4.171576</td>\n",
       "      <td>4.177408</td>\n",
       "      <td>4.183448</td>\n",
       "      <td>4.188398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>4.266047</td>\n",
       "      <td>4.268552</td>\n",
       "      <td>4.272393</td>\n",
       "      <td>4.269393</td>\n",
       "      <td>4.269289</td>\n",
       "      <td>4.265750</td>\n",
       "      <td>4.267559</td>\n",
       "      <td>4.264317</td>\n",
       "      <td>4.261687</td>\n",
       "      <td>4.272612</td>\n",
       "      <td>...</td>\n",
       "      <td>4.271577</td>\n",
       "      <td>4.261497</td>\n",
       "      <td>4.261745</td>\n",
       "      <td>4.262595</td>\n",
       "      <td>4.266280</td>\n",
       "      <td>4.264107</td>\n",
       "      <td>4.261000</td>\n",
       "      <td>4.266396</td>\n",
       "      <td>4.265758</td>\n",
       "      <td>4.264325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>4.153614</td>\n",
       "      <td>4.144321</td>\n",
       "      <td>4.154165</td>\n",
       "      <td>4.156446</td>\n",
       "      <td>4.153860</td>\n",
       "      <td>4.157824</td>\n",
       "      <td>4.155025</td>\n",
       "      <td>4.149766</td>\n",
       "      <td>4.153466</td>\n",
       "      <td>4.152328</td>\n",
       "      <td>...</td>\n",
       "      <td>4.157331</td>\n",
       "      <td>4.147135</td>\n",
       "      <td>4.150684</td>\n",
       "      <td>4.145673</td>\n",
       "      <td>4.156319</td>\n",
       "      <td>4.152633</td>\n",
       "      <td>4.151596</td>\n",
       "      <td>4.153152</td>\n",
       "      <td>4.155070</td>\n",
       "      <td>4.153667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "index                                                                          \n",
       "3       4.391864  4.390656  4.395023  4.388587  4.389089  4.393358  4.391413   \n",
       "24      4.180397  4.181886  4.192978  4.185059  4.189871  4.193472  4.185056   \n",
       "42      4.123135  4.120899  4.129018  4.123671  4.123624  4.127313  4.124439   \n",
       "49      4.432966  4.429181  4.441802  4.434208  4.440212  4.434598  4.431140   \n",
       "73      4.183240  4.178967  4.178316  4.173810  4.181118  4.178365  4.179305   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  4.140194  4.128915  4.137585  4.133186  4.141717  4.143141  4.142426   \n",
       "212365  4.142812  4.138370  4.134733  4.131379  4.137354  4.154920  4.134166   \n",
       "212394  4.169956  4.172345  4.187812  4.173820  4.183793  4.185894  4.194084   \n",
       "212399  4.266047  4.268552  4.272393  4.269393  4.269289  4.265750  4.267559   \n",
       "212402  4.153614  4.144321  4.154165  4.156446  4.153860  4.157824  4.155025   \n",
       "\n",
       "             7         8         9    ...       246       247       248  \\\n",
       "index                                 ...                                 \n",
       "3       4.386438  4.399346  4.395594  ...  4.398738  4.389634  4.393667   \n",
       "24      4.182319  4.184555  4.191511  ...  4.187167  4.184680  4.186547   \n",
       "42      4.119629  4.123821  4.130284  ...  4.121098  4.125817  4.124756   \n",
       "49      4.431800  4.440544  4.436188  ...  4.439245  4.433405  4.437764   \n",
       "73      4.186319  4.179751  4.176682  ...  4.176789  4.177713  4.175012   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  4.135183  4.147612  4.141233  ...  4.142248  4.141637  4.126824   \n",
       "212365  4.126167  4.146227  4.136079  ...  4.118249  4.145643  4.141264   \n",
       "212394  4.185398  4.180892  4.182940  ...  4.193676  4.174583  4.180787   \n",
       "212399  4.264317  4.261687  4.272612  ...  4.271577  4.261497  4.261745   \n",
       "212402  4.149766  4.153466  4.152328  ...  4.157331  4.147135  4.150684   \n",
       "\n",
       "             249       250       251       252       253       254       255  \n",
       "index                                                                         \n",
       "3       4.393713  4.393002  4.389347  4.391222  4.397897  4.394945  4.394764  \n",
       "24      4.180562  4.187067  4.186366  4.184957  4.185093  4.183947  4.185663  \n",
       "42      4.118839  4.124855  4.121467  4.118801  4.122696  4.123878  4.120490  \n",
       "49      4.436251  4.438423  4.430849  4.430588  4.434324  4.437403  4.434786  \n",
       "73      4.180591  4.178446  4.176745  4.176698  4.183792  4.179440  4.174779  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "212361  4.139755  4.135477  4.141424  4.136750  4.144142  4.141432  4.131066  \n",
       "212365  4.114353  4.123492  4.146420  4.134927  4.154398  4.127565  4.140688  \n",
       "212394  4.185509  4.178794  4.177565  4.171576  4.177408  4.183448  4.188398  \n",
       "212399  4.262595  4.266280  4.264107  4.261000  4.266396  4.265758  4.264325  \n",
       "212402  4.145673  4.156319  4.152633  4.151596  4.153152  4.155070  4.153667  \n",
       "\n",
       "[21419 rows x 256 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that indices are correct\n",
    "NLP_intermediate_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y cuts\n",
    "We are going to run two models for two target variables\n",
    "- Target variable: Average rating\n",
    "  - baseline (i.e. excluding image embeddings)\n",
    "  - including image embeddings\n",
    "- Target variable: weighted rating\n",
    "  - baseline (i.e. excluding image embeddings)\n",
    "  - including image embeddings\n",
    "\n",
    "We therefore need to create the following datsets\n",
    "- X train and X test with embeddings\n",
    "- X train and X text without embeddings\n",
    "- y train and y test using average rating\n",
    "- y train and y test using weighted rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\t Dense\t Dropout\t NLP_df_test\t NLP_df_train\t NLP_intermediate_test\t NLP_intermediate_test_df\t NLP_intermediate_train\t NLP_intermediate_train_df\t \n",
      "Pipeline\t RandomForestRegressor\t SGD\t SVR\t StandardScaler\t X\t X_columns\t X_images_test\t X_images_train\t \n",
      "X_test\t X_train\t analysis_folder\t baseline_model\t columns_to_keep\t date\t dates_columns\t epochs_hist\t folders_set_up\t \n",
      "input_folder\t input_shape\t keras\t layers\t mean_absolute_error\t mean_squared_error\t n_neurons\t np\t os\t \n",
      "output_folder\t pd\t plt\t sns\t test_indices\t time\t title_embeddings_df\t train_indices\t train_test_split\t \n",
      "tree\t y\t y_avg_r_test\t y_avg_r_train\t y_test\t y_train\t y_wr_test\t y_wr_train\t \n"
     ]
    }
   ],
   "source": [
    "%who\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model data for SVR\n",
    "X_baseline_train = pd.merge(\n",
    "    NLP_df_train,\n",
    "    X_train['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "\n",
    "X_baseline_test = pd.merge(\n",
    "    NLP_df_test,\n",
    "    X_test['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2992  tSVD2993  tSVD2994  \\\n",
       "3       0.006327  0.040408  0.035931  ... -0.012096  0.012755  0.004345   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.002294 -0.002052 -0.013243   \n",
       "42      0.042483  0.019502  0.016915  ... -0.007230 -0.005164  0.000416   \n",
       "49     -0.027805 -0.038459 -0.016390  ... -0.003206  0.011330 -0.004898   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.003473 -0.013758  0.000954   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.000571  0.001393 -0.001693   \n",
       "212365 -0.028389  0.026683  0.003763  ... -0.010071 -0.008840  0.011693   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.005601  0.000670 -0.010949   \n",
       "212399 -0.028692 -0.017410  0.020350  ...  0.005578 -0.000719 -0.004401   \n",
       "212402 -0.004967 -0.007246 -0.009244  ...  0.017672  0.005686  0.007956   \n",
       "\n",
       "        tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000    year  \n",
       "3       0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  2005.0  \n",
       "24     -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  2001.0  \n",
       "42     -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  2002.0  \n",
       "49     -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  1997.0  \n",
       "73     -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  2016.0  \n",
       "...          ...       ...       ...       ...       ...       ...     ...  \n",
       "212361 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  2009.0  \n",
       "212365  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  1998.0  \n",
       "212394 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  2015.0  \n",
       "212399  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  2000.0  \n",
       "212402 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  2003.0  \n",
       "\n",
       "[21419 rows x 3001 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21419, 3001)\n",
      "(5355, 3001)\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline_train.shape)\n",
    "print(X_baseline_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2992  tSVD2993  tSVD2994  \\\n",
       "3       0.006327  0.040408  0.035931  ... -0.012096  0.012755  0.004345   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.002294 -0.002052 -0.013243   \n",
       "42      0.042483  0.019502  0.016915  ... -0.007230 -0.005164  0.000416   \n",
       "49     -0.027805 -0.038459 -0.016390  ... -0.003206  0.011330 -0.004898   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.003473 -0.013758  0.000954   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.000571  0.001393 -0.001693   \n",
       "212365 -0.028389  0.026683  0.003763  ... -0.010071 -0.008840  0.011693   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.005601  0.000670 -0.010949   \n",
       "212399 -0.028692 -0.017410  0.020350  ...  0.005578 -0.000719 -0.004401   \n",
       "212402 -0.004967 -0.007246 -0.009244  ...  0.017672  0.005686  0.007956   \n",
       "\n",
       "        tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000    year  \n",
       "3       0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  2005.0  \n",
       "24     -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  2001.0  \n",
       "42     -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  2002.0  \n",
       "49     -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  1997.0  \n",
       "73     -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  2016.0  \n",
       "...          ...       ...       ...       ...       ...       ...     ...  \n",
       "212361 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  2009.0  \n",
       "212365  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  1998.0  \n",
       "212394 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  2015.0  \n",
       "212399  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  2000.0  \n",
       "212402 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  2003.0  \n",
       "\n",
       "[21419 rows x 3001 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With image embeddings\n",
    "X_final_train = pd.merge(\n",
    "    X_images_train,\n",
    "    X_baseline_train,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "    \n",
    "X_final_test = pd.merge(\n",
    "    X_images_test,\n",
    "    X_baseline_test,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_0', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5',\n",
       "       'image_6', 'image_7', 'image_8', 'image_9',\n",
       "       ...\n",
       "       'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996', 'tSVD2997',\n",
       "       'tSVD2998', 'tSVD2999', 'tSVD3000', 'year'],\n",
       "      dtype='object', length=3257)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model data for NN\n",
    "X_baseline_train_NN = NLP_intermediate_train_df\n",
    "X_baseline_test_NN = NLP_intermediate_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack description + publish year and images embeddings\n",
    "\n",
    "X_final_train_NN = pd.merge(\n",
    "    X_baseline_train_NN, \n",
    "    X_images_train, \n",
    "    left_index = True, \n",
    "    right_index = True)\n",
    "\n",
    "X_final_test_NN = pd.merge(\n",
    "    X_baseline_test_NN, \n",
    "    X_images_test, \n",
    "    left_index = True, \n",
    "    right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using NMF output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "X_baseline_train_NMF = pd.merge(\n",
    "    NMF_df_train,\n",
    "    X_train['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "\n",
    "X_baseline_test_NMF = pd.merge(\n",
    "    NMF_df_train,\n",
    "    X_test['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With image embeddings\n",
    "X_final_train_NMF = pd.merge(\n",
    "    X_images_train,\n",
    "    X_baseline_train_NMF,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "    \n",
    "X_final_test_NMF = pd.merge(\n",
    "    X_images_test,\n",
    "    X_baseline_test_NMF,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression & co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "\n",
    "# Random forest\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# SVR\n",
    "svr_model = SVR(kernel='rbf')  # 'rbf' for radial basis function kernel\n",
    "\n",
    "# Lightgbm\n",
    "\n",
    "\n",
    "# Define pipeline steps\n",
    "rf_pipeline = Pipeline([\n",
    "    ('rf', rf)  # Random Forest classifier\n",
    "])\n",
    "\n",
    "svr_pipeline = Pipeline([\n",
    "    ('svr', svr_model)  # Neural Network classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  \\\n",
       "Baseline Support Vector Regression  (SVR())   \n",
       "Final Support Vector Regression     (SVR())   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                   prediction   MAE   MSE  \n",
       "Baseline Support Vector Regression       None  None  None  \n",
       "Final Support Vector Regression          None  None  None  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "evaluation_metrics = pd.DataFrame({\n",
    "    #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "    'Baseline Support Vector Regression': {'model': svr_pipeline, 'X_train': X_baseline_train_NMF, 'X_test' : X_baseline_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Support Vector Regression': {'model': svr_pipeline, 'X_train': X_final_train_NMF, 'X_test' : X_final_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "}).transpose()\n",
    "\n",
    "evaluation_metrics = evaluation_metrics.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Support Vector Regression\n",
      "> Training completed. Duration: 01:52\n",
      "> Predictions completed. Duration: 28541896:09\n",
      "> Evaluation completed. Duration: 01:52\n",
      ">> Total time taken: 02:36\n",
      "\n",
      "\n",
      "Final Support Vector Regression\n",
      "> Training completed. Duration: 01:55\n",
      "> Predictions completed. Duration: 28541898:48\n",
      "> Evaluation completed. Duration: 01:55\n",
      ">> Total time taken: 02:43\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics.iterrows():\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "    # Call model\n",
    "    model = row['model']\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(row['X_train'], y_wr_train)\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate predictions\n",
    "    y_wr_pred = model.predict(row['X_test'])\n",
    "\n",
    "    # save predictions\n",
    "    row['prediction'] = y_wr_pred\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Predictions completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_wr_test, y_wr_pred)\n",
    "    mae = mean_absolute_error(y_wr_test, y_wr_pred)\n",
    "\n",
    "    # Save metrics\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\">> Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>[4.26317910221547, 4.263480602359525, 4.262913...</td>\n",
       "      <td>0.13147</td>\n",
       "      <td>0.045265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.269796558491903, 4.251500951120229, 4.26040...</td>\n",
       "      <td>0.131182</td>\n",
       "      <td>0.044787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  \\\n",
       "Baseline Support Vector Regression  (SVR())   \n",
       "Final Support Vector Regression     (SVR())   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.26317910221547, 4.263480602359525, 4.262913...   \n",
       "Final Support Vector Regression     [4.269796558491903, 4.251500951120229, 4.26040...   \n",
       "\n",
       "                                         MAE       MSE  \n",
       "Baseline Support Vector Regression   0.13147  0.045265  \n",
       "Final Support Vector Regression     0.131182  0.044787  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>None</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>None</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  \\\n",
       "Baseline Neural Network  None   \n",
       "Final Neural Network     None   \n",
       "\n",
       "                                                                   X_train  \\\n",
       "Baseline Neural Network             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                    X_test  \\\n",
       "Baseline Neural Network             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                        prediction   MAE   MSE  \n",
       "Baseline Neural Network       None  None  None  \n",
       "Final Neural Network          None  None  None  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "# evaluation_metrics_NN = pd.DataFrame({\n",
    "#     #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "#     'Baseline Neural Network': {'model': None, 'X_train': X_baseline_train_NN, 'X_test' : X_baseline_test_NN, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "#     'Final Neural Network': {'model': None, 'X_train': X_final_train_NN, 'X_test' : X_final_test_NN, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "# }).transpose()\n",
    "\n",
    "# NOTE: trying to run the NN without the dimension reduction of the NLP output\n",
    "evaluation_metrics_NN = pd.DataFrame({\n",
    "    #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "    'Baseline Neural Network': {'model': None, 'X_train': X_baseline_train_NMF, 'X_test' : X_baseline_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Neural Network': {'model': None, 'X_train': X_final_train_NMF, 'X_test' : X_final_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "}).transpose()\n",
    "\n",
    "evaluation_metrics_NN = evaluation_metrics_NN.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Neural Network\n",
      "> Input shape: 3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,537,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,537,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │        \u001b[38;5;34m15,934\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,684,349</span> (6.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,684,349\u001b[0m (6.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,684,349</span> (6.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,684,349\u001b[0m (6.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 202.8350 - mae: 6.4982 - mean_squared_error: 202.8353 - val_loss: 0.5787 - val_mae: 0.7395 - val_mean_squared_error: 0.5793\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2.6920 - mae: 1.2421 - mean_squared_error: 2.6920 - val_loss: 0.3243 - val_mae: 0.5449 - val_mean_squared_error: 0.3243\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2.1182 - mae: 1.1294 - mean_squared_error: 2.1182 - val_loss: 0.4793 - val_mae: 0.6700 - val_mean_squared_error: 0.4796\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 1.5847 - mae: 1.0014 - mean_squared_error: 1.5847 - val_loss: 0.2777 - val_mae: 0.5015 - val_mean_squared_error: 0.2775\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 1.4243 - mae: 0.9427 - mean_squared_error: 1.4243 - val_loss: 0.4353 - val_mae: 0.6370 - val_mean_squared_error: 0.4355\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 1.2291 - mae: 0.8785 - mean_squared_error: 1.2291 - val_loss: 0.2964 - val_mae: 0.5194 - val_mean_squared_error: 0.2962\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 1.0107 - mae: 0.7995 - mean_squared_error: 1.0107 - val_loss: 0.2246 - val_mae: 0.4471 - val_mean_squared_error: 0.2242\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.9341 - mae: 0.7676 - mean_squared_error: 0.9341 - val_loss: 0.1426 - val_mae: 0.3467 - val_mean_squared_error: 0.1418\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.8470 - mae: 0.7355 - mean_squared_error: 0.8470 - val_loss: 0.3518 - val_mae: 0.5692 - val_mean_squared_error: 0.3518\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.6831 - mae: 0.6547 - mean_squared_error: 0.6831 - val_loss: 0.3697 - val_mae: 0.5844 - val_mean_squared_error: 0.3697\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.6171 - mae: 0.6217 - mean_squared_error: 0.6171 - val_loss: 0.1887 - val_mae: 0.4063 - val_mean_squared_error: 0.1882\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.5650 - mae: 0.5975 - mean_squared_error: 0.5650 - val_loss: 0.3048 - val_mae: 0.5275 - val_mean_squared_error: 0.3047\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.4831 - mae: 0.5497 - mean_squared_error: 0.4831 - val_loss: 0.2731 - val_mae: 0.4973 - val_mean_squared_error: 0.2729\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.4220 - mae: 0.5150 - mean_squared_error: 0.4220 - val_loss: 0.0966 - val_mae: 0.2742 - val_mean_squared_error: 0.0955\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.3709 - mae: 0.4840 - mean_squared_error: 0.3709 - val_loss: 0.1556 - val_mae: 0.3646 - val_mean_squared_error: 0.1549\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.3035 - mae: 0.4344 - mean_squared_error: 0.3035 - val_loss: 0.1635 - val_mae: 0.3750 - val_mean_squared_error: 0.1628\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2561 - mae: 0.3975 - mean_squared_error: 0.2561 - val_loss: 0.1965 - val_mae: 0.4157 - val_mean_squared_error: 0.1960\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2058 - mae: 0.3549 - mean_squared_error: 0.2058 - val_loss: 0.0838 - val_mae: 0.2501 - val_mean_squared_error: 0.0827\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1610 - mae: 0.3148 - mean_squared_error: 0.1610 - val_loss: 0.1058 - val_mae: 0.2905 - val_mean_squared_error: 0.1048\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1393 - mae: 0.2906 - mean_squared_error: 0.1393 - val_loss: 0.1023 - val_mae: 0.2847 - val_mean_squared_error: 0.1013\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1124 - mae: 0.2579 - mean_squared_error: 0.1124 - val_loss: 0.0811 - val_mae: 0.2445 - val_mean_squared_error: 0.0799\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0921 - mae: 0.2310 - mean_squared_error: 0.0921 - val_loss: 0.0521 - val_mae: 0.1689 - val_mean_squared_error: 0.0506\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0855 - mae: 0.2102 - mean_squared_error: 0.0855 - val_loss: 0.0481 - val_mae: 0.1509 - val_mean_squared_error: 0.0465\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0627 - mae: 0.1840 - mean_squared_error: 0.0627 - val_loss: 0.0578 - val_mae: 0.1879 - val_mean_squared_error: 0.0564\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0562 - mae: 0.1681 - mean_squared_error: 0.0562 - val_loss: 0.0509 - val_mae: 0.1644 - val_mean_squared_error: 0.0494\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0477 - mae: 0.1531 - mean_squared_error: 0.0477 - val_loss: 0.0471 - val_mae: 0.1438 - val_mean_squared_error: 0.0454\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0466 - mae: 0.1463 - mean_squared_error: 0.0466 - val_loss: 0.0466 - val_mae: 0.1369 - val_mean_squared_error: 0.0449\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0459 - mae: 0.1440 - mean_squared_error: 0.0459 - val_loss: 0.0468 - val_mae: 0.1409 - val_mean_squared_error: 0.0451\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0430 - mae: 0.1379 - mean_squared_error: 0.0430 - val_loss: 0.0467 - val_mae: 0.1403 - val_mean_squared_error: 0.0450\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0431 - mae: 0.1362 - mean_squared_error: 0.0431 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0432 - mae: 0.1344 - mean_squared_error: 0.0432 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0438 - mae: 0.1342 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mae: 0.1369 - val_mean_squared_error: 0.0449\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0598 - mae: 0.1362 - mean_squared_error: 0.0598 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0438 - mae: 0.1354 - mean_squared_error: 0.0438 - val_loss: 0.0467 - val_mae: 0.1338 - val_mean_squared_error: 0.0449\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0429 - mae: 0.1342 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0420 - mae: 0.1331 - mean_squared_error: 0.0420 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0435 - mae: 0.1352 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0418 - mae: 0.1333 - mean_squared_error: 0.0418 - val_loss: 0.0466 - val_mae: 0.1377 - val_mean_squared_error: 0.0449\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0412 - mae: 0.1327 - mean_squared_error: 0.0412 - val_loss: 0.0467 - val_mae: 0.1340 - val_mean_squared_error: 0.0449\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0412 - mae: 0.1324 - mean_squared_error: 0.0412 - val_loss: 0.0466 - val_mae: 0.1368 - val_mean_squared_error: 0.0449\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0435 - mae: 0.1353 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0429 - mae: 0.1341 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0506 - mae: 0.1361 - mean_squared_error: 0.0506 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0480 - mae: 0.1358 - mean_squared_error: 0.0480 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0419 - mae: 0.1328 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0447 - mae: 0.1374 - mean_squared_error: 0.0447 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0432 - mae: 0.1352 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0426 - mae: 0.1340 - mean_squared_error: 0.0426 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0445 - mae: 0.1360 - mean_squared_error: 0.0445 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0471 - mae: 0.1371 - mean_squared_error: 0.0471 - val_loss: 0.0467 - val_mae: 0.1335 - val_mean_squared_error: 0.0449\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0449 - mae: 0.1345 - mean_squared_error: 0.0449 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0418 - mae: 0.1328 - mean_squared_error: 0.0418 - val_loss: 0.0466 - val_mae: 0.1361 - val_mean_squared_error: 0.0449\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0438 - mae: 0.1360 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0524 - mae: 0.1374 - mean_squared_error: 0.0524 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0442 - mae: 0.1364 - mean_squared_error: 0.0442 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0423 - mae: 0.1335 - mean_squared_error: 0.0423 - val_loss: 0.0467 - val_mae: 0.1338 - val_mean_squared_error: 0.0449\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0434 - mae: 0.1347 - mean_squared_error: 0.0434 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0418 - mae: 0.1335 - mean_squared_error: 0.0418 - val_loss: 0.0466 - val_mae: 0.1371 - val_mean_squared_error: 0.0449\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0434 - mae: 0.1353 - mean_squared_error: 0.0434 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0449 - mae: 0.1358 - mean_squared_error: 0.0449 - val_loss: 0.0467 - val_mae: 0.1341 - val_mean_squared_error: 0.0449\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0414 - mae: 0.1329 - mean_squared_error: 0.0414 - val_loss: 0.0466 - val_mae: 0.1367 - val_mean_squared_error: 0.0449\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0440 - mae: 0.1358 - mean_squared_error: 0.0440 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0437 - mae: 0.1346 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0439 - mae: 0.1353 - mean_squared_error: 0.0439 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.3483 - mae: 0.1393 - mean_squared_error: 0.3483 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0438 - mae: 0.1342 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0511 - mae: 0.1351 - mean_squared_error: 0.0511 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0428 - mae: 0.1339 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0430 - mae: 0.1362 - mean_squared_error: 0.0430 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0424 - mae: 0.1326 - mean_squared_error: 0.0424 - val_loss: 0.0466 - val_mae: 0.1361 - val_mean_squared_error: 0.0449\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0430 - mae: 0.1344 - mean_squared_error: 0.0430 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0447 - mae: 0.1368 - mean_squared_error: 0.0447 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0473 - mae: 0.1354 - mean_squared_error: 0.0473 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0454 - mae: 0.1375 - mean_squared_error: 0.0454 - val_loss: 0.0467 - val_mae: 0.1336 - val_mean_squared_error: 0.0449\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0448 - mae: 0.1361 - mean_squared_error: 0.0448 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0450 - mae: 0.1366 - mean_squared_error: 0.0450 - val_loss: 0.0466 - val_mae: 0.1361 - val_mean_squared_error: 0.0449\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0428 - mae: 0.1344 - mean_squared_error: 0.0428 - val_loss: 0.0467 - val_mae: 0.1341 - val_mean_squared_error: 0.0449\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0432 - mae: 0.1352 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0431 - mae: 0.1351 - mean_squared_error: 0.0431 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0424 - mae: 0.1339 - mean_squared_error: 0.0424 - val_loss: 0.0466 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0434 - mae: 0.1339 - mean_squared_error: 0.0434 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0432 - mae: 0.1352 - mean_squared_error: 0.0432 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0425 - mae: 0.1351 - mean_squared_error: 0.0425 - val_loss: 0.0467 - val_mae: 0.1338 - val_mean_squared_error: 0.0449\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0446 - mae: 0.1361 - mean_squared_error: 0.0446 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0426 - mae: 0.1339 - mean_squared_error: 0.0426 - val_loss: 0.0468 - val_mae: 0.1333 - val_mean_squared_error: 0.0450\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0415 - mae: 0.1328 - mean_squared_error: 0.0415 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0439 - mae: 0.1355 - mean_squared_error: 0.0439 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0427 - mae: 0.1341 - mean_squared_error: 0.0427 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0420 - mae: 0.1327 - mean_squared_error: 0.0420 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0422 - mae: 0.1337 - mean_squared_error: 0.0422 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0428 - mae: 0.1347 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0435 - mae: 0.1356 - mean_squared_error: 0.0435 - val_loss: 0.0467 - val_mae: 0.1338 - val_mean_squared_error: 0.0449\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0420 - mae: 0.1335 - mean_squared_error: 0.0420 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0429 - mae: 0.1351 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0442 - mae: 0.1362 - mean_squared_error: 0.0442 - val_loss: 0.0466 - val_mae: 0.1369 - val_mean_squared_error: 0.0449\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0441 - mae: 0.1362 - mean_squared_error: 0.0441 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0426 - mae: 0.1333 - mean_squared_error: 0.0426 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0422 - mae: 0.1340 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0430 - mae: 0.1345 - mean_squared_error: 0.0430 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0437 - mae: 0.1344 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1374 - val_mean_squared_error: 0.0449\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "> Model set up completed. Duration:  1: 55\n",
      "> Compilation completed. Duration:  28541898: 48\n",
      "> Training completed. Duration:  7: 37\n",
      "> Prediction completed. Duration:  28541898: 49\n",
      "> Evaluation completed. Duration:  7: 37\n",
      "Total time taken: 05:42\n",
      "\n",
      "\n",
      "Final Neural Network\n",
      "> Input shape: 3257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,668,096</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,668,096\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │        \u001b[38;5;34m15,934\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,815,421</span> (6.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,815,421\u001b[0m (6.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,815,421</span> (6.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,815,421\u001b[0m (6.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 196.8195 - mae: 6.0563 - mean_squared_error: 196.8198 - val_loss: 0.1993 - val_mae: 0.4184 - val_mean_squared_error: 0.1988\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.0704 - mae: 0.8067 - mean_squared_error: 1.0704 - val_loss: 0.2433 - val_mae: 0.4669 - val_mean_squared_error: 0.2429\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.8558 - mae: 0.7303 - mean_squared_error: 0.8558 - val_loss: 0.1778 - val_mae: 0.3926 - val_mean_squared_error: 0.1772\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.7583 - mae: 0.6861 - mean_squared_error: 0.7583 - val_loss: 0.2943 - val_mae: 0.5175 - val_mean_squared_error: 0.2941\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.7206 - mae: 0.6721 - mean_squared_error: 0.7206 - val_loss: 0.1532 - val_mae: 0.3610 - val_mean_squared_error: 0.1525\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.6405 - mae: 0.6352 - mean_squared_error: 0.6405 - val_loss: 0.3714 - val_mae: 0.5858 - val_mean_squared_error: 0.3714\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.5992 - mae: 0.6143 - mean_squared_error: 0.5992 - val_loss: 0.2569 - val_mae: 0.4810 - val_mean_squared_error: 0.2566\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.5843 - mae: 0.6055 - mean_squared_error: 0.5843 - val_loss: 0.6091 - val_mae: 0.7597 - val_mean_squared_error: 0.6096\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.5533 - mae: 0.5899 - mean_squared_error: 0.5533 - val_loss: 0.5137 - val_mae: 0.6951 - val_mean_squared_error: 0.5140\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.5036 - mae: 0.5636 - mean_squared_error: 0.5036 - val_loss: 0.6114 - val_mae: 0.7613 - val_mean_squared_error: 0.6119\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.4819 - mae: 0.5537 - mean_squared_error: 0.4819 - val_loss: 0.2912 - val_mae: 0.5147 - val_mean_squared_error: 0.2909\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.4338 - mae: 0.5234 - mean_squared_error: 0.4338 - val_loss: 0.4675 - val_mae: 0.6616 - val_mean_squared_error: 0.4677\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.4173 - mae: 0.5133 - mean_squared_error: 0.4173 - val_loss: 0.2307 - val_mae: 0.4538 - val_mean_squared_error: 0.2302\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.3716 - mae: 0.4828 - mean_squared_error: 0.3716 - val_loss: 0.2856 - val_mae: 0.5095 - val_mean_squared_error: 0.2854\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.3473 - mae: 0.4685 - mean_squared_error: 0.3473 - val_loss: 0.1968 - val_mae: 0.4160 - val_mean_squared_error: 0.1963\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.2987 - mae: 0.4325 - mean_squared_error: 0.2987 - val_loss: 0.1759 - val_mae: 0.3907 - val_mean_squared_error: 0.1752\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.2669 - mae: 0.4082 - mean_squared_error: 0.2669 - val_loss: 0.2357 - val_mae: 0.4593 - val_mean_squared_error: 0.2353\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2430 - mae: 0.3896 - mean_squared_error: 0.2430 - val_loss: 0.2920 - val_mae: 0.5156 - val_mean_squared_error: 0.2917\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.2107 - mae: 0.3613 - mean_squared_error: 0.2107 - val_loss: 0.2331 - val_mae: 0.4566 - val_mean_squared_error: 0.2327\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1932 - mae: 0.3436 - mean_squared_error: 0.1932 - val_loss: 0.1122 - val_mae: 0.3013 - val_mean_squared_error: 0.1113\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.1638 - mae: 0.3189 - mean_squared_error: 0.1638 - val_loss: 0.1746 - val_mae: 0.3893 - val_mean_squared_error: 0.1740\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1393 - mae: 0.2913 - mean_squared_error: 0.1393 - val_loss: 0.1363 - val_mae: 0.3381 - val_mean_squared_error: 0.1355\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.1189 - mae: 0.2683 - mean_squared_error: 0.1189 - val_loss: 0.0999 - val_mae: 0.2806 - val_mean_squared_error: 0.0989\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1066 - mae: 0.2518 - mean_squared_error: 0.1066 - val_loss: 0.0980 - val_mae: 0.2773 - val_mean_squared_error: 0.0970\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0935 - mae: 0.2349 - mean_squared_error: 0.0935 - val_loss: 0.0945 - val_mae: 0.2709 - val_mean_squared_error: 0.0934\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0795 - mae: 0.2138 - mean_squared_error: 0.0795 - val_loss: 0.0599 - val_mae: 0.1950 - val_mean_squared_error: 0.0586\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0714 - mae: 0.2010 - mean_squared_error: 0.0714 - val_loss: 0.0698 - val_mae: 0.2202 - val_mean_squared_error: 0.0686\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0627 - mae: 0.1856 - mean_squared_error: 0.0627 - val_loss: 0.0587 - val_mae: 0.1918 - val_mean_squared_error: 0.0574\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0591 - mae: 0.1767 - mean_squared_error: 0.0591 - val_loss: 0.0595 - val_mae: 0.1940 - val_mean_squared_error: 0.0582\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0560 - mae: 0.1692 - mean_squared_error: 0.0560 - val_loss: 0.0506 - val_mae: 0.1653 - val_mean_squared_error: 0.0492\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0513 - mae: 0.1598 - mean_squared_error: 0.0513 - val_loss: 0.0522 - val_mae: 0.1711 - val_mean_squared_error: 0.0508\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0470 - mae: 0.1494 - mean_squared_error: 0.0470 - val_loss: 0.0477 - val_mae: 0.1529 - val_mean_squared_error: 0.0462\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0449 - mae: 0.1438 - mean_squared_error: 0.0449 - val_loss: 0.0454 - val_mae: 0.1337 - val_mean_squared_error: 0.0438\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0424 - mae: 0.1378 - mean_squared_error: 0.0424 - val_loss: 0.0460 - val_mae: 0.1425 - val_mean_squared_error: 0.0444\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0450 - mae: 0.1415 - mean_squared_error: 0.0450 - val_loss: 0.0484 - val_mae: 0.1532 - val_mean_squared_error: 0.0468\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0437 - mae: 0.1416 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1368 - val_mean_squared_error: 0.0449\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0419 - mae: 0.1332 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0460 - mae: 0.1363 - mean_squared_error: 0.0460 - val_loss: 0.0467 - val_mae: 0.1336 - val_mean_squared_error: 0.0449\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0440 - mae: 0.1351 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0423 - mae: 0.1337 - mean_squared_error: 0.0423 - val_loss: 0.0466 - val_mae: 0.1362 - val_mean_squared_error: 0.0449\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0447 - mae: 0.1372 - mean_squared_error: 0.0447 - val_loss: 0.0467 - val_mae: 0.1338 - val_mean_squared_error: 0.0449\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0423 - mae: 0.1333 - mean_squared_error: 0.0423 - val_loss: 0.0466 - val_mae: 0.1379 - val_mean_squared_error: 0.0449\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0448 - mae: 0.1371 - mean_squared_error: 0.0448 - val_loss: 0.0467 - val_mae: 0.1336 - val_mean_squared_error: 0.0449\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0420 - mae: 0.1331 - mean_squared_error: 0.0420 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0411 - mae: 0.1324 - mean_squared_error: 0.0411 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0407 - mae: 0.1305 - mean_squared_error: 0.0407 - val_loss: 0.0467 - val_mae: 0.1387 - val_mean_squared_error: 0.0449\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0437 - mae: 0.1367 - mean_squared_error: 0.0437 - val_loss: 0.0468 - val_mae: 0.1328 - val_mean_squared_error: 0.0450\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0457 - mae: 0.1364 - mean_squared_error: 0.0457 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0426 - mae: 0.1336 - mean_squared_error: 0.0426 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0445 - mae: 0.1359 - mean_squared_error: 0.0445 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0428 - mae: 0.1337 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0432 - mae: 0.1339 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1360 - val_mean_squared_error: 0.0449\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0415 - mae: 0.1335 - mean_squared_error: 0.0415 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0432 - mae: 0.1356 - mean_squared_error: 0.0432 - val_loss: 0.0467 - val_mae: 0.1345 - val_mean_squared_error: 0.0449\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0435 - mae: 0.1356 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0441 - mae: 0.1353 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mae: 0.1371 - val_mean_squared_error: 0.0449\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0428 - mae: 0.1351 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0430 - mae: 0.1364 - mean_squared_error: 0.0430 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0441 - mae: 0.1364 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0426 - mae: 0.1350 - mean_squared_error: 0.0426 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0437 - mae: 0.1354 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0423 - mae: 0.1340 - mean_squared_error: 0.0423 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0428 - mae: 0.1342 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0411 - mae: 0.1323 - mean_squared_error: 0.0411 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0443 - mae: 0.1366 - mean_squared_error: 0.0443 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0414 - mae: 0.1326 - mean_squared_error: 0.0414 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0441 - mae: 0.1350 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mae: 0.1364 - val_mean_squared_error: 0.0449\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0463 - mae: 0.1369 - mean_squared_error: 0.0463 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0422 - mae: 0.1331 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0443 - mae: 0.1364 - mean_squared_error: 0.0443 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0420 - mae: 0.1340 - mean_squared_error: 0.0420 - val_loss: 0.0466 - val_mae: 0.1366 - val_mean_squared_error: 0.0449\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0429 - mae: 0.1339 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0459 - mae: 0.1369 - mean_squared_error: 0.0459 - val_loss: 0.0466 - val_mae: 0.1375 - val_mean_squared_error: 0.0449\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0430 - mae: 0.1349 - mean_squared_error: 0.0430 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0447 - mae: 0.1376 - mean_squared_error: 0.0447 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0419 - mae: 0.1336 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0410 - mae: 0.1330 - mean_squared_error: 0.0410 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0429 - mae: 0.1344 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0429 - mae: 0.1340 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0441 - mae: 0.1362 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0476 - mae: 0.1378 - mean_squared_error: 0.0476 - val_loss: 0.0468 - val_mae: 0.1332 - val_mean_squared_error: 0.0450\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0417 - mae: 0.1320 - mean_squared_error: 0.0417 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0426 - mae: 0.1345 - mean_squared_error: 0.0426 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0426 - mae: 0.1345 - mean_squared_error: 0.0426 - val_loss: 0.0466 - val_mae: 0.1370 - val_mean_squared_error: 0.0449\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0437 - mae: 0.1354 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0437 - mae: 0.1352 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0419 - mae: 0.1339 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0429 - mae: 0.1353 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0429 - mae: 0.1353 - mean_squared_error: 0.0429 - val_loss: 0.0467 - val_mae: 0.1341 - val_mean_squared_error: 0.0449\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0424 - mae: 0.1335 - mean_squared_error: 0.0424 - val_loss: 0.0466 - val_mae: 0.1364 - val_mean_squared_error: 0.0449\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0436 - mae: 0.1367 - mean_squared_error: 0.0436 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0436 - mae: 0.1349 - mean_squared_error: 0.0436 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0421 - mae: 0.1334 - mean_squared_error: 0.0421 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0436 - mae: 0.1355 - mean_squared_error: 0.0436 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0440 - mae: 0.1353 - mean_squared_error: 0.0440 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0441 - mae: 0.1354 - mean_squared_error: 0.0441 - val_loss: 0.0467 - val_mae: 0.1338 - val_mean_squared_error: 0.0449\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0422 - mae: 0.1337 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0421 - mae: 0.1338 - mean_squared_error: 0.0421 - val_loss: 0.0467 - val_mae: 0.1341 - val_mean_squared_error: 0.0449\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0428 - mae: 0.1336 - mean_squared_error: 0.0428 - val_loss: 0.0467 - val_mae: 0.1340 - val_mean_squared_error: 0.0449\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0465 - mae: 0.1373 - mean_squared_error: 0.0465 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "> Model set up completed. Duration:  1: 55\n",
      "> Compilation completed. Duration:  28541904: 31\n",
      "> Training completed. Duration:  7: 56\n",
      "> Prediction completed. Duration:  28541904: 31\n",
      "> Evaluation completed. Duration:  7: 56\n",
      "Total time taken: 06:01\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run model for different moodels\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics_NN.iterrows():\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "\n",
    "    input_shape = row['X_train'].shape[1]\n",
    "    print(f\"> Input shape: {input_shape}\")\n",
    "\n",
    "    # neurons number\n",
    "    n_neurons = 512\n",
    "\n",
    "### define a model\n",
    "    final_model = keras.Sequential()\n",
    "\n",
    "    # Add input layer\n",
    "    final_model.add(layers.Dense(\n",
    "                n_neurons, # number of neurons\n",
    "                input_dim = input_shape, # number of inputs \n",
    "                activation = 'relu' # activation faunction\n",
    "                ))\n",
    "\n",
    "    # Hidden - Layers\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.3, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "    final_model.add(layers.Dense(\n",
    "        256, \n",
    "        activation = \"relu\"))\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.2, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "    final_model.add(layers.Dense(\n",
    "        62, \n",
    "        activation = \"relu\"))\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.2, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "\n",
    "    # Final layer\n",
    "    final_model.add(layers.Dense(\n",
    "        1, \n",
    "        activation = 'linear'))\n",
    "\n",
    "    final_model.summary()\n",
    "\n",
    "    # Add model to table\n",
    "    row['model'] = final_model\n",
    "    \n",
    "### Compile the model\n",
    "    final_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mean_squared_error'], \n",
    "    metrics = ['mae', 'mean_squared_error']\n",
    "    )\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "\n",
    "### Train the model\n",
    "    epochs_hist = final_model.fit(\n",
    "    row['X_train'], # input\n",
    "    y_wr_train, # output\n",
    "    epochs=100, # number of iterations\n",
    "    batch_size=50, # number of observations taken to train the data - 1030 obs/50 -> there are 17 groups (observations are taken once for epoch) so model is trained 17 times in each epoch\n",
    "    verbose=1,\n",
    "    validation_data = (row['X_test'], y_wr_test),\n",
    "    shuffle = True\n",
    "    #validation_split=0.2,    \n",
    "    )\n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "\n",
    "\n",
    "# ### Predictions\n",
    "    y_pred = final_model.predict(row['X_test'])\n",
    "    # Store predictions\n",
    "    row['prediction'] = y_pred\n",
    "    # Time elapsed\n",
    "    elapsed_time_4 = time.time() - elapsed_time_3\n",
    "\n",
    "\n",
    "# ### Evaluation\n",
    "    mse = mean_squared_error(y_pred, y_wr_test)\n",
    "    mae = mean_absolute_error(y_pred, y_wr_test)\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_5 = time.time() - elapsed_time_4\n",
    "\n",
    "\n",
    "    # Timings\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Model set up completed. Duration: {minutes: 02d}:{seconds: 02d}\")\n",
    "\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Compilation completed. Duration: {minutes: 02d}:{seconds: 02d}\")\n",
    "\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    minutes = int(elapsed_time_4 // 60)\n",
    "    seconds = int(elapsed_time_4 % 60)\n",
    "    print(f\"> Prediction completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    minutes = int(elapsed_time_5 // 60)\n",
    "    seconds = int(elapsed_time_5 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\"Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_1, built=True&gt;</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>[[4.238551], [4.238551], [4.238551], [4.238551...</td>\n",
       "      <td>0.137372</td>\n",
       "      <td>0.044869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_2, built=True&gt;</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[[4.246907], [4.246907], [4.246907], [4.246907...</td>\n",
       "      <td>0.134964</td>\n",
       "      <td>0.044868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              model  \\\n",
       "Baseline Neural Network  <Sequential name=sequential_1, built=True>   \n",
       "Final Neural Network     <Sequential name=sequential_2, built=True>   \n",
       "\n",
       "                                                                   X_train  \\\n",
       "Baseline Neural Network             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                    X_test  \\\n",
       "Baseline Neural Network             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                prediction  \\\n",
       "Baseline Neural Network  [[4.238551], [4.238551], [4.238551], [4.238551...   \n",
       "Final Neural Network     [[4.246907], [4.246907], [4.246907], [4.246907...   \n",
       "\n",
       "                              MAE       MSE  \n",
       "Baseline Neural Network  0.137372  0.044869  \n",
       "Final Neural Network     0.134964  0.044868  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_all = pd.concat([evaluation_metrics, evaluation_metrics_NN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>[4.26317910221547, 4.263480602359525, 4.262913...</td>\n",
       "      <td>0.13147</td>\n",
       "      <td>0.045265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.269796558491903, 4.251500951120229, 4.26040...</td>\n",
       "      <td>0.131182</td>\n",
       "      <td>0.044787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_1, built=True&gt;</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>[[4.238551], [4.238551], [4.238551], [4.238551...</td>\n",
       "      <td>0.137372</td>\n",
       "      <td>0.044869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_2, built=True&gt;</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[[4.246907], [4.246907], [4.246907], [4.246907...</td>\n",
       "      <td>0.134964</td>\n",
       "      <td>0.044868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         model  \\\n",
       "Baseline Support Vector Regression                                     (SVR())   \n",
       "Final Support Vector Regression                                        (SVR())   \n",
       "Baseline Neural Network             <Sequential name=sequential_1, built=True>   \n",
       "Final Neural Network                <Sequential name=sequential_2, built=True>   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                        tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                        tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.26317910221547, 4.263480602359525, 4.262913...   \n",
       "Final Support Vector Regression     [4.269796558491903, 4.251500951120229, 4.26040...   \n",
       "Baseline Neural Network             [[4.238551], [4.238551], [4.238551], [4.238551...   \n",
       "Final Neural Network                [[4.246907], [4.246907], [4.246907], [4.246907...   \n",
       "\n",
       "                                         MAE       MSE  \n",
       "Baseline Support Vector Regression   0.13147  0.045265  \n",
       "Final Support Vector Regression     0.131182  0.044787  \n",
       "Baseline Neural Network             0.137372  0.044869  \n",
       "Final Neural Network                0.134964  0.044868  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise NN\n",
    "\n",
    "# # Plotting Loss And Mean Square Error For both Training And Test Sets\n",
    "# plt.plot(epochs_hist.history['mse'])\n",
    "# plt.plot(epochs_hist.history['val_mse'])\n",
    "# plt.title('MSE')\n",
    "# plt.ylabel('mae')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(output_folder, '{i} mse chart.png'))\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(epochs_hist.history['loss'])\n",
    "# plt.plot(epochs_hist.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.savefig('4.png')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(output_folder, '{i} summary chart.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
