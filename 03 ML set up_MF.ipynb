{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up folders\n",
    "from EDA_functions import folders_set_up\n",
    "import os\n",
    "\n",
    "# Work with datarames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charts\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# X, Y preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# Neural Network\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Evaluate models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Time\n",
    "import time\n",
    "\n",
    "#from scipy.sparse import spmatrixc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM\n",
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders\n",
    "Run the code below if you have the following structure:\n",
    "- Group-project: GitHub folder\n",
    "- 01 Input\n",
    "- 02 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_folder, input_folder, output_folder = folders_set_up.generate_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'description', 'authors', 'image', 'previewLink',\n",
       "       'publisher', 'infoLink', 'categories', 'reviews number',\n",
       "       'average rating', 'median rating', 'min review date', 'max review date',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Title-level dataset with embeddings\n",
    "title_embeddings_df = pd.read_pickle(\n",
    "    os.path.join(output_folder, 'English_fiction_pre_PCA_3_with_av_pool_embeddings')\n",
    ")\n",
    "\n",
    "title_embeddings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                     int64\n",
       "Title                    object\n",
       "description              object\n",
       "authors                  object\n",
       "image                    object\n",
       "previewLink              object\n",
       "publisher                object\n",
       "infoLink                 object\n",
       "categories               object\n",
       "reviews number            int64\n",
       "average rating          float64\n",
       "median rating           float64\n",
       "min review date          object\n",
       "max review date          object\n",
       "weighted rating         float64\n",
       "date                     object\n",
       "year                    float64\n",
       "description_language     object\n",
       "Embedding                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_columns = ['min review date', 'max review date', 'date']\n",
    "\n",
    "for date in dates_columns:\n",
    "    # get date from strings with time\n",
    "    title_embeddings_df[date] = title_embeddings_df[date].str.split().str[0]\n",
    "    # convert in datetime\n",
    "    title_embeddings_df[date] = pd.to_datetime(title_embeddings_df[date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min review date    0\n",
       "max review date    0\n",
       "date               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df[dates_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8228249664151188\n",
      "4.886083503427672\n"
     ]
    }
   ],
   "source": [
    "# what is the max and minimu of the ratings?\n",
    "print(title_embeddings_df['weighted rating'].min())\n",
    "print(title_embeddings_df['weighted rating'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we work on a subset of data for now to make the ML run faster\n",
    "#title_embeddings_df = title_embeddings_df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image embeddings\n",
    "These need may need to be transformed in from arrays to columns if the model we use is not NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>median rating</th>\n",
       "      <th>min review date</th>\n",
       "      <th>max review date</th>\n",
       "      <th>weighted rating</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>description_language</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>index_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-02-14</td>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>3.938400</td>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.5179044, -0.7533603, -1.1291503, -0.4418345...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>4.306145</td>\n",
       "      <td>2001-03-06</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.706188, -0.4773652, -0.17887038, 0.07989502...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-22</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>4.256189</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.294651, -0.24902871, -0.6188333, -0.7722471...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-04-16</td>\n",
       "      <td>2000-05-14</td>\n",
       "      <td>4.336313</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.37794992, -0.6178984, -0.81393754, -0.66795...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-08-07</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>4.701517</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.34032565, -2.1706967, -0.21470371, -0.10447...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26769</th>\n",
       "      <td>212361</td>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>4.137453</td>\n",
       "      <td>2009-03-17</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[1.1648176, 0.56768346, -0.22511423, -0.185316...</td>\n",
       "      <td>212361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26770</th>\n",
       "      <td>212365</td>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-05-17</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>4.466716</td>\n",
       "      <td>1998-01-27</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.023786038, -1.9050528, -0.38564998, 0.14921...</td>\n",
       "      <td>212365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26771</th>\n",
       "      <td>212394</td>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>4.260690</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.2700834, -0.11750376, -2.0253444, -1.039558...</td>\n",
       "      <td>212394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26772</th>\n",
       "      <td>212399</td>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-07-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>4.504800</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.6904726, -0.96442795, 0.093034565, -1.69420...</td>\n",
       "      <td>212399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773</th>\n",
       "      <td>212402</td>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2002-11-11</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>3.989408</td>\n",
       "      <td>2003-08-12</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.47349918, -0.8046489, -0.88566315, -0.04958...</td>\n",
       "      <td>212402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26774 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                              Title  \\\n",
       "0           3                      Whispers of the Wicked Saints   \n",
       "1          24           The Forbidden Stories of Marta Veneranda   \n",
       "2          42                            Tess and the Highlander   \n",
       "3          49  Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "4          73                 Night World: Daughters Of Darkness   \n",
       "...       ...                                                ...   \n",
       "26769  212361                                       Calder Pride   \n",
       "26770  212365                                      The Road Back   \n",
       "26771  212394                                       Final things   \n",
       "26772  212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "26773  212402                                  The Autograph Man   \n",
       "\n",
       "                                             description  \\\n",
       "0      Julia Thomas finds her life spinning out of co...   \n",
       "1      Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "2      In 1543, on a windswept isle off of Scotland, ...   \n",
       "3      Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "4      \"There’s something strange about the new girls...   \n",
       "...                                                  ...   \n",
       "26769  The Long-Awaited Addition to the Beloved Calde...   \n",
       "26770  The sequel to the masterpiece All Quiet on the...   \n",
       "26771  Grace's father believes in science and builds ...   \n",
       "26772  During a school trip to Ellis Island, Dominick...   \n",
       "26773  Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                        authors  \\\n",
       "0           ['Veronica Haddon']   \n",
       "1       ['Sonia Rivera-Valdes']   \n",
       "2            ['May Mcgoldrick']   \n",
       "3        ['Elizabeth Sinclair']   \n",
       "4                ['L.J. Smith']   \n",
       "...                         ...   \n",
       "26769          ['Janet Dailey']   \n",
       "26770  ['Erich Maria Remarque']   \n",
       "26771          ['Jenny Offill']   \n",
       "26772       ['Elvira Woodruff']   \n",
       "26773           ['Zadie Smith']   \n",
       "\n",
       "                                                   image  \\\n",
       "0      http://books.google.com/books/content?id=aRSIg...   \n",
       "1      http://books.google.com/books/content?id=A7aYb...   \n",
       "2      http://books.google.com/books/content?id=VmCRS...   \n",
       "3      http://books.google.com/books/content?id=Z6uzJ...   \n",
       "4      http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                  ...   \n",
       "26769  http://books.google.com/books/content?id=nlsgd...   \n",
       "26770  http://books.google.com/books/content?id=obZdA...   \n",
       "26771  http://books.google.com/books/content?id=UbSFB...   \n",
       "26772  http://books.google.com/books/content?id=J7M-N...   \n",
       "26773  http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                             previewLink  \\\n",
       "0      http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "1      http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "2      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "3      http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "4      http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                  ...   \n",
       "26769  http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "26770  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "26771  http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "26772  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "26773  http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                               publisher  \\\n",
       "0                                              iUniverse   \n",
       "1                                    Seven Stories Press   \n",
       "2                                         Harper Collins   \n",
       "3      Harlequin Treasury-Harlequin American Romance 90s   \n",
       "4                                     Simon and Schuster   \n",
       "...                                                  ...   \n",
       "26769                                     Harper Collins   \n",
       "26770                      Random House Trade Paperbacks   \n",
       "26771                                            Vintage   \n",
       "26772                              Scholastic Paperbacks   \n",
       "26773                                            Vintage   \n",
       "\n",
       "                                                infoLink  \\\n",
       "0      http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "1      http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "2      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "3      http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "4      http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                  ...   \n",
       "26769  https://play.google.com/store/books/details?id...   \n",
       "26770  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "26771  https://play.google.com/store/books/details?id...   \n",
       "26772  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "26773  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                 categories  reviews number  average rating  median rating  \\\n",
       "0               ['fiction']              32        3.718750            5.0   \n",
       "1               ['fiction']               1        5.000000            5.0   \n",
       "2      ['juvenile fiction']              17        4.235294            5.0   \n",
       "3               ['fiction']               2        5.000000            5.0   \n",
       "4      ['juvenile fiction']             134        4.768657            5.0   \n",
       "...                     ...             ...             ...            ...   \n",
       "26769           ['fiction']              28        4.035714            5.0   \n",
       "26770           ['fiction']              17        4.705882            5.0   \n",
       "26771           ['fiction']               1        4.000000            4.0   \n",
       "26772  ['juvenile fiction']              28        4.678571            5.0   \n",
       "26773           ['fiction']               4        2.500000            2.5   \n",
       "\n",
       "      min review date max review date  weighted rating       date    year  \\\n",
       "0          2005-02-14      2006-07-01         3.938400 2005-02-01  2005.0   \n",
       "1          2005-01-24      2005-01-24         4.306145 2001-03-06  2001.0   \n",
       "2          2002-10-22      2011-05-25         4.256189 2002-11-01  2002.0   \n",
       "3          1998-04-16      2000-05-14         4.336313 1997-01-01  1997.0   \n",
       "4          1996-08-07      2012-09-18         4.701517 2016-12-06  2016.0   \n",
       "...               ...             ...              ...        ...     ...   \n",
       "26769      1999-09-30      2012-04-04         4.137453 2009-03-17  2009.0   \n",
       "26770      1997-05-17      2012-01-23         4.466716 1998-01-27  1998.0   \n",
       "26771      2012-01-26      2012-01-26         4.260690 2015-03-17  2015.0   \n",
       "26772      1998-07-10      2011-12-31         4.504800 2000-06-01  2000.0   \n",
       "26773      2002-11-11      2005-11-14         3.989408 2003-08-12  2003.0   \n",
       "\n",
       "      description_language                                          Embedding  \\\n",
       "0                  English  [0.5179044, -0.7533603, -1.1291503, -0.4418345...   \n",
       "1                  English  [0.706188, -0.4773652, -0.17887038, 0.07989502...   \n",
       "2                  English  [2.294651, -0.24902871, -0.6188333, -0.7722471...   \n",
       "3                  English  [0.37794992, -0.6178984, -0.81393754, -0.66795...   \n",
       "4                  English  [0.34032565, -2.1706967, -0.21470371, -0.10447...   \n",
       "...                    ...                                                ...   \n",
       "26769              English  [1.1648176, 0.56768346, -0.22511423, -0.185316...   \n",
       "26770              English  [0.023786038, -1.9050528, -0.38564998, 0.14921...   \n",
       "26771              English  [2.2700834, -0.11750376, -2.0253444, -1.039558...   \n",
       "26772              English  [2.6904726, -0.96442795, 0.093034565, -1.69420...   \n",
       "26773              English  [0.47349918, -0.8046489, -0.88566315, -0.04958...   \n",
       "\n",
       "       index_key  \n",
       "0              3  \n",
       "1             24  \n",
       "2             42  \n",
       "3             49  \n",
       "4             73  \n",
       "...          ...  \n",
       "26769     212361  \n",
       "26770     212365  \n",
       "26771     212394  \n",
       "26772     212399  \n",
       "26773     212402  \n",
       "\n",
       "[26774 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df['index_key'] = title_embeddings_df['index']\n",
    "title_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings_df = title_embeddings_df.set_index('index_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>median rating</th>\n",
       "      <th>min review date</th>\n",
       "      <th>max review date</th>\n",
       "      <th>weighted rating</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>description_language</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-02-14</td>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>3.938400</td>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.5179044, -0.7533603, -1.1291503, -0.4418345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>4.306145</td>\n",
       "      <td>2001-03-06</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.706188, -0.4773652, -0.17887038, 0.07989502...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-22</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>4.256189</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.294651, -0.24902871, -0.6188333, -0.7722471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-04-16</td>\n",
       "      <td>2000-05-14</td>\n",
       "      <td>4.336313</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.37794992, -0.6178984, -0.81393754, -0.66795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-08-07</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>4.701517</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.34032565, -2.1706967, -0.21470371, -0.10447...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>212361</td>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>4.137453</td>\n",
       "      <td>2009-03-17</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[1.1648176, 0.56768346, -0.22511423, -0.185316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>212365</td>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-05-17</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>4.466716</td>\n",
       "      <td>1998-01-27</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.023786038, -1.9050528, -0.38564998, 0.14921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>212394</td>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>4.260690</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.2700834, -0.11750376, -2.0253444, -1.039558...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>212399</td>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-07-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>4.504800</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.6904726, -0.96442795, 0.093034565, -1.69420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>212402</td>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2002-11-11</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>3.989408</td>\n",
       "      <td>2003-08-12</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.47349918, -0.8046489, -0.88566315, -0.04958...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26774 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index                                              Title  \\\n",
       "index_key                                                              \n",
       "3               3                      Whispers of the Wicked Saints   \n",
       "24             24           The Forbidden Stories of Marta Veneranda   \n",
       "42             42                            Tess and the Highlander   \n",
       "49             49  Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "73             73                 Night World: Daughters Of Darkness   \n",
       "...           ...                                                ...   \n",
       "212361     212361                                       Calder Pride   \n",
       "212365     212365                                      The Road Back   \n",
       "212394     212394                                       Final things   \n",
       "212399     212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "212402     212402                                  The Autograph Man   \n",
       "\n",
       "                                                 description  \\\n",
       "index_key                                                      \n",
       "3          Julia Thomas finds her life spinning out of co...   \n",
       "24         Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "42         In 1543, on a windswept isle off of Scotland, ...   \n",
       "49         Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "73         \"There’s something strange about the new girls...   \n",
       "...                                                      ...   \n",
       "212361     The Long-Awaited Addition to the Beloved Calde...   \n",
       "212365     The sequel to the masterpiece All Quiet on the...   \n",
       "212394     Grace's father believes in science and builds ...   \n",
       "212399     During a school trip to Ellis Island, Dominick...   \n",
       "212402     Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                            authors  \\\n",
       "index_key                             \n",
       "3               ['Veronica Haddon']   \n",
       "24          ['Sonia Rivera-Valdes']   \n",
       "42               ['May Mcgoldrick']   \n",
       "49           ['Elizabeth Sinclair']   \n",
       "73                   ['L.J. Smith']   \n",
       "...                             ...   \n",
       "212361             ['Janet Dailey']   \n",
       "212365     ['Erich Maria Remarque']   \n",
       "212394             ['Jenny Offill']   \n",
       "212399          ['Elvira Woodruff']   \n",
       "212402              ['Zadie Smith']   \n",
       "\n",
       "                                                       image  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.com/books/content?id=aRSIg...   \n",
       "24         http://books.google.com/books/content?id=A7aYb...   \n",
       "42         http://books.google.com/books/content?id=VmCRS...   \n",
       "49         http://books.google.com/books/content?id=Z6uzJ...   \n",
       "73         http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                      ...   \n",
       "212361     http://books.google.com/books/content?id=nlsgd...   \n",
       "212365     http://books.google.com/books/content?id=obZdA...   \n",
       "212394     http://books.google.com/books/content?id=UbSFB...   \n",
       "212399     http://books.google.com/books/content?id=J7M-N...   \n",
       "212402     http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                                 previewLink  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24         http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "42         http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49         http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "73         http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                      ...   \n",
       "212361     http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "212365     http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394     http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "212399     http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402     http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                                   publisher  \\\n",
       "index_key                                                      \n",
       "3                                                  iUniverse   \n",
       "24                                       Seven Stories Press   \n",
       "42                                            Harper Collins   \n",
       "49         Harlequin Treasury-Harlequin American Romance 90s   \n",
       "73                                        Simon and Schuster   \n",
       "...                                                      ...   \n",
       "212361                                        Harper Collins   \n",
       "212365                         Random House Trade Paperbacks   \n",
       "212394                                               Vintage   \n",
       "212399                                 Scholastic Paperbacks   \n",
       "212402                                               Vintage   \n",
       "\n",
       "                                                    infoLink  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24         http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "42         http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49         http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "73         http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                      ...   \n",
       "212361     https://play.google.com/store/books/details?id...   \n",
       "212365     http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394     https://play.google.com/store/books/details?id...   \n",
       "212399     http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402     https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                     categories  reviews number  average rating  \\\n",
       "index_key                                                         \n",
       "3                   ['fiction']              32        3.718750   \n",
       "24                  ['fiction']               1        5.000000   \n",
       "42         ['juvenile fiction']              17        4.235294   \n",
       "49                  ['fiction']               2        5.000000   \n",
       "73         ['juvenile fiction']             134        4.768657   \n",
       "...                         ...             ...             ...   \n",
       "212361              ['fiction']              28        4.035714   \n",
       "212365              ['fiction']              17        4.705882   \n",
       "212394              ['fiction']               1        4.000000   \n",
       "212399     ['juvenile fiction']              28        4.678571   \n",
       "212402              ['fiction']               4        2.500000   \n",
       "\n",
       "           median rating min review date max review date  weighted rating  \\\n",
       "index_key                                                                   \n",
       "3                    5.0      2005-02-14      2006-07-01         3.938400   \n",
       "24                   5.0      2005-01-24      2005-01-24         4.306145   \n",
       "42                   5.0      2002-10-22      2011-05-25         4.256189   \n",
       "49                   5.0      1998-04-16      2000-05-14         4.336313   \n",
       "73                   5.0      1996-08-07      2012-09-18         4.701517   \n",
       "...                  ...             ...             ...              ...   \n",
       "212361               5.0      1999-09-30      2012-04-04         4.137453   \n",
       "212365               5.0      1997-05-17      2012-01-23         4.466716   \n",
       "212394               4.0      2012-01-26      2012-01-26         4.260690   \n",
       "212399               5.0      1998-07-10      2011-12-31         4.504800   \n",
       "212402               2.5      2002-11-11      2005-11-14         3.989408   \n",
       "\n",
       "                date    year description_language  \\\n",
       "index_key                                           \n",
       "3         2005-02-01  2005.0              English   \n",
       "24        2001-03-06  2001.0              English   \n",
       "42        2002-11-01  2002.0              English   \n",
       "49        1997-01-01  1997.0              English   \n",
       "73        2016-12-06  2016.0              English   \n",
       "...              ...     ...                  ...   \n",
       "212361    2009-03-17  2009.0              English   \n",
       "212365    1998-01-27  1998.0              English   \n",
       "212394    2015-03-17  2015.0              English   \n",
       "212399    2000-06-01  2000.0              English   \n",
       "212402    2003-08-12  2003.0              English   \n",
       "\n",
       "                                                   Embedding  \n",
       "index_key                                                     \n",
       "3          [0.5179044, -0.7533603, -1.1291503, -0.4418345...  \n",
       "24         [0.706188, -0.4773652, -0.17887038, 0.07989502...  \n",
       "42         [2.294651, -0.24902871, -0.6188333, -0.7722471...  \n",
       "49         [0.37794992, -0.6178984, -0.81393754, -0.66795...  \n",
       "73         [0.34032565, -2.1706967, -0.21470371, -0.10447...  \n",
       "...                                                      ...  \n",
       "212361     [1.1648176, 0.56768346, -0.22511423, -0.185316...  \n",
       "212365     [0.023786038, -1.9050528, -0.38564998, 0.14921...  \n",
       "212394     [2.2700834, -0.11750376, -2.0253444, -1.039558...  \n",
       "212399     [2.6904726, -0.96442795, 0.093034565, -1.69420...  \n",
       "212402     [0.47349918, -0.8046489, -0.88566315, -0.04958...  \n",
       "\n",
       "[26774 rows x 19 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "Most of the cleaning is done in '02 Consolidate books dataset':\n",
    "- English description\n",
    "- category containing the word 'fiction'\n",
    "- non-missing date\n",
    "- non-missing author\n",
    "- non-missing publisher\n",
    "- non-missing cover image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'description', 'authors', 'image', 'previewLink',\n",
       "       'publisher', 'infoLink', 'categories', 'reviews number',\n",
       "       'average rating', 'median rating', 'min review date', 'max review date',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y including all X features and all all teh possible target variables\n",
    "# NOTE: we will have to add the description PCA in X_features\n",
    "X_columns = ['year', 'Embedding', 'index', 'Title']\n",
    "\n",
    "X = title_embeddings_df[X_columns]\n",
    "y = title_embeddings_df[['average rating', 'weighted rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'Embedding', 'index', 'Title'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['average rating', 'weighted rating'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "\n",
    "# Need to create train test split for different combinations of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size= 0.2, \n",
    "    random_state= 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store indices of train test split for the NLP of description\n",
    "train_indices = X_train[['Title', 'index']]\n",
    "test_indices = X_test[['Title', 'index']]\n",
    "\n",
    "train_indices.to_csv(\n",
    "    os.path.join(output_folder, 'train_indices.csv')\n",
    ")\n",
    "\n",
    "\n",
    "test_indices.to_csv(\n",
    "    os.path.join(output_folder, 'test_indices.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y train with average rating\n",
    "y_avg_r_train = y_train['average rating']\n",
    "y_avg_r_test = y_test['average rating']\n",
    "\n",
    "# Y train with weighted rating\n",
    "y_wr_train = y_train['weighted rating']\n",
    "y_wr_test = y_test['weighted rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings X\n",
    "Transform the arrays into columns so that they can feed into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images_train = X_train['Embedding'].apply(pd.Series)\n",
    "X_images_test = X_test['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "X_images_train = X_images_train.add_prefix('image_')\n",
    "X_images_test = X_images_test.add_prefix('image_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_0</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "      <th>image_6</th>\n",
       "      <th>image_7</th>\n",
       "      <th>image_8</th>\n",
       "      <th>image_9</th>\n",
       "      <th>...</th>\n",
       "      <th>image_246</th>\n",
       "      <th>image_247</th>\n",
       "      <th>image_248</th>\n",
       "      <th>image_249</th>\n",
       "      <th>image_250</th>\n",
       "      <th>image_251</th>\n",
       "      <th>image_252</th>\n",
       "      <th>image_253</th>\n",
       "      <th>image_254</th>\n",
       "      <th>image_255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19757</th>\n",
       "      <td>1.310540</td>\n",
       "      <td>0.253238</td>\n",
       "      <td>-0.208362</td>\n",
       "      <td>-0.584103</td>\n",
       "      <td>-0.794551</td>\n",
       "      <td>-1.803357</td>\n",
       "      <td>-2.700018</td>\n",
       "      <td>-0.848385</td>\n",
       "      <td>0.949902</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475167</td>\n",
       "      <td>-0.116055</td>\n",
       "      <td>0.735540</td>\n",
       "      <td>-2.354414</td>\n",
       "      <td>0.956939</td>\n",
       "      <td>-1.065875</td>\n",
       "      <td>-0.428229</td>\n",
       "      <td>-0.285047</td>\n",
       "      <td>1.098027</td>\n",
       "      <td>-1.029737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111405</th>\n",
       "      <td>1.078856</td>\n",
       "      <td>-0.691140</td>\n",
       "      <td>-0.908770</td>\n",
       "      <td>-0.527087</td>\n",
       "      <td>-1.044688</td>\n",
       "      <td>-0.904328</td>\n",
       "      <td>0.210946</td>\n",
       "      <td>-1.238919</td>\n",
       "      <td>2.290273</td>\n",
       "      <td>-0.155667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833435</td>\n",
       "      <td>-1.053398</td>\n",
       "      <td>-2.031961</td>\n",
       "      <td>-2.716383</td>\n",
       "      <td>0.817275</td>\n",
       "      <td>-0.434370</td>\n",
       "      <td>-1.456125</td>\n",
       "      <td>0.112614</td>\n",
       "      <td>0.199815</td>\n",
       "      <td>-2.946273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12269</th>\n",
       "      <td>0.350093</td>\n",
       "      <td>-0.135313</td>\n",
       "      <td>0.512653</td>\n",
       "      <td>0.466054</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>0.014968</td>\n",
       "      <td>-2.224684</td>\n",
       "      <td>-0.740723</td>\n",
       "      <td>0.951188</td>\n",
       "      <td>0.978717</td>\n",
       "      <td>...</td>\n",
       "      <td>1.401945</td>\n",
       "      <td>-0.270900</td>\n",
       "      <td>-1.967142</td>\n",
       "      <td>-0.814089</td>\n",
       "      <td>0.170715</td>\n",
       "      <td>0.335253</td>\n",
       "      <td>-0.030882</td>\n",
       "      <td>-0.557203</td>\n",
       "      <td>0.037506</td>\n",
       "      <td>-1.951925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186303</th>\n",
       "      <td>4.560193</td>\n",
       "      <td>0.550376</td>\n",
       "      <td>-0.331547</td>\n",
       "      <td>-2.215812</td>\n",
       "      <td>0.023501</td>\n",
       "      <td>-0.370375</td>\n",
       "      <td>-0.838666</td>\n",
       "      <td>-0.905349</td>\n",
       "      <td>2.655245</td>\n",
       "      <td>0.461321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073267</td>\n",
       "      <td>0.354717</td>\n",
       "      <td>-1.745163</td>\n",
       "      <td>-2.610591</td>\n",
       "      <td>-0.239567</td>\n",
       "      <td>-1.714204</td>\n",
       "      <td>-0.914772</td>\n",
       "      <td>-0.354720</td>\n",
       "      <td>0.098914</td>\n",
       "      <td>-3.272672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134045</th>\n",
       "      <td>2.236698</td>\n",
       "      <td>-0.498289</td>\n",
       "      <td>-1.946595</td>\n",
       "      <td>-0.618339</td>\n",
       "      <td>-2.150359</td>\n",
       "      <td>1.257699</td>\n",
       "      <td>-1.547900</td>\n",
       "      <td>-1.492419</td>\n",
       "      <td>2.133226</td>\n",
       "      <td>1.513557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192120</td>\n",
       "      <td>-0.731990</td>\n",
       "      <td>-3.023546</td>\n",
       "      <td>-2.410438</td>\n",
       "      <td>-0.701821</td>\n",
       "      <td>-0.403381</td>\n",
       "      <td>0.522945</td>\n",
       "      <td>0.256027</td>\n",
       "      <td>1.826598</td>\n",
       "      <td>-3.091393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167206</th>\n",
       "      <td>0.998582</td>\n",
       "      <td>-1.018484</td>\n",
       "      <td>-0.603367</td>\n",
       "      <td>-0.467136</td>\n",
       "      <td>0.090738</td>\n",
       "      <td>-1.786356</td>\n",
       "      <td>-1.359131</td>\n",
       "      <td>-0.057187</td>\n",
       "      <td>-0.699324</td>\n",
       "      <td>-0.277699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967072</td>\n",
       "      <td>-0.407894</td>\n",
       "      <td>0.675619</td>\n",
       "      <td>-2.690143</td>\n",
       "      <td>0.811195</td>\n",
       "      <td>0.162421</td>\n",
       "      <td>-0.319673</td>\n",
       "      <td>-0.655105</td>\n",
       "      <td>1.837679</td>\n",
       "      <td>-2.292945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40262</th>\n",
       "      <td>0.993674</td>\n",
       "      <td>-0.236895</td>\n",
       "      <td>-1.161556</td>\n",
       "      <td>-0.831213</td>\n",
       "      <td>-0.535987</td>\n",
       "      <td>1.596271</td>\n",
       "      <td>-1.988977</td>\n",
       "      <td>-0.020737</td>\n",
       "      <td>0.298276</td>\n",
       "      <td>0.706597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235213</td>\n",
       "      <td>-0.146208</td>\n",
       "      <td>-1.484007</td>\n",
       "      <td>-1.698481</td>\n",
       "      <td>-0.935188</td>\n",
       "      <td>-0.480799</td>\n",
       "      <td>0.807755</td>\n",
       "      <td>0.346888</td>\n",
       "      <td>1.523457</td>\n",
       "      <td>-1.739759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>0.622462</td>\n",
       "      <td>-0.793748</td>\n",
       "      <td>-0.837395</td>\n",
       "      <td>-0.745482</td>\n",
       "      <td>-0.137351</td>\n",
       "      <td>0.551706</td>\n",
       "      <td>0.313828</td>\n",
       "      <td>-0.853430</td>\n",
       "      <td>0.280148</td>\n",
       "      <td>-0.483826</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114822</td>\n",
       "      <td>-0.845614</td>\n",
       "      <td>-1.312257</td>\n",
       "      <td>-0.366303</td>\n",
       "      <td>-0.078197</td>\n",
       "      <td>1.005683</td>\n",
       "      <td>0.627290</td>\n",
       "      <td>-1.296975</td>\n",
       "      <td>1.453560</td>\n",
       "      <td>-0.719741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124694</th>\n",
       "      <td>1.551901</td>\n",
       "      <td>0.323544</td>\n",
       "      <td>-1.019795</td>\n",
       "      <td>-0.265791</td>\n",
       "      <td>-0.342785</td>\n",
       "      <td>0.346762</td>\n",
       "      <td>0.363122</td>\n",
       "      <td>-0.480910</td>\n",
       "      <td>0.268287</td>\n",
       "      <td>-0.495149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396447</td>\n",
       "      <td>0.472859</td>\n",
       "      <td>-1.687484</td>\n",
       "      <td>-2.504541</td>\n",
       "      <td>-0.655285</td>\n",
       "      <td>0.751038</td>\n",
       "      <td>0.302664</td>\n",
       "      <td>-0.154664</td>\n",
       "      <td>1.174814</td>\n",
       "      <td>-2.710322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185394</th>\n",
       "      <td>1.494337</td>\n",
       "      <td>-0.428976</td>\n",
       "      <td>-0.336699</td>\n",
       "      <td>-0.481797</td>\n",
       "      <td>-1.894806</td>\n",
       "      <td>0.845233</td>\n",
       "      <td>-1.619887</td>\n",
       "      <td>-0.521737</td>\n",
       "      <td>-0.534672</td>\n",
       "      <td>-2.321126</td>\n",
       "      <td>...</td>\n",
       "      <td>1.549637</td>\n",
       "      <td>-0.056938</td>\n",
       "      <td>-0.942204</td>\n",
       "      <td>-1.869788</td>\n",
       "      <td>0.764314</td>\n",
       "      <td>-1.347982</td>\n",
       "      <td>-0.955746</td>\n",
       "      <td>-0.672004</td>\n",
       "      <td>1.069264</td>\n",
       "      <td>-1.114935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_0   image_1   image_2   image_3   image_4   image_5  \\\n",
       "index_key                                                               \n",
       "19757      1.310540  0.253238 -0.208362 -0.584103 -0.794551 -1.803357   \n",
       "111405     1.078856 -0.691140 -0.908770 -0.527087 -1.044688 -0.904328   \n",
       "12269      0.350093 -0.135313  0.512653  0.466054  0.326599  0.014968   \n",
       "186303     4.560193  0.550376 -0.331547 -2.215812  0.023501 -0.370375   \n",
       "134045     2.236698 -0.498289 -1.946595 -0.618339 -2.150359  1.257699   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "167206     0.998582 -1.018484 -0.603367 -0.467136  0.090738 -1.786356   \n",
       "40262      0.993674 -0.236895 -1.161556 -0.831213 -0.535987  1.596271   \n",
       "7002       0.622462 -0.793748 -0.837395 -0.745482 -0.137351  0.551706   \n",
       "124694     1.551901  0.323544 -1.019795 -0.265791 -0.342785  0.346762   \n",
       "185394     1.494337 -0.428976 -0.336699 -0.481797 -1.894806  0.845233   \n",
       "\n",
       "            image_6   image_7   image_8   image_9  ...  image_246  image_247  \\\n",
       "index_key                                          ...                         \n",
       "19757     -2.700018 -0.848385  0.949902  0.030431  ...  -0.475167  -0.116055   \n",
       "111405     0.210946 -1.238919  2.290273 -0.155667  ...   0.833435  -1.053398   \n",
       "12269     -2.224684 -0.740723  0.951188  0.978717  ...   1.401945  -0.270900   \n",
       "186303    -0.838666 -0.905349  2.655245  0.461321  ...   1.073267   0.354717   \n",
       "134045    -1.547900 -1.492419  2.133226  1.513557  ...   0.192120  -0.731990   \n",
       "...             ...       ...       ...       ...  ...        ...        ...   \n",
       "167206    -1.359131 -0.057187 -0.699324 -0.277699  ...   0.967072  -0.407894   \n",
       "40262     -1.988977 -0.020737  0.298276  0.706597  ...   1.235213  -0.146208   \n",
       "7002       0.313828 -0.853430  0.280148 -0.483826  ...   1.114822  -0.845614   \n",
       "124694     0.363122 -0.480910  0.268287 -0.495149  ...   1.396447   0.472859   \n",
       "185394    -1.619887 -0.521737 -0.534672 -2.321126  ...   1.549637  -0.056938   \n",
       "\n",
       "           image_248  image_249  image_250  image_251  image_252  image_253  \\\n",
       "index_key                                                                     \n",
       "19757       0.735540  -2.354414   0.956939  -1.065875  -0.428229  -0.285047   \n",
       "111405     -2.031961  -2.716383   0.817275  -0.434370  -1.456125   0.112614   \n",
       "12269      -1.967142  -0.814089   0.170715   0.335253  -0.030882  -0.557203   \n",
       "186303     -1.745163  -2.610591  -0.239567  -1.714204  -0.914772  -0.354720   \n",
       "134045     -3.023546  -2.410438  -0.701821  -0.403381   0.522945   0.256027   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "167206      0.675619  -2.690143   0.811195   0.162421  -0.319673  -0.655105   \n",
       "40262      -1.484007  -1.698481  -0.935188  -0.480799   0.807755   0.346888   \n",
       "7002       -1.312257  -0.366303  -0.078197   1.005683   0.627290  -1.296975   \n",
       "124694     -1.687484  -2.504541  -0.655285   0.751038   0.302664  -0.154664   \n",
       "185394     -0.942204  -1.869788   0.764314  -1.347982  -0.955746  -0.672004   \n",
       "\n",
       "           image_254  image_255  \n",
       "index_key                        \n",
       "19757       1.098027  -1.029737  \n",
       "111405      0.199815  -2.946273  \n",
       "12269       0.037506  -1.951925  \n",
       "186303      0.098914  -3.272672  \n",
       "134045      1.826598  -3.091393  \n",
       "...              ...        ...  \n",
       "167206      1.837679  -2.292945  \n",
       "40262       1.523457  -1.739759  \n",
       "7002        1.453560  -0.719741  \n",
       "124694      1.174814  -2.710322  \n",
       "185394      1.069264  -1.114935  \n",
       "\n",
       "[21419 rows x 256 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'tSVD2991', 'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996',\n",
       "       'tSVD2997', 'tSVD2998', 'tSVD2999', 'tSVD3000'],\n",
       "      dtype='object', length=3018)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP test\n",
    "NLP_df_test = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_test_tSVD_3000.csv')\n",
    ")\n",
    "\n",
    "# Set indices as in train test split\n",
    "NLP_df_test = NLP_df_test.set_index('index')\n",
    "\n",
    "NLP_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>From Potter's Field</td>\n",
       "      <td>The sixth book in the Kay Scarpetta series, fr...</td>\n",
       "      <td>['Patricia Cornwell']</td>\n",
       "      <td>http://books.google.com/books/content?id=prefg...</td>\n",
       "      <td>http://books.google.nl/books?id=prefgSxnGOwC&amp;p...</td>\n",
       "      <td>Hachette UK</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>157</td>\n",
       "      <td>3.783439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.011035</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>-0.009888</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.021092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Riverworld and Other Stories</td>\n",
       "      <td>Three stories of a world shared by resurrected...</td>\n",
       "      <td>['Philip José Farmer']</td>\n",
       "      <td>http://books.google.com/books/content?id=TP4oD...</td>\n",
       "      <td>http://books.google.nl/books?id=TP4oDwAAQBAJ&amp;p...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>7</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.007662</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>-0.007470</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>0.012055</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>0.011430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Kenny Doin' Just Fine</td>\n",
       "      <td>KENNY DOIN' JUST FINE Miriam Greenfield, a pro...</td>\n",
       "      <td>['Sadie Wernick Hurwitz']</td>\n",
       "      <td>http://books.google.com/books/content?id=D6Wgi...</td>\n",
       "      <td>http://books.google.nl/books?id=D6WgitXrr8sC&amp;p...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=D6WgitXrr8sC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>-0.004538</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>-0.015920</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>-0.003591</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>-0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Harry on the Rocks</td>\n",
       "      <td>Harry and his boat become stranded on an islan...</td>\n",
       "      <td>['Susan Meddaugh']</td>\n",
       "      <td>http://books.google.com/books/content?id=u5r79...</td>\n",
       "      <td>http://books.google.nl/books?id=u5r79DAUeIYC&amp;q...</td>\n",
       "      <td>Houghton Mifflin Harcourt</td>\n",
       "      <td>http://books.google.nl/books?id=u5r79DAUeIYC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.003397</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.011397</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>-0.004257</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>-0.002277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>The National Review Treasury of Classic Childr...</td>\n",
       "      <td>A collection of over forty stories, tales, poe...</td>\n",
       "      <td>['William F. Buckley, Jr.']</td>\n",
       "      <td>http://books.google.com/books/content?id=NZm7P...</td>\n",
       "      <td>http://books.google.nl/books?id=NZm7PAAACAAJ&amp;d...</td>\n",
       "      <td>Isi Books</td>\n",
       "      <td>http://books.google.nl/books?id=NZm7PAAACAAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002185</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>-0.000960</td>\n",
       "      <td>-0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212041</th>\n",
       "      <td>Man For Maggie Moore (Montana Matchmakers) (Ha...</td>\n",
       "      <td>You don't know what love is until you have los...</td>\n",
       "      <td>['Steven Labree']</td>\n",
       "      <td>http://books.google.com/books/content?id=NZpeJ...</td>\n",
       "      <td>http://books.google.com/books?id=NZpeJhtmGo8C&amp;...</td>\n",
       "      <td>Steven LaBree</td>\n",
       "      <td>http://books.google.com/books?id=NZpeJhtmGo8C&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>3</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>-0.010747</td>\n",
       "      <td>-0.012375</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>-0.006146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212144</th>\n",
       "      <td>Prancing Tiger</td>\n",
       "      <td>To clear the name of his ex-girlfriend's son, ...</td>\n",
       "      <td>['Philip Singerman']</td>\n",
       "      <td>http://books.google.com/books/content?id=68R7S...</td>\n",
       "      <td>http://books.google.com/books?id=68R7SppHYHcC&amp;...</td>\n",
       "      <td>William Morrow</td>\n",
       "      <td>http://books.google.com/books?id=68R7SppHYHcC&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011748</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>-0.006230</td>\n",
       "      <td>-0.008297</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>-0.002252</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>-0.001916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212256</th>\n",
       "      <td>Nude Men: A Novel</td>\n",
       "      <td>The internationally acclaimed debut of a novel...</td>\n",
       "      <td>['Amanda Filipacchi']</td>\n",
       "      <td>http://books.google.com/books/content?id=uM-1A...</td>\n",
       "      <td>http://books.google.com/books?id=uM-1AwAAQBAJ&amp;...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>23</td>\n",
       "      <td>3.739130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010690</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>-0.001923</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>-0.002761</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>0.018299</td>\n",
       "      <td>-0.007997</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.007875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212260</th>\n",
       "      <td>The Tale of Digby</td>\n",
       "      <td>In Digby, Willy Wink finds himself in the midd...</td>\n",
       "      <td>['Timothy Lee Bonnette, Jr.']</td>\n",
       "      <td>http://books.google.com/books/content?id=pcgBA...</td>\n",
       "      <td>http://books.google.com/books?id=pcgBAAAACAAJ&amp;...</td>\n",
       "      <td>Publishamerica Incorporated</td>\n",
       "      <td>http://books.google.com/books?id=pcgBAAAACAAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>0.008438</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>-0.006659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212333</th>\n",
       "      <td>Blueprint For Revenge</td>\n",
       "      <td>Johanna is devastated when her beloved grandmo...</td>\n",
       "      <td>['Martine Jardin']</td>\n",
       "      <td>http://books.google.com/books/content?id=I96XB...</td>\n",
       "      <td>http://books.google.com/books?id=I96XBQAAQBAJ&amp;...</td>\n",
       "      <td>Devine Destinies</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001040</td>\n",
       "      <td>-0.004460</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.008113</td>\n",
       "      <td>-0.002739</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>-0.008496</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5355 rows × 3018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "index                                                       \n",
       "115                                   From Potter's Field   \n",
       "209                          Riverworld and Other Stories   \n",
       "330                                 Kenny Doin' Just Fine   \n",
       "333                                    Harry on the Rocks   \n",
       "371     The National Review Treasury of Classic Childr...   \n",
       "...                                                   ...   \n",
       "212041  Man For Maggie Moore (Montana Matchmakers) (Ha...   \n",
       "212144                                     Prancing Tiger   \n",
       "212256                                  Nude Men: A Novel   \n",
       "212260                                  The Tale of Digby   \n",
       "212333                              Blueprint For Revenge   \n",
       "\n",
       "                                              description  \\\n",
       "index                                                       \n",
       "115     The sixth book in the Kay Scarpetta series, fr...   \n",
       "209     Three stories of a world shared by resurrected...   \n",
       "330     KENNY DOIN' JUST FINE Miriam Greenfield, a pro...   \n",
       "333     Harry and his boat become stranded on an islan...   \n",
       "371     A collection of over forty stories, tales, poe...   \n",
       "...                                                   ...   \n",
       "212041  You don't know what love is until you have los...   \n",
       "212144  To clear the name of his ex-girlfriend's son, ...   \n",
       "212256  The internationally acclaimed debut of a novel...   \n",
       "212260  In Digby, Willy Wink finds himself in the midd...   \n",
       "212333  Johanna is devastated when her beloved grandmo...   \n",
       "\n",
       "                              authors  \\\n",
       "index                                   \n",
       "115             ['Patricia Cornwell']   \n",
       "209            ['Philip José Farmer']   \n",
       "330         ['Sadie Wernick Hurwitz']   \n",
       "333                ['Susan Meddaugh']   \n",
       "371       ['William F. Buckley, Jr.']   \n",
       "...                               ...   \n",
       "212041              ['Steven Labree']   \n",
       "212144           ['Philip Singerman']   \n",
       "212256          ['Amanda Filipacchi']   \n",
       "212260  ['Timothy Lee Bonnette, Jr.']   \n",
       "212333             ['Martine Jardin']   \n",
       "\n",
       "                                                    image  \\\n",
       "index                                                       \n",
       "115     http://books.google.com/books/content?id=prefg...   \n",
       "209     http://books.google.com/books/content?id=TP4oD...   \n",
       "330     http://books.google.com/books/content?id=D6Wgi...   \n",
       "333     http://books.google.com/books/content?id=u5r79...   \n",
       "371     http://books.google.com/books/content?id=NZm7P...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books/content?id=NZpeJ...   \n",
       "212144  http://books.google.com/books/content?id=68R7S...   \n",
       "212256  http://books.google.com/books/content?id=uM-1A...   \n",
       "212260  http://books.google.com/books/content?id=pcgBA...   \n",
       "212333  http://books.google.com/books/content?id=I96XB...   \n",
       "\n",
       "                                              previewLink  \\\n",
       "index                                                       \n",
       "115     http://books.google.nl/books?id=prefgSxnGOwC&p...   \n",
       "209     http://books.google.nl/books?id=TP4oDwAAQBAJ&p...   \n",
       "330     http://books.google.nl/books?id=D6WgitXrr8sC&p...   \n",
       "333     http://books.google.nl/books?id=u5r79DAUeIYC&q...   \n",
       "371     http://books.google.nl/books?id=NZm7PAAACAAJ&d...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books?id=NZpeJhtmGo8C&...   \n",
       "212144  http://books.google.com/books?id=68R7SppHYHcC&...   \n",
       "212256  http://books.google.com/books?id=uM-1AwAAQBAJ&...   \n",
       "212260  http://books.google.com/books?id=pcgBAAAACAAJ&...   \n",
       "212333  http://books.google.com/books?id=I96XBQAAQBAJ&...   \n",
       "\n",
       "                          publisher  \\\n",
       "index                                 \n",
       "115                     Hachette UK   \n",
       "209                 Open Road Media   \n",
       "330                       iUniverse   \n",
       "333       Houghton Mifflin Harcourt   \n",
       "371                       Isi Books   \n",
       "...                             ...   \n",
       "212041                Steven LaBree   \n",
       "212144               William Morrow   \n",
       "212256              Open Road Media   \n",
       "212260  Publishamerica Incorporated   \n",
       "212333             Devine Destinies   \n",
       "\n",
       "                                                 infoLink  \\\n",
       "index                                                       \n",
       "115     https://play.google.com/store/books/details?id...   \n",
       "209     https://play.google.com/store/books/details?id...   \n",
       "330     http://books.google.nl/books?id=D6WgitXrr8sC&d...   \n",
       "333     http://books.google.nl/books?id=u5r79DAUeIYC&d...   \n",
       "371     http://books.google.nl/books?id=NZm7PAAACAAJ&d...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books?id=NZpeJhtmGo8C&...   \n",
       "212144  http://books.google.com/books?id=68R7SppHYHcC&...   \n",
       "212256  https://play.google.com/store/books/details?id...   \n",
       "212260  http://books.google.com/books?id=pcgBAAAACAAJ&...   \n",
       "212333  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                  categories  reviews number  average rating  ...  tSVD2991  \\\n",
       "index                                                         ...             \n",
       "115              ['fiction']             157        3.783439  ...  0.004673   \n",
       "209              ['fiction']               7        4.285714  ...  0.000682   \n",
       "330              ['fiction']               1        5.000000  ... -0.005573   \n",
       "333     ['juvenile fiction']               2        5.000000  ...  0.003240   \n",
       "371     ['juvenile fiction']               3        5.000000  ... -0.002185   \n",
       "...                      ...             ...             ...  ...       ...   \n",
       "212041           ['fiction']               3        4.666667  ...  0.004356   \n",
       "212144           ['fiction']               4        4.250000  ...  0.011748   \n",
       "212256           ['fiction']              23        3.739130  ... -0.010690   \n",
       "212260           ['fiction']               2        4.000000  ...  0.002488   \n",
       "212333           ['fiction']               1        5.000000  ... -0.001040   \n",
       "\n",
       "        tSVD2992  tSVD2993  tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  \\\n",
       "index                                                                          \n",
       "115     0.011035  0.000885  0.011355  0.003110 -0.009888  0.001707 -0.000568   \n",
       "209    -0.000072 -0.007662  0.010220  0.001084 -0.007470 -0.006064  0.012055   \n",
       "330    -0.004538  0.000019  0.002786 -0.015920  0.004716 -0.000231 -0.003591   \n",
       "333    -0.001592 -0.003397  0.001563  0.008503  0.011397 -0.003413 -0.004257   \n",
       "371    -0.001413  0.003879  0.004376  0.000110  0.002358  0.005980  0.005749   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212041  0.004014  0.000388 -0.010747 -0.012375 -0.001525  0.006544  0.013144   \n",
       "212144 -0.000504  0.006213 -0.006230 -0.008297 -0.009560 -0.002252  0.006993   \n",
       "212256  0.012784 -0.001923  0.008254 -0.002761 -0.003366  0.018299 -0.007997   \n",
       "212260  0.002354  0.004941 -0.000441  0.008438  0.012563  0.002361  0.008794   \n",
       "212333 -0.004460 -0.001457 -0.008113 -0.002739 -0.001419  0.011348 -0.008496   \n",
       "\n",
       "        tSVD2999  tSVD3000  \n",
       "index                       \n",
       "115     0.000850  0.021092  \n",
       "209    -0.007764  0.011430  \n",
       "330     0.000867 -0.001421  \n",
       "333     0.006653 -0.002277  \n",
       "371    -0.000960 -0.003924  \n",
       "...          ...       ...  \n",
       "212041  0.003183 -0.006146  \n",
       "212144  0.007963 -0.001916  \n",
       "212256  0.004508  0.007875  \n",
       "212260  0.000451 -0.006659  \n",
       "212333  0.005376  0.000832  \n",
       "\n",
       "[5355 rows x 3018 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'tSVD2991', 'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996',\n",
       "       'tSVD2997', 'tSVD2998', 'tSVD2999', 'tSVD3000'],\n",
       "      dtype='object', length=3018)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP train\n",
    "NLP_df_train = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_train_tSVD_3000.csv')\n",
    ")\n",
    "\n",
    "NLP_df_train = NLP_df_train.set_index('index')\n",
    "\n",
    "NLP_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "index                                                       \n",
       "3                           Whispers of the Wicked Saints   \n",
       "24               The Forbidden Stories of Marta Veneranda   \n",
       "42                                Tess and the Highlander   \n",
       "49      Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "73                     Night World: Daughters Of Darkness   \n",
       "...                                                   ...   \n",
       "212361                                       Calder Pride   \n",
       "212365                                      The Road Back   \n",
       "212394                                       Final things   \n",
       "212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "212402                                  The Autograph Man   \n",
       "\n",
       "                                              description  \\\n",
       "index                                                       \n",
       "3       Julia Thomas finds her life spinning out of co...   \n",
       "24      Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "42      In 1543, on a windswept isle off of Scotland, ...   \n",
       "49      Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "73      \"There’s something strange about the new girls...   \n",
       "...                                                   ...   \n",
       "212361  The Long-Awaited Addition to the Beloved Calde...   \n",
       "212365  The sequel to the masterpiece All Quiet on the...   \n",
       "212394  Grace's father believes in science and builds ...   \n",
       "212399  During a school trip to Ellis Island, Dominick...   \n",
       "212402  Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                         authors  \\\n",
       "index                              \n",
       "3            ['Veronica Haddon']   \n",
       "24       ['Sonia Rivera-Valdes']   \n",
       "42            ['May Mcgoldrick']   \n",
       "49        ['Elizabeth Sinclair']   \n",
       "73                ['L.J. Smith']   \n",
       "...                          ...   \n",
       "212361          ['Janet Dailey']   \n",
       "212365  ['Erich Maria Remarque']   \n",
       "212394          ['Jenny Offill']   \n",
       "212399       ['Elvira Woodruff']   \n",
       "212402           ['Zadie Smith']   \n",
       "\n",
       "                                                    image  \\\n",
       "index                                                       \n",
       "3       http://books.google.com/books/content?id=aRSIg...   \n",
       "24      http://books.google.com/books/content?id=A7aYb...   \n",
       "42      http://books.google.com/books/content?id=VmCRS...   \n",
       "49      http://books.google.com/books/content?id=Z6uzJ...   \n",
       "73      http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                   ...   \n",
       "212361  http://books.google.com/books/content?id=nlsgd...   \n",
       "212365  http://books.google.com/books/content?id=obZdA...   \n",
       "212394  http://books.google.com/books/content?id=UbSFB...   \n",
       "212399  http://books.google.com/books/content?id=J7M-N...   \n",
       "212402  http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                              previewLink  \\\n",
       "index                                                       \n",
       "3       http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24      http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "42      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49      http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "73      http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                   ...   \n",
       "212361  http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "212365  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394  http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "212399  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402  http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                                publisher  \\\n",
       "index                                                       \n",
       "3                                               iUniverse   \n",
       "24                                    Seven Stories Press   \n",
       "42                                         Harper Collins   \n",
       "49      Harlequin Treasury-Harlequin American Romance 90s   \n",
       "73                                     Simon and Schuster   \n",
       "...                                                   ...   \n",
       "212361                                     Harper Collins   \n",
       "212365                      Random House Trade Paperbacks   \n",
       "212394                                            Vintage   \n",
       "212399                              Scholastic Paperbacks   \n",
       "212402                                            Vintage   \n",
       "\n",
       "                                                 infoLink  \\\n",
       "index                                                       \n",
       "3       http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24      http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "42      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49      http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "73      http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                   ...   \n",
       "212361  https://play.google.com/store/books/details?id...   \n",
       "212365  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394  https://play.google.com/store/books/details?id...   \n",
       "212399  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                  categories  reviews number  average rating  ...  tSVD2991  \\\n",
       "index                                                         ...             \n",
       "3                ['fiction']              32        3.718750  ...  0.001714   \n",
       "24               ['fiction']               1        5.000000  ...  0.000900   \n",
       "42      ['juvenile fiction']              17        4.235294  ... -0.002241   \n",
       "49               ['fiction']               2        5.000000  ...  0.004947   \n",
       "73      ['juvenile fiction']             134        4.768657  ...  0.000070   \n",
       "...                      ...             ...             ...  ...       ...   \n",
       "212361           ['fiction']              28        4.035714  ... -0.003849   \n",
       "212365           ['fiction']              17        4.705882  ...  0.014579   \n",
       "212394           ['fiction']               1        4.000000  ...  0.007651   \n",
       "212399  ['juvenile fiction']              28        4.678571  ... -0.012930   \n",
       "212402           ['fiction']               4        2.500000  ... -0.007902   \n",
       "\n",
       "        tSVD2992  tSVD2993  tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  \\\n",
       "index                                                                          \n",
       "3      -0.012096  0.012755  0.004345  0.016297  0.022422  0.026395 -0.001808   \n",
       "24      0.002294 -0.002052 -0.013243 -0.002329 -0.005818 -0.000012  0.007939   \n",
       "42     -0.007230 -0.005164  0.000416 -0.007837 -0.002487 -0.004066  0.010140   \n",
       "49     -0.003206  0.011330 -0.004898 -0.002988  0.001996 -0.008123 -0.000639   \n",
       "73      0.003473 -0.013758  0.000954 -0.011287 -0.008795  0.003780  0.002349   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361 -0.000571  0.001393 -0.001693 -0.012675 -0.000349  0.002634 -0.004115   \n",
       "212365 -0.010071 -0.008840  0.011693  0.006862 -0.001960  0.004995 -0.007247   \n",
       "212394  0.005601  0.000670 -0.010949 -0.012086  0.002205 -0.004838 -0.008230   \n",
       "212399  0.005578 -0.000719 -0.004401  0.003777 -0.001121 -0.006025 -0.007464   \n",
       "212402  0.017672  0.005686  0.007956 -0.003892 -0.007589 -0.002541  0.005038   \n",
       "\n",
       "        tSVD2999  tSVD3000  \n",
       "index                       \n",
       "3      -0.028102  0.011717  \n",
       "24     -0.024247 -0.006868  \n",
       "42     -0.007082  0.001236  \n",
       "49     -0.016424 -0.004971  \n",
       "73     -0.013346  0.001237  \n",
       "...          ...       ...  \n",
       "212361  0.004868 -0.006720  \n",
       "212365 -0.013905 -0.001202  \n",
       "212394  0.000084 -0.002611  \n",
       "212399 -0.008325  0.005250  \n",
       "212402 -0.010622 -0.009332  \n",
       "\n",
       "[21419 rows x 3018 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [col for col in NLP_df_test.columns if col.startswith('tSVD')]\n",
    "\n",
    "NLP_df_train = NLP_df_train[columns_to_keep]\n",
    "NLP_df_test = NLP_df_test[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "index                                                                          \n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2991  tSVD2992  tSVD2993  \\\n",
       "index                                 ...                                 \n",
       "3       0.006327  0.040408  0.035931  ...  0.001714 -0.012096  0.012755   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.000900  0.002294 -0.002052   \n",
       "42      0.042483  0.019502  0.016915  ... -0.002241 -0.007230 -0.005164   \n",
       "49     -0.027805 -0.038459 -0.016390  ...  0.004947 -0.003206  0.011330   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.000070  0.003473 -0.013758   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.003849 -0.000571  0.001393   \n",
       "212365 -0.028389  0.026683  0.003763  ...  0.014579 -0.010071 -0.008840   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.007651  0.005601  0.000670   \n",
       "212399 -0.028692 -0.017410  0.020350  ... -0.012930  0.005578 -0.000719   \n",
       "212402 -0.004967 -0.007246 -0.009244  ... -0.007902  0.017672  0.005686   \n",
       "\n",
       "        tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000  \n",
       "index                                                                         \n",
       "3       0.004345  0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  \n",
       "24     -0.013243 -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  \n",
       "42      0.000416 -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  \n",
       "49     -0.004898 -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  \n",
       "73      0.000954 -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "212361 -0.001693 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  \n",
       "212365  0.011693  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  \n",
       "212394 -0.010949 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  \n",
       "212399 -0.004401  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  \n",
       "212402  0.007956 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  \n",
       "\n",
       "[21419 rows x 3000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description dimension reduction NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "Inputs to choose:\n",
    "- number of layers:\n",
    "    - Description NN\n",
    "        - input\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - final layer\n",
    "    - Description and image embeddings NN\n",
    "        - input\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - noise\n",
    "        - final layer\n",
    "    Too many?   \n",
    "- add dense layers to avoid overfitting?\n",
    "- activation functions\n",
    "    - ReLu (Rectified linear activation function): piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. Simple but effective.\n",
    "- Use linear in the last layer to obtain a continuous variable\n",
    "- optimizer: \n",
    "    - Adam; works with momentums of first and second order. \n",
    "    - sdg: variant of Gradient Descent (Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. It's easy and works well but there is the risk that the model gets stuck in local minima)\n",
    "- loss function\n",
    "    - MSE?\n",
    "- number of epochs\n",
    "- which metric to use to evaluate the model?\n",
    "    - MSE\n",
    "    - MAE\n",
    "\n",
    "- Use gridsearch to optimise hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,536,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,667,840</span> (6.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,667,840\u001b[0m (6.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,667,840</span> (6.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,667,840\u001b[0m (6.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get number of inputs - second element of shape (i.e. number of columns in X)\n",
    "input_shape = NLP_df_train.shape[1]\n",
    "\n",
    "# neurons number\n",
    "n_neurons = 512\n",
    "\n",
    "# define a model\n",
    "baseline_model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "baseline_model.add(layers.Dense(\n",
    "            n_neurons, # number of neurons\n",
    "            input_dim = input_shape, # number of inputs \n",
    "            activation = 'relu' # activation faunction\n",
    "            ))\n",
    "\n",
    "# Hidden - Layers\n",
    "baseline_model.add(layers.Dense(\n",
    "                    256, \n",
    "                    activation = \"linear\"))\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mean_squared_error'], \n",
    "    metrics = ['mae', 'mean_squared_error']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m 59/429\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 16.1463 - mae: 3.9923 - mean_squared_error: 16.1463"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 5.7757 - mae: 1.6848 - mean_squared_error: 5.7757 - val_loss: 0.0666 - val_mae: 0.1820 - val_mean_squared_error: 0.0648\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0606 - mae: 0.1794 - mean_squared_error: 0.0606 - val_loss: 0.0655 - val_mae: 0.1814 - val_mean_squared_error: 0.0639\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0634 - mae: 0.1841 - mean_squared_error: 0.0634 - val_loss: 0.0654 - val_mae: 0.1786 - val_mean_squared_error: 0.0639\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0616 - mae: 0.1809 - mean_squared_error: 0.0616 - val_loss: 0.0625 - val_mae: 0.1749 - val_mean_squared_error: 0.0611\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0592 - mae: 0.1769 - mean_squared_error: 0.0592 - val_loss: 0.0623 - val_mae: 0.1724 - val_mean_squared_error: 0.0603\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0584 - mae: 0.1755 - mean_squared_error: 0.0584 - val_loss: 0.0593 - val_mae: 0.1700 - val_mean_squared_error: 0.0581\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0561 - mae: 0.1712 - mean_squared_error: 0.0561 - val_loss: 0.0619 - val_mae: 0.1665 - val_mean_squared_error: 0.0603\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0554 - mae: 0.1687 - mean_squared_error: 0.0554 - val_loss: 0.0593 - val_mae: 0.1752 - val_mean_squared_error: 0.0579\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0535 - mae: 0.1662 - mean_squared_error: 0.0535 - val_loss: 0.0590 - val_mae: 0.1614 - val_mean_squared_error: 0.0576\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0522 - mae: 0.1630 - mean_squared_error: 0.0522 - val_loss: 0.0560 - val_mae: 0.1609 - val_mean_squared_error: 0.0545\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0520 - mae: 0.1630 - mean_squared_error: 0.0520 - val_loss: 0.0586 - val_mae: 0.1779 - val_mean_squared_error: 0.0574\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0509 - mae: 0.1609 - mean_squared_error: 0.0509 - val_loss: 0.0552 - val_mae: 0.1619 - val_mean_squared_error: 0.0540\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0498 - mae: 0.1587 - mean_squared_error: 0.0498 - val_loss: 0.0543 - val_mae: 0.1594 - val_mean_squared_error: 0.0528\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0491 - mae: 0.1565 - mean_squared_error: 0.0491 - val_loss: 0.0547 - val_mae: 0.1531 - val_mean_squared_error: 0.0534\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0464 - mae: 0.1524 - mean_squared_error: 0.0464 - val_loss: 0.0539 - val_mae: 0.1597 - val_mean_squared_error: 0.0526\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0466 - mae: 0.1532 - mean_squared_error: 0.0466 - val_loss: 0.0536 - val_mae: 0.1571 - val_mean_squared_error: 0.0521\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0476 - mae: 0.1535 - mean_squared_error: 0.0476 - val_loss: 0.0586 - val_mae: 0.1551 - val_mean_squared_error: 0.0567\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0468 - mae: 0.1533 - mean_squared_error: 0.0468 - val_loss: 0.0588 - val_mae: 0.1558 - val_mean_squared_error: 0.0574\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0457 - mae: 0.1508 - mean_squared_error: 0.0457 - val_loss: 0.0533 - val_mae: 0.1582 - val_mean_squared_error: 0.0522\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0426 - mae: 0.1469 - mean_squared_error: 0.0426 - val_loss: 0.0546 - val_mae: 0.1623 - val_mean_squared_error: 0.0532\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0438 - mae: 0.1485 - mean_squared_error: 0.0438 - val_loss: 0.0540 - val_mae: 0.1543 - val_mean_squared_error: 0.0526\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0433 - mae: 0.1450 - mean_squared_error: 0.0433 - val_loss: 0.0549 - val_mae: 0.1537 - val_mean_squared_error: 0.0532\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0405 - mae: 0.1427 - mean_squared_error: 0.0405 - val_loss: 0.0580 - val_mae: 0.1580 - val_mean_squared_error: 0.0566\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0403 - mae: 0.1432 - mean_squared_error: 0.0403 - val_loss: 0.0618 - val_mae: 0.1866 - val_mean_squared_error: 0.0609\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0400 - mae: 0.1422 - mean_squared_error: 0.0400 - val_loss: 0.0588 - val_mae: 0.1588 - val_mean_squared_error: 0.0573\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0395 - mae: 0.1424 - mean_squared_error: 0.0395 - val_loss: 0.0571 - val_mae: 0.1587 - val_mean_squared_error: 0.0554\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0378 - mae: 0.1389 - mean_squared_error: 0.0378 - val_loss: 0.0570 - val_mae: 0.1672 - val_mean_squared_error: 0.0558\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0363 - mae: 0.1372 - mean_squared_error: 0.0363 - val_loss: 0.0588 - val_mae: 0.1607 - val_mean_squared_error: 0.0574\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0350 - mae: 0.1341 - mean_squared_error: 0.0350 - val_loss: 0.0576 - val_mae: 0.1621 - val_mean_squared_error: 0.0563\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0339 - mae: 0.1312 - mean_squared_error: 0.0339 - val_loss: 0.0675 - val_mae: 0.1729 - val_mean_squared_error: 0.0660\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0329 - mae: 0.1310 - mean_squared_error: 0.0329 - val_loss: 0.0709 - val_mae: 0.2070 - val_mean_squared_error: 0.0700\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0321 - mae: 0.1305 - mean_squared_error: 0.0321 - val_loss: 0.0595 - val_mae: 0.1671 - val_mean_squared_error: 0.0584\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0301 - mae: 0.1266 - mean_squared_error: 0.0301 - val_loss: 0.0614 - val_mae: 0.1670 - val_mean_squared_error: 0.0600\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0297 - mae: 0.1266 - mean_squared_error: 0.0297 - val_loss: 0.0597 - val_mae: 0.1694 - val_mean_squared_error: 0.0584\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0289 - mae: 0.1228 - mean_squared_error: 0.0289 - val_loss: 0.0603 - val_mae: 0.1683 - val_mean_squared_error: 0.0592\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0276 - mae: 0.1208 - mean_squared_error: 0.0276 - val_loss: 0.0620 - val_mae: 0.1707 - val_mean_squared_error: 0.0608\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0261 - mae: 0.1173 - mean_squared_error: 0.0261 - val_loss: 0.0617 - val_mae: 0.1702 - val_mean_squared_error: 0.0609\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0259 - mae: 0.1170 - mean_squared_error: 0.0259 - val_loss: 0.0648 - val_mae: 0.1732 - val_mean_squared_error: 0.0637\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0249 - mae: 0.1164 - mean_squared_error: 0.0249 - val_loss: 0.0717 - val_mae: 0.1820 - val_mean_squared_error: 0.0703\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0233 - mae: 0.1128 - mean_squared_error: 0.0233 - val_loss: 0.0664 - val_mae: 0.1888 - val_mean_squared_error: 0.0656\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0209 - mae: 0.1063 - mean_squared_error: 0.0209 - val_loss: 0.0653 - val_mae: 0.1781 - val_mean_squared_error: 0.0643\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0190 - mae: 0.1014 - mean_squared_error: 0.0190 - val_loss: 0.0759 - val_mae: 0.1899 - val_mean_squared_error: 0.0744\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0172 - mae: 0.0957 - mean_squared_error: 0.0172 - val_loss: 0.0665 - val_mae: 0.1793 - val_mean_squared_error: 0.0655\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0153 - mae: 0.0888 - mean_squared_error: 0.0153 - val_loss: 0.0662 - val_mae: 0.1797 - val_mean_squared_error: 0.0654\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0136 - mae: 0.0832 - mean_squared_error: 0.0136 - val_loss: 0.0679 - val_mae: 0.1816 - val_mean_squared_error: 0.0668\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0125 - mae: 0.0794 - mean_squared_error: 0.0125 - val_loss: 0.0740 - val_mae: 0.1901 - val_mean_squared_error: 0.0731\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0123 - mae: 0.0789 - mean_squared_error: 0.0123 - val_loss: 0.0677 - val_mae: 0.1840 - val_mean_squared_error: 0.0669\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0109 - mae: 0.0737 - mean_squared_error: 0.0109 - val_loss: 0.0782 - val_mae: 0.1957 - val_mean_squared_error: 0.0769\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0111 - mae: 0.0741 - mean_squared_error: 0.0111 - val_loss: 0.0823 - val_mae: 0.2022 - val_mean_squared_error: 0.0812\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0102 - mae: 0.0681 - mean_squared_error: 0.0102 - val_loss: 0.0690 - val_mae: 0.1870 - val_mean_squared_error: 0.0683\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0101 - mae: 0.0679 - mean_squared_error: 0.0101 - val_loss: 0.0695 - val_mae: 0.1886 - val_mean_squared_error: 0.0688\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0095 - mae: 0.0659 - mean_squared_error: 0.0095 - val_loss: 0.0747 - val_mae: 0.1905 - val_mean_squared_error: 0.0734\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0085 - mae: 0.0616 - mean_squared_error: 0.0085 - val_loss: 0.0740 - val_mae: 0.1905 - val_mean_squared_error: 0.0730\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0082 - mae: 0.0592 - mean_squared_error: 0.0082 - val_loss: 0.0740 - val_mae: 0.1903 - val_mean_squared_error: 0.0730\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0082 - mae: 0.0591 - mean_squared_error: 0.0082 - val_loss: 0.0692 - val_mae: 0.1880 - val_mean_squared_error: 0.0685\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0091 - mae: 0.0629 - mean_squared_error: 0.0091 - val_loss: 0.0710 - val_mae: 0.1873 - val_mean_squared_error: 0.0700\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0074 - mae: 0.0550 - mean_squared_error: 0.0074 - val_loss: 0.0794 - val_mae: 0.1971 - val_mean_squared_error: 0.0782\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0078 - mae: 0.0566 - mean_squared_error: 0.0078 - val_loss: 0.0824 - val_mae: 0.2036 - val_mean_squared_error: 0.0812\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0081 - mae: 0.0565 - mean_squared_error: 0.0081 - val_loss: 0.0763 - val_mae: 0.1940 - val_mean_squared_error: 0.0754\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0077 - mae: 0.0553 - mean_squared_error: 0.0077 - val_loss: 0.0749 - val_mae: 0.1929 - val_mean_squared_error: 0.0741\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0070 - mae: 0.0522 - mean_squared_error: 0.0070 - val_loss: 0.0772 - val_mae: 0.1950 - val_mean_squared_error: 0.0760\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0079 - mae: 0.0562 - mean_squared_error: 0.0079 - val_loss: 0.0768 - val_mae: 0.1951 - val_mean_squared_error: 0.0756\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0074 - mae: 0.0531 - mean_squared_error: 0.0074 - val_loss: 0.0774 - val_mae: 0.1955 - val_mean_squared_error: 0.0764\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0525 - mean_squared_error: 0.0071 - val_loss: 0.0783 - val_mae: 0.1973 - val_mean_squared_error: 0.0775\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0077 - mae: 0.0541 - mean_squared_error: 0.0077 - val_loss: 0.0790 - val_mae: 0.1976 - val_mean_squared_error: 0.0779\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0527 - mean_squared_error: 0.0070 - val_loss: 0.0772 - val_mae: 0.1956 - val_mean_squared_error: 0.0762\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0495 - mean_squared_error: 0.0070 - val_loss: 0.0733 - val_mae: 0.1899 - val_mean_squared_error: 0.0724\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0073 - mae: 0.0535 - mean_squared_error: 0.0073 - val_loss: 0.0792 - val_mae: 0.1989 - val_mean_squared_error: 0.0782\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0541 - mean_squared_error: 0.0071 - val_loss: 0.0752 - val_mae: 0.1918 - val_mean_squared_error: 0.0741\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0064 - mae: 0.0493 - mean_squared_error: 0.0064 - val_loss: 0.0697 - val_mae: 0.1867 - val_mean_squared_error: 0.0688\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0067 - mae: 0.0501 - mean_squared_error: 0.0067 - val_loss: 0.0688 - val_mae: 0.1875 - val_mean_squared_error: 0.0681\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0062 - mae: 0.0483 - mean_squared_error: 0.0062 - val_loss: 0.0734 - val_mae: 0.1897 - val_mean_squared_error: 0.0725\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0064 - mae: 0.0482 - mean_squared_error: 0.0064 - val_loss: 0.0769 - val_mae: 0.1947 - val_mean_squared_error: 0.0759\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0062 - mae: 0.0475 - mean_squared_error: 0.0062 - val_loss: 0.0752 - val_mae: 0.1921 - val_mean_squared_error: 0.0741\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0061 - mae: 0.0467 - mean_squared_error: 0.0061 - val_loss: 0.0746 - val_mae: 0.1909 - val_mean_squared_error: 0.0736\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0062 - mae: 0.0481 - mean_squared_error: 0.0062 - val_loss: 0.0699 - val_mae: 0.1869 - val_mean_squared_error: 0.0691\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0066 - mae: 0.0493 - mean_squared_error: 0.0066 - val_loss: 0.0691 - val_mae: 0.1860 - val_mean_squared_error: 0.0682\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0061 - mae: 0.0475 - mean_squared_error: 0.0061 - val_loss: 0.0781 - val_mae: 0.1970 - val_mean_squared_error: 0.0772\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0459 - mean_squared_error: 0.0059 - val_loss: 0.0772 - val_mae: 0.1951 - val_mean_squared_error: 0.0762\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0063 - mae: 0.0467 - mean_squared_error: 0.0063 - val_loss: 0.0698 - val_mae: 0.1857 - val_mean_squared_error: 0.0689\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0446 - mean_squared_error: 0.0058 - val_loss: 0.0733 - val_mae: 0.1891 - val_mean_squared_error: 0.0721\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0446 - mean_squared_error: 0.0059 - val_loss: 0.0740 - val_mae: 0.1902 - val_mean_squared_error: 0.0730\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0444 - mean_squared_error: 0.0059 - val_loss: 0.0713 - val_mae: 0.1869 - val_mean_squared_error: 0.0704\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0438 - mean_squared_error: 0.0052 - val_loss: 0.0741 - val_mae: 0.1906 - val_mean_squared_error: 0.0733\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0061 - mae: 0.0476 - mean_squared_error: 0.0061 - val_loss: 0.0742 - val_mae: 0.1905 - val_mean_squared_error: 0.0733\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0061 - mae: 0.0457 - mean_squared_error: 0.0061 - val_loss: 0.0723 - val_mae: 0.1873 - val_mean_squared_error: 0.0713\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0449 - mean_squared_error: 0.0057 - val_loss: 0.0678 - val_mae: 0.1846 - val_mean_squared_error: 0.0670\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0060 - mae: 0.0469 - mean_squared_error: 0.0060 - val_loss: 0.0745 - val_mae: 0.1906 - val_mean_squared_error: 0.0735\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0449 - mean_squared_error: 0.0058 - val_loss: 0.0720 - val_mae: 0.1875 - val_mean_squared_error: 0.0711\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0440 - mean_squared_error: 0.0058 - val_loss: 0.0724 - val_mae: 0.1880 - val_mean_squared_error: 0.0716\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0051 - mae: 0.0428 - mean_squared_error: 0.0051 - val_loss: 0.0740 - val_mae: 0.1898 - val_mean_squared_error: 0.0730\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0061 - mae: 0.0466 - mean_squared_error: 0.0061 - val_loss: 0.0717 - val_mae: 0.1867 - val_mean_squared_error: 0.0709\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0454 - mean_squared_error: 0.0058 - val_loss: 0.0736 - val_mae: 0.1895 - val_mean_squared_error: 0.0727\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0434 - mean_squared_error: 0.0052 - val_loss: 0.0708 - val_mae: 0.1855 - val_mean_squared_error: 0.0697\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0051 - mae: 0.0431 - mean_squared_error: 0.0051 - val_loss: 0.0705 - val_mae: 0.1855 - val_mean_squared_error: 0.0696\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0439 - mean_squared_error: 0.0057 - val_loss: 0.0705 - val_mae: 0.1857 - val_mean_squared_error: 0.0697\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0054 - mae: 0.0426 - mean_squared_error: 0.0054 - val_loss: 0.0724 - val_mae: 0.1873 - val_mean_squared_error: 0.0713\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0051 - mae: 0.0433 - mean_squared_error: 0.0051 - val_loss: 0.0673 - val_mae: 0.1825 - val_mean_squared_error: 0.0665\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0054 - mae: 0.0443 - mean_squared_error: 0.0054 - val_loss: 0.0744 - val_mae: 0.1905 - val_mean_squared_error: 0.0733\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0053 - mae: 0.0414 - mean_squared_error: 0.0053 - val_loss: 0.0754 - val_mae: 0.1915 - val_mean_squared_error: 0.0743\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs_hist = baseline_model.fit(\n",
    "    NLP_df_train, # input\n",
    "    y_wr_train, # output\n",
    "    epochs=100, # number of iterations\n",
    "    batch_size=50, # number of observations taken to train the data\n",
    "    verbose=1,\n",
    "    validation_data = (NLP_df_test, y_wr_test),\n",
    "    shuffle = True\n",
    "    #validation_split=0.2,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate intermediate description features with lower dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m670/670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict baseline X train and X test \n",
    "\n",
    "NLP_intermediate_train = baseline_model.predict(NLP_df_train)\n",
    "NLP_intermediate_test = baseline_model.predict(NLP_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.4089246, 4.420882 , 4.4168897, ..., 4.41385  , 4.417997 ,\n",
       "        4.416403 ],\n",
       "       [4.2271676, 4.231065 , 4.2289457, ..., 4.224927 , 4.231066 ,\n",
       "        4.2293634],\n",
       "       [4.1883283, 4.194393 , 4.1940336, ..., 4.195767 , 4.191303 ,\n",
       "        4.189158 ],\n",
       "       ...,\n",
       "       [4.00177  , 3.984882 , 3.9928422, ..., 3.9939249, 3.9958043,\n",
       "        3.9959364],\n",
       "       [4.3644314, 4.3600974, 4.362086 , ..., 4.3642035, 4.3628893,\n",
       "        4.363909 ],\n",
       "       [4.200646 , 4.206281 , 4.2025747, ..., 4.202437 , 4.211825 ,\n",
       "        4.1969256]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_intermediate_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NLP_intermediate_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store these into a dataframe\n",
    "NLP_intermediate_train_df = pd.DataFrame(NLP_intermediate_train, index=NLP_df_train.index)\n",
    "NLP_intermediate_test_df = pd.DataFrame(NLP_intermediate_test, index=NLP_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.408925</td>\n",
       "      <td>4.420882</td>\n",
       "      <td>4.416890</td>\n",
       "      <td>4.418366</td>\n",
       "      <td>4.417753</td>\n",
       "      <td>4.417819</td>\n",
       "      <td>4.419184</td>\n",
       "      <td>4.419909</td>\n",
       "      <td>4.414232</td>\n",
       "      <td>4.415713</td>\n",
       "      <td>...</td>\n",
       "      <td>4.419310</td>\n",
       "      <td>4.408286</td>\n",
       "      <td>4.422114</td>\n",
       "      <td>4.418770</td>\n",
       "      <td>4.415804</td>\n",
       "      <td>4.421103</td>\n",
       "      <td>4.423278</td>\n",
       "      <td>4.413850</td>\n",
       "      <td>4.417997</td>\n",
       "      <td>4.416403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.227168</td>\n",
       "      <td>4.231065</td>\n",
       "      <td>4.228946</td>\n",
       "      <td>4.230300</td>\n",
       "      <td>4.231961</td>\n",
       "      <td>4.235414</td>\n",
       "      <td>4.234513</td>\n",
       "      <td>4.230048</td>\n",
       "      <td>4.229636</td>\n",
       "      <td>4.231685</td>\n",
       "      <td>...</td>\n",
       "      <td>4.236648</td>\n",
       "      <td>4.226399</td>\n",
       "      <td>4.229749</td>\n",
       "      <td>4.232564</td>\n",
       "      <td>4.227556</td>\n",
       "      <td>4.231503</td>\n",
       "      <td>4.229834</td>\n",
       "      <td>4.224927</td>\n",
       "      <td>4.231066</td>\n",
       "      <td>4.229363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.188328</td>\n",
       "      <td>4.194393</td>\n",
       "      <td>4.194034</td>\n",
       "      <td>4.185140</td>\n",
       "      <td>4.193111</td>\n",
       "      <td>4.193242</td>\n",
       "      <td>4.191313</td>\n",
       "      <td>4.192410</td>\n",
       "      <td>4.186793</td>\n",
       "      <td>4.191766</td>\n",
       "      <td>...</td>\n",
       "      <td>4.193043</td>\n",
       "      <td>4.191612</td>\n",
       "      <td>4.194394</td>\n",
       "      <td>4.195267</td>\n",
       "      <td>4.187212</td>\n",
       "      <td>4.196862</td>\n",
       "      <td>4.197066</td>\n",
       "      <td>4.195767</td>\n",
       "      <td>4.191303</td>\n",
       "      <td>4.189158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.525066</td>\n",
       "      <td>4.526392</td>\n",
       "      <td>4.522406</td>\n",
       "      <td>4.529515</td>\n",
       "      <td>4.528418</td>\n",
       "      <td>4.531471</td>\n",
       "      <td>4.528793</td>\n",
       "      <td>4.527687</td>\n",
       "      <td>4.526726</td>\n",
       "      <td>4.536198</td>\n",
       "      <td>...</td>\n",
       "      <td>4.529837</td>\n",
       "      <td>4.530842</td>\n",
       "      <td>4.525315</td>\n",
       "      <td>4.529678</td>\n",
       "      <td>4.528812</td>\n",
       "      <td>4.528993</td>\n",
       "      <td>4.534094</td>\n",
       "      <td>4.525110</td>\n",
       "      <td>4.526510</td>\n",
       "      <td>4.527074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4.337204</td>\n",
       "      <td>4.342837</td>\n",
       "      <td>4.341679</td>\n",
       "      <td>4.339909</td>\n",
       "      <td>4.336855</td>\n",
       "      <td>4.340528</td>\n",
       "      <td>4.337700</td>\n",
       "      <td>4.337024</td>\n",
       "      <td>4.336594</td>\n",
       "      <td>4.337668</td>\n",
       "      <td>...</td>\n",
       "      <td>4.342944</td>\n",
       "      <td>4.339314</td>\n",
       "      <td>4.341485</td>\n",
       "      <td>4.337862</td>\n",
       "      <td>4.339727</td>\n",
       "      <td>4.343600</td>\n",
       "      <td>4.340711</td>\n",
       "      <td>4.333910</td>\n",
       "      <td>4.343233</td>\n",
       "      <td>4.339843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>4.445068</td>\n",
       "      <td>4.436575</td>\n",
       "      <td>4.432493</td>\n",
       "      <td>4.443172</td>\n",
       "      <td>4.436512</td>\n",
       "      <td>4.438483</td>\n",
       "      <td>4.444669</td>\n",
       "      <td>4.450074</td>\n",
       "      <td>4.446179</td>\n",
       "      <td>4.442899</td>\n",
       "      <td>...</td>\n",
       "      <td>4.453914</td>\n",
       "      <td>4.448549</td>\n",
       "      <td>4.443313</td>\n",
       "      <td>4.438620</td>\n",
       "      <td>4.446571</td>\n",
       "      <td>4.438053</td>\n",
       "      <td>4.443110</td>\n",
       "      <td>4.444717</td>\n",
       "      <td>4.442869</td>\n",
       "      <td>4.436579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>4.096858</td>\n",
       "      <td>4.100168</td>\n",
       "      <td>4.090538</td>\n",
       "      <td>4.097077</td>\n",
       "      <td>4.097855</td>\n",
       "      <td>4.086901</td>\n",
       "      <td>4.091972</td>\n",
       "      <td>4.088095</td>\n",
       "      <td>4.093302</td>\n",
       "      <td>4.093448</td>\n",
       "      <td>...</td>\n",
       "      <td>4.098001</td>\n",
       "      <td>4.095216</td>\n",
       "      <td>4.087335</td>\n",
       "      <td>4.103864</td>\n",
       "      <td>4.084136</td>\n",
       "      <td>4.091631</td>\n",
       "      <td>4.090065</td>\n",
       "      <td>4.091859</td>\n",
       "      <td>4.090687</td>\n",
       "      <td>4.088017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>4.001770</td>\n",
       "      <td>3.984882</td>\n",
       "      <td>3.992842</td>\n",
       "      <td>3.992184</td>\n",
       "      <td>3.985528</td>\n",
       "      <td>3.998158</td>\n",
       "      <td>4.007915</td>\n",
       "      <td>3.994741</td>\n",
       "      <td>3.993707</td>\n",
       "      <td>3.998885</td>\n",
       "      <td>...</td>\n",
       "      <td>3.997579</td>\n",
       "      <td>4.003650</td>\n",
       "      <td>3.992235</td>\n",
       "      <td>3.993426</td>\n",
       "      <td>4.008648</td>\n",
       "      <td>3.990647</td>\n",
       "      <td>4.016080</td>\n",
       "      <td>3.993925</td>\n",
       "      <td>3.995804</td>\n",
       "      <td>3.995936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>4.364431</td>\n",
       "      <td>4.360097</td>\n",
       "      <td>4.362086</td>\n",
       "      <td>4.358554</td>\n",
       "      <td>4.364618</td>\n",
       "      <td>4.365531</td>\n",
       "      <td>4.364735</td>\n",
       "      <td>4.365673</td>\n",
       "      <td>4.358621</td>\n",
       "      <td>4.364763</td>\n",
       "      <td>...</td>\n",
       "      <td>4.368271</td>\n",
       "      <td>4.357871</td>\n",
       "      <td>4.366419</td>\n",
       "      <td>4.363382</td>\n",
       "      <td>4.362494</td>\n",
       "      <td>4.364899</td>\n",
       "      <td>4.362829</td>\n",
       "      <td>4.364203</td>\n",
       "      <td>4.362889</td>\n",
       "      <td>4.363909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>4.200646</td>\n",
       "      <td>4.206281</td>\n",
       "      <td>4.202575</td>\n",
       "      <td>4.205654</td>\n",
       "      <td>4.202597</td>\n",
       "      <td>4.212648</td>\n",
       "      <td>4.207676</td>\n",
       "      <td>4.208747</td>\n",
       "      <td>4.204621</td>\n",
       "      <td>4.209055</td>\n",
       "      <td>...</td>\n",
       "      <td>4.210135</td>\n",
       "      <td>4.202863</td>\n",
       "      <td>4.210028</td>\n",
       "      <td>4.207239</td>\n",
       "      <td>4.212411</td>\n",
       "      <td>4.213313</td>\n",
       "      <td>4.205900</td>\n",
       "      <td>4.202437</td>\n",
       "      <td>4.211825</td>\n",
       "      <td>4.196926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "index                                                                          \n",
       "3       4.408925  4.420882  4.416890  4.418366  4.417753  4.417819  4.419184   \n",
       "24      4.227168  4.231065  4.228946  4.230300  4.231961  4.235414  4.234513   \n",
       "42      4.188328  4.194393  4.194034  4.185140  4.193111  4.193242  4.191313   \n",
       "49      4.525066  4.526392  4.522406  4.529515  4.528418  4.531471  4.528793   \n",
       "73      4.337204  4.342837  4.341679  4.339909  4.336855  4.340528  4.337700   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  4.445068  4.436575  4.432493  4.443172  4.436512  4.438483  4.444669   \n",
       "212365  4.096858  4.100168  4.090538  4.097077  4.097855  4.086901  4.091972   \n",
       "212394  4.001770  3.984882  3.992842  3.992184  3.985528  3.998158  4.007915   \n",
       "212399  4.364431  4.360097  4.362086  4.358554  4.364618  4.365531  4.364735   \n",
       "212402  4.200646  4.206281  4.202575  4.205654  4.202597  4.212648  4.207676   \n",
       "\n",
       "             7         8         9    ...       246       247       248  \\\n",
       "index                                 ...                                 \n",
       "3       4.419909  4.414232  4.415713  ...  4.419310  4.408286  4.422114   \n",
       "24      4.230048  4.229636  4.231685  ...  4.236648  4.226399  4.229749   \n",
       "42      4.192410  4.186793  4.191766  ...  4.193043  4.191612  4.194394   \n",
       "49      4.527687  4.526726  4.536198  ...  4.529837  4.530842  4.525315   \n",
       "73      4.337024  4.336594  4.337668  ...  4.342944  4.339314  4.341485   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  4.450074  4.446179  4.442899  ...  4.453914  4.448549  4.443313   \n",
       "212365  4.088095  4.093302  4.093448  ...  4.098001  4.095216  4.087335   \n",
       "212394  3.994741  3.993707  3.998885  ...  3.997579  4.003650  3.992235   \n",
       "212399  4.365673  4.358621  4.364763  ...  4.368271  4.357871  4.366419   \n",
       "212402  4.208747  4.204621  4.209055  ...  4.210135  4.202863  4.210028   \n",
       "\n",
       "             249       250       251       252       253       254       255  \n",
       "index                                                                         \n",
       "3       4.418770  4.415804  4.421103  4.423278  4.413850  4.417997  4.416403  \n",
       "24      4.232564  4.227556  4.231503  4.229834  4.224927  4.231066  4.229363  \n",
       "42      4.195267  4.187212  4.196862  4.197066  4.195767  4.191303  4.189158  \n",
       "49      4.529678  4.528812  4.528993  4.534094  4.525110  4.526510  4.527074  \n",
       "73      4.337862  4.339727  4.343600  4.340711  4.333910  4.343233  4.339843  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "212361  4.438620  4.446571  4.438053  4.443110  4.444717  4.442869  4.436579  \n",
       "212365  4.103864  4.084136  4.091631  4.090065  4.091859  4.090687  4.088017  \n",
       "212394  3.993426  4.008648  3.990647  4.016080  3.993925  3.995804  3.995936  \n",
       "212399  4.363382  4.362494  4.364899  4.362829  4.364203  4.362889  4.363909  \n",
       "212402  4.207239  4.212411  4.213313  4.205900  4.202437  4.211825  4.196926  \n",
       "\n",
       "[21419 rows x 256 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that indices are correct\n",
    "NLP_intermediate_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y cuts\n",
    "We are going to run two models for two target variables\n",
    "- Target variable: Average rating\n",
    "  - baseline (i.e. excluding image embeddings)\n",
    "  - including image embeddings\n",
    "- Target variable: weighted rating\n",
    "  - baseline (i.e. excluding image embeddings)\n",
    "  - including image embeddings\n",
    "\n",
    "We therefore need to create the following datsets\n",
    "- X train and X test with embeddings\n",
    "- X train and X text without embeddings\n",
    "- y train and y test using average rating\n",
    "- y train and y test using weighted rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\t Dense\t Dropout\t NLP_df_test\t NLP_df_train\t NLP_intermediate_test\t NLP_intermediate_test_df\t NLP_intermediate_train\t NLP_intermediate_train_df\t \n",
      "Pipeline\t RandomForestRegressor\t SGD\t SVR\t StandardScaler\t X\t X_columns\t X_images_test\t X_images_train\t \n",
      "X_test\t X_train\t analysis_folder\t baseline_model\t columns_to_keep\t date\t dates_columns\t epochs_hist\t folders_set_up\t \n",
      "input_folder\t input_shape\t keras\t layers\t mean_absolute_error\t mean_squared_error\t n_neurons\t np\t os\t \n",
      "output_folder\t pd\t plt\t sns\t test_indices\t time\t title_embeddings_df\t train_indices\t train_test_split\t \n",
      "tree\t y\t y_avg_r_test\t y_avg_r_train\t y_test\t y_train\t y_wr_test\t y_wr_train\t \n"
     ]
    }
   ],
   "source": [
    "%who\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model data for SVR\n",
    "X_baseline_train = pd.merge(\n",
    "    NLP_df_train,\n",
    "    X_train['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "\n",
    "X_baseline_test = pd.merge(\n",
    "    NLP_df_test,\n",
    "    X_test['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2992  tSVD2993  tSVD2994  \\\n",
       "3       0.006327  0.040408  0.035931  ... -0.012096  0.012755  0.004345   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.002294 -0.002052 -0.013243   \n",
       "42      0.042483  0.019502  0.016915  ... -0.007230 -0.005164  0.000416   \n",
       "49     -0.027805 -0.038459 -0.016390  ... -0.003206  0.011330 -0.004898   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.003473 -0.013758  0.000954   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.000571  0.001393 -0.001693   \n",
       "212365 -0.028389  0.026683  0.003763  ... -0.010071 -0.008840  0.011693   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.005601  0.000670 -0.010949   \n",
       "212399 -0.028692 -0.017410  0.020350  ...  0.005578 -0.000719 -0.004401   \n",
       "212402 -0.004967 -0.007246 -0.009244  ...  0.017672  0.005686  0.007956   \n",
       "\n",
       "        tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000    year  \n",
       "3       0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  2005.0  \n",
       "24     -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  2001.0  \n",
       "42     -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  2002.0  \n",
       "49     -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  1997.0  \n",
       "73     -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  2016.0  \n",
       "...          ...       ...       ...       ...       ...       ...     ...  \n",
       "212361 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  2009.0  \n",
       "212365  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  1998.0  \n",
       "212394 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  2015.0  \n",
       "212399  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  2000.0  \n",
       "212402 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  2003.0  \n",
       "\n",
       "[21419 rows x 3001 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21419, 3001)\n",
      "(5355, 3001)\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline_train.shape)\n",
    "print(X_baseline_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2992  tSVD2993  tSVD2994  \\\n",
       "3       0.006327  0.040408  0.035931  ... -0.012096  0.012755  0.004345   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.002294 -0.002052 -0.013243   \n",
       "42      0.042483  0.019502  0.016915  ... -0.007230 -0.005164  0.000416   \n",
       "49     -0.027805 -0.038459 -0.016390  ... -0.003206  0.011330 -0.004898   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.003473 -0.013758  0.000954   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.000571  0.001393 -0.001693   \n",
       "212365 -0.028389  0.026683  0.003763  ... -0.010071 -0.008840  0.011693   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.005601  0.000670 -0.010949   \n",
       "212399 -0.028692 -0.017410  0.020350  ...  0.005578 -0.000719 -0.004401   \n",
       "212402 -0.004967 -0.007246 -0.009244  ...  0.017672  0.005686  0.007956   \n",
       "\n",
       "        tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000    year  \n",
       "3       0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  2005.0  \n",
       "24     -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  2001.0  \n",
       "42     -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  2002.0  \n",
       "49     -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  1997.0  \n",
       "73     -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  2016.0  \n",
       "...          ...       ...       ...       ...       ...       ...     ...  \n",
       "212361 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  2009.0  \n",
       "212365  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  1998.0  \n",
       "212394 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  2015.0  \n",
       "212399  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  2000.0  \n",
       "212402 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  2003.0  \n",
       "\n",
       "[21419 rows x 3001 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With image embeddings\n",
    "X_final_train = pd.merge(\n",
    "    X_images_train,\n",
    "    X_baseline_train,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "    \n",
    "X_final_test = pd.merge(\n",
    "    X_images_test,\n",
    "    X_baseline_test,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_0', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5',\n",
       "       'image_6', 'image_7', 'image_8', 'image_9',\n",
       "       ...\n",
       "       'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996', 'tSVD2997',\n",
       "       'tSVD2998', 'tSVD2999', 'tSVD3000', 'year'],\n",
       "      dtype='object', length=3257)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model data for NN\n",
    "X_baseline_train_NN = NLP_intermediate_train_df\n",
    "X_baseline_test_NN = NLP_intermediate_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack description + publish year and images embeddings\n",
    "\n",
    "X_final_train_NN = pd.merge(\n",
    "    X_baseline_train_NN, \n",
    "    X_images_train, \n",
    "    left_index = True, \n",
    "    right_index = True)\n",
    "\n",
    "X_final_test_NN = pd.merge(\n",
    "    X_baseline_test_NN, \n",
    "    X_images_test, \n",
    "    left_index = True, \n",
    "    right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression & co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "\n",
    "# Random forest\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# SVR\n",
    "svr_model = SVR(kernel='rbf')  # 'rbf' for radial basis function kernel\n",
    "\n",
    "# Lightgbm\n",
    "\n",
    "\n",
    "# Define pipeline steps\n",
    "rf_pipeline = Pipeline([\n",
    "    ('rf', rf)  # Random Forest classifier\n",
    "])\n",
    "\n",
    "svr_pipeline = Pipeline([\n",
    "    ('svr', svr_model)  # Neural Network classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  \\\n",
       "Baseline Support Vector Regression  (SVR())   \n",
       "Final Support Vector Regression     (SVR())   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                   prediction   MAE   MSE  \n",
       "Baseline Support Vector Regression       None  None  None  \n",
       "Final Support Vector Regression          None  None  None  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "evaluation_metrics = pd.DataFrame({\n",
    "    #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "    'Baseline Support Vector Regression': {'model': svr_pipeline, 'X_train': X_baseline_train, 'X_test' : X_baseline_test, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Support Vector Regression': {'model': svr_pipeline, 'X_train': X_final_train, 'X_test' : X_final_test, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "}).transpose()\n",
    "\n",
    "evaluation_metrics = evaluation_metrics.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Support Vector Regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Training completed. Duration: 02:10\n",
      "> Predictions completed. Duration: 28540518:25\n",
      "> Evaluation completed. Duration: 02:10\n",
      "Total time taken: 03:03\n",
      "\n",
      "\n",
      "Final Support Vector Regression\n",
      "> Training completed. Duration: 05:17\n",
      "> Predictions completed. Duration: 28540518:26\n",
      "> Evaluation completed. Duration: 05:17\n",
      "Total time taken: 06:11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics.iterrows():\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "    # Call model\n",
    "    model = row['model']\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(row['X_train'], y_wr_train)\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate predictions\n",
    "    y_wr_pred = model.predict(row['X_test'])\n",
    "\n",
    "    # save predictions\n",
    "    row['prediction'] = y_wr_pred\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Predictions completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_wr_test, y_wr_pred)\n",
    "    mae = mean_absolute_error(y_wr_test, y_wr_pred)\n",
    "\n",
    "    # Save metrics\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\">> Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>[4.26317910221547, 4.263480602359525, 4.262913...</td>\n",
       "      <td>0.13147</td>\n",
       "      <td>0.045265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.269796558491903, 4.251500951120229, 4.26040...</td>\n",
       "      <td>0.131182</td>\n",
       "      <td>0.044787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  \\\n",
       "Baseline Support Vector Regression  (SVR())   \n",
       "Final Support Vector Regression     (SVR())   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.26317910221547, 4.263480602359525, 4.262913...   \n",
       "Final Support Vector Regression     [4.269796558491903, 4.251500951120229, 4.26040...   \n",
       "\n",
       "                                         MAE       MSE  \n",
       "Baseline Support Vector Regression   0.13147  0.045265  \n",
       "Final Support Vector Regression     0.131182  0.044787  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>None</td>\n",
       "      <td>0         1         2         3  ...</td>\n",
       "      <td>0         1         2         3  ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>None</td>\n",
       "      <td>0         1         2         3...</td>\n",
       "      <td>0         1         2         3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  \\\n",
       "Baseline Neural Network  None   \n",
       "Final Neural Network     None   \n",
       "\n",
       "                                                                   X_train  \\\n",
       "Baseline Neural Network               0         1         2         3  ...   \n",
       "Final Neural Network                    0         1         2         3...   \n",
       "\n",
       "                                                                    X_test  \\\n",
       "Baseline Neural Network               0         1         2         3  ...   \n",
       "Final Neural Network                    0         1         2         3...   \n",
       "\n",
       "                        prediction   MAE   MSE  \n",
       "Baseline Neural Network       None  None  None  \n",
       "Final Neural Network          None  None  None  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "evaluation_metrics_NN = pd.DataFrame({\n",
    "    #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "    'Baseline Neural Network': {'model': None, 'X_train': X_baseline_train_NN, 'X_test' : X_baseline_test_NN, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Neural Network': {'model': None, 'X_train': X_final_train_NN, 'X_test' : X_final_test_NN, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "}).transpose()\n",
    "\n",
    "evaluation_metrics_NN = evaluation_metrics_NN.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Neural Network\n",
      "> Input shape: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │        \u001b[38;5;34m15,934\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">278,909</span> (1.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m278,909\u001b[0m (1.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">278,909</span> (1.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m278,909\u001b[0m (1.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0489 - mae: 0.9263 - mean_squared_error: 2.0489 - val_loss: 0.9012 - val_mae: 0.9250 - val_mean_squared_error: 0.9026\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3879 - mae: 0.4986 - mean_squared_error: 0.3879 - val_loss: 1.6791 - val_mae: 1.2778 - val_mean_squared_error: 1.6815\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2966 - mae: 0.4344 - mean_squared_error: 0.2966 - val_loss: 0.8272 - val_mae: 0.8866 - val_mean_squared_error: 0.8285\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2570 - mae: 0.4040 - mean_squared_error: 0.2570 - val_loss: 0.9139 - val_mae: 0.9346 - val_mean_squared_error: 0.9153\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2250 - mae: 0.3780 - mean_squared_error: 0.2250 - val_loss: 1.1128 - val_mae: 1.0355 - val_mean_squared_error: 1.1144\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2061 - mae: 0.3603 - mean_squared_error: 0.2061 - val_loss: 0.4390 - val_mae: 0.6379 - val_mean_squared_error: 0.4394\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1815 - mae: 0.3385 - mean_squared_error: 0.1815 - val_loss: 0.5359 - val_mae: 0.7094 - val_mean_squared_error: 0.5366\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1599 - mae: 0.3170 - mean_squared_error: 0.1599 - val_loss: 0.3895 - val_mae: 0.5996 - val_mean_squared_error: 0.3898\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1514 - mae: 0.3079 - mean_squared_error: 0.1514 - val_loss: 0.4114 - val_mae: 0.6176 - val_mean_squared_error: 0.4118\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1394 - mae: 0.2965 - mean_squared_error: 0.1394 - val_loss: 0.5003 - val_mae: 0.6849 - val_mean_squared_error: 0.5008\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1335 - mae: 0.2896 - mean_squared_error: 0.1335 - val_loss: 0.2842 - val_mae: 0.5073 - val_mean_squared_error: 0.2842\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1249 - mae: 0.2795 - mean_squared_error: 0.1249 - val_loss: 0.3187 - val_mae: 0.5396 - val_mean_squared_error: 0.3187\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1129 - mae: 0.2663 - mean_squared_error: 0.1129 - val_loss: 0.0999 - val_mae: 0.2782 - val_mean_squared_error: 0.0991\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1090 - mae: 0.2603 - mean_squared_error: 0.1090 - val_loss: 0.2284 - val_mae: 0.4507 - val_mean_squared_error: 0.2281\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1013 - mae: 0.2529 - mean_squared_error: 0.1013 - val_loss: 0.2049 - val_mae: 0.4247 - val_mean_squared_error: 0.2046\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0929 - mae: 0.2400 - mean_squared_error: 0.0929 - val_loss: 0.2157 - val_mae: 0.4368 - val_mean_squared_error: 0.2154\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0874 - mae: 0.2313 - mean_squared_error: 0.0874 - val_loss: 0.2444 - val_mae: 0.4679 - val_mean_squared_error: 0.2442\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0809 - mae: 0.2219 - mean_squared_error: 0.0809 - val_loss: 0.2479 - val_mae: 0.4717 - val_mean_squared_error: 0.2477\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0766 - mae: 0.2160 - mean_squared_error: 0.0766 - val_loss: 0.1802 - val_mae: 0.3951 - val_mean_squared_error: 0.1798\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0704 - mae: 0.2053 - mean_squared_error: 0.0704 - val_loss: 0.2038 - val_mae: 0.4233 - val_mean_squared_error: 0.2034\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0649 - mae: 0.1974 - mean_squared_error: 0.0649 - val_loss: 0.2571 - val_mae: 0.4808 - val_mean_squared_error: 0.2569\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0599 - mae: 0.1902 - mean_squared_error: 0.0599 - val_loss: 0.3482 - val_mae: 0.5658 - val_mean_squared_error: 0.3483\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0563 - mae: 0.1838 - mean_squared_error: 0.0563 - val_loss: 0.2604 - val_mae: 0.4839 - val_mean_squared_error: 0.2603\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0484 - mae: 0.1707 - mean_squared_error: 0.0484 - val_loss: 0.3717 - val_mae: 0.5862 - val_mean_squared_error: 0.3719\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0447 - mae: 0.1629 - mean_squared_error: 0.0447 - val_loss: 0.3660 - val_mae: 0.5814 - val_mean_squared_error: 0.3662\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1633 - mean_squared_error: 0.0445 - val_loss: 0.4358 - val_mae: 0.6377 - val_mean_squared_error: 0.4361\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0388 - mae: 0.1520 - mean_squared_error: 0.0388 - val_loss: 0.6654 - val_mae: 0.7953 - val_mean_squared_error: 0.6662\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0330 - mae: 0.1396 - mean_squared_error: 0.0330 - val_loss: 0.8347 - val_mae: 0.8940 - val_mean_squared_error: 0.8359\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0312 - mae: 0.1353 - mean_squared_error: 0.0312 - val_loss: 0.9101 - val_mae: 0.9349 - val_mean_squared_error: 0.9114\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0289 - mae: 0.1279 - mean_squared_error: 0.0289 - val_loss: 0.8221 - val_mae: 0.8867 - val_mean_squared_error: 0.8233\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0256 - mae: 0.1219 - mean_squared_error: 0.0256 - val_loss: 0.8762 - val_mae: 0.9167 - val_mean_squared_error: 0.8774\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0278 - mae: 0.1245 - mean_squared_error: 0.0278 - val_loss: 1.0328 - val_mae: 0.9978 - val_mean_squared_error: 1.0343\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0233 - mae: 0.1120 - mean_squared_error: 0.0233 - val_loss: 1.1061 - val_mae: 1.0337 - val_mean_squared_error: 1.1076\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0228 - mae: 0.1132 - mean_squared_error: 0.0228 - val_loss: 0.8669 - val_mae: 0.9113 - val_mean_squared_error: 0.8681\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0227 - mae: 0.1112 - mean_squared_error: 0.0227 - val_loss: 1.0478 - val_mae: 1.0048 - val_mean_squared_error: 1.0493\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0220 - mae: 0.1101 - mean_squared_error: 0.0220 - val_loss: 1.1639 - val_mae: 1.0611 - val_mean_squared_error: 1.1656\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0182 - mae: 0.0974 - mean_squared_error: 0.0182 - val_loss: 0.9598 - val_mae: 0.9598 - val_mean_squared_error: 0.9612\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0187 - mae: 0.1001 - mean_squared_error: 0.0187 - val_loss: 1.0383 - val_mae: 0.9999 - val_mean_squared_error: 1.0398\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0175 - mae: 0.0951 - mean_squared_error: 0.0175 - val_loss: 0.9199 - val_mae: 0.9388 - val_mean_squared_error: 0.9212\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0183 - mae: 0.0963 - mean_squared_error: 0.0183 - val_loss: 0.7446 - val_mae: 0.8408 - val_mean_squared_error: 0.7457\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0165 - mae: 0.0908 - mean_squared_error: 0.0165 - val_loss: 0.6224 - val_mae: 0.7653 - val_mean_squared_error: 0.6232\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0176 - mae: 0.0949 - mean_squared_error: 0.0176 - val_loss: 1.3229 - val_mae: 1.1326 - val_mean_squared_error: 1.3248\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0157 - mae: 0.0876 - mean_squared_error: 0.0157 - val_loss: 0.9724 - val_mae: 0.9630 - val_mean_squared_error: 0.9738\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0179 - mae: 0.0936 - mean_squared_error: 0.0179 - val_loss: 0.9141 - val_mae: 0.9355 - val_mean_squared_error: 0.9154\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0157 - mae: 0.0872 - mean_squared_error: 0.0157 - val_loss: 0.9279 - val_mae: 0.9427 - val_mean_squared_error: 0.9293\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0150 - mae: 0.0840 - mean_squared_error: 0.0150 - val_loss: 0.9248 - val_mae: 0.9410 - val_mean_squared_error: 0.9261\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0168 - mae: 0.0899 - mean_squared_error: 0.0168 - val_loss: 1.1693 - val_mae: 1.0621 - val_mean_squared_error: 1.1710\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0144 - mae: 0.0829 - mean_squared_error: 0.0144 - val_loss: 0.9266 - val_mae: 0.9411 - val_mean_squared_error: 0.9280\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0159 - mae: 0.0855 - mean_squared_error: 0.0159 - val_loss: 1.2082 - val_mae: 1.0795 - val_mean_squared_error: 1.2099\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0156 - mae: 0.0864 - mean_squared_error: 0.0156 - val_loss: 1.3566 - val_mae: 1.1468 - val_mean_squared_error: 1.3585\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0134 - mae: 0.0778 - mean_squared_error: 0.0134 - val_loss: 1.2461 - val_mae: 1.0977 - val_mean_squared_error: 1.2479\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0142 - mae: 0.0828 - mean_squared_error: 0.0142 - val_loss: 1.0890 - val_mae: 1.0228 - val_mean_squared_error: 1.0906\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0168 - mae: 0.0911 - mean_squared_error: 0.0168 - val_loss: 0.9658 - val_mae: 0.9599 - val_mean_squared_error: 0.9673\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0139 - mae: 0.0804 - mean_squared_error: 0.0139 - val_loss: 0.9546 - val_mae: 0.9537 - val_mean_squared_error: 0.9561\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0142 - mae: 0.0832 - mean_squared_error: 0.0142 - val_loss: 0.6971 - val_mae: 0.8101 - val_mean_squared_error: 0.6982\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0137 - mae: 0.0793 - mean_squared_error: 0.0137 - val_loss: 0.8334 - val_mae: 0.8903 - val_mean_squared_error: 0.8347\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0144 - mae: 0.0805 - mean_squared_error: 0.0144 - val_loss: 1.0784 - val_mae: 1.0173 - val_mean_squared_error: 1.0799\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0156 - mae: 0.0862 - mean_squared_error: 0.0156 - val_loss: 1.0685 - val_mae: 1.0114 - val_mean_squared_error: 1.0702\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0161 - mae: 0.0883 - mean_squared_error: 0.0161 - val_loss: 0.6201 - val_mae: 0.7613 - val_mean_squared_error: 0.6210\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0143 - mae: 0.0818 - mean_squared_error: 0.0143 - val_loss: 0.7710 - val_mae: 0.8522 - val_mean_squared_error: 0.7722\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0137 - mae: 0.0798 - mean_squared_error: 0.0137 - val_loss: 0.9216 - val_mae: 0.9357 - val_mean_squared_error: 0.9231\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0139 - mae: 0.0781 - mean_squared_error: 0.0139 - val_loss: 1.1456 - val_mae: 1.0501 - val_mean_squared_error: 1.1473\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0139 - mae: 0.0808 - mean_squared_error: 0.0139 - val_loss: 1.0835 - val_mae: 1.0185 - val_mean_squared_error: 1.0852\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0137 - mae: 0.0786 - mean_squared_error: 0.0137 - val_loss: 0.6416 - val_mae: 0.7757 - val_mean_squared_error: 0.6426\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129 - mae: 0.0762 - mean_squared_error: 0.0130 - val_loss: 1.6144 - val_mae: 1.2539 - val_mean_squared_error: 1.6166\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0134 - mae: 0.0790 - mean_squared_error: 0.0134 - val_loss: 0.7268 - val_mae: 0.8290 - val_mean_squared_error: 0.7278\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0126 - mae: 0.0746 - mean_squared_error: 0.0126 - val_loss: 0.9631 - val_mae: 0.9587 - val_mean_squared_error: 0.9646\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0125 - mae: 0.0757 - mean_squared_error: 0.0125 - val_loss: 1.3953 - val_mae: 1.1604 - val_mean_squared_error: 1.3974\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0152 - mae: 0.0819 - mean_squared_error: 0.0152 - val_loss: 0.8133 - val_mae: 0.8759 - val_mean_squared_error: 0.8146\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0134 - mae: 0.0777 - mean_squared_error: 0.0134 - val_loss: 1.8723 - val_mae: 1.3535 - val_mean_squared_error: 1.8748\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0138 - mae: 0.0813 - mean_squared_error: 0.0138 - val_loss: 0.9153 - val_mae: 0.9320 - val_mean_squared_error: 0.9168\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0144 - mae: 0.0809 - mean_squared_error: 0.0144 - val_loss: 1.1306 - val_mae: 1.0393 - val_mean_squared_error: 1.1323\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0159 - mae: 0.0851 - mean_squared_error: 0.0159 - val_loss: 1.0113 - val_mae: 0.9760 - val_mean_squared_error: 1.0129\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0146 - mae: 0.0837 - mean_squared_error: 0.0146 - val_loss: 0.7403 - val_mae: 0.8377 - val_mean_squared_error: 0.7415\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0143 - mae: 0.0817 - mean_squared_error: 0.0143 - val_loss: 0.5485 - val_mae: 0.7181 - val_mean_squared_error: 0.5491\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0134 - mae: 0.0788 - mean_squared_error: 0.0134 - val_loss: 1.0252 - val_mae: 0.9889 - val_mean_squared_error: 1.0268\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0139 - mae: 0.0833 - mean_squared_error: 0.0139 - val_loss: 0.7384 - val_mae: 0.8346 - val_mean_squared_error: 0.7395\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0145 - mae: 0.0835 - mean_squared_error: 0.0145 - val_loss: 1.0924 - val_mae: 1.0224 - val_mean_squared_error: 1.0941\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129 - mae: 0.0753 - mean_squared_error: 0.0129 - val_loss: 0.5042 - val_mae: 0.6877 - val_mean_squared_error: 0.5047\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0139 - mae: 0.0809 - mean_squared_error: 0.0139 - val_loss: 1.2605 - val_mae: 1.1028 - val_mean_squared_error: 1.2623\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0146 - mae: 0.0821 - mean_squared_error: 0.0146 - val_loss: 0.6568 - val_mae: 0.7860 - val_mean_squared_error: 0.6578\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0142 - mae: 0.0803 - mean_squared_error: 0.0142 - val_loss: 1.1731 - val_mae: 1.0585 - val_mean_squared_error: 1.1749\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0131 - mae: 0.0753 - mean_squared_error: 0.0131 - val_loss: 1.5103 - val_mae: 1.2114 - val_mean_squared_error: 1.5125\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0156 - mae: 0.0838 - mean_squared_error: 0.0156 - val_loss: 1.2802 - val_mae: 1.1124 - val_mean_squared_error: 1.2821\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0132 - mae: 0.0762 - mean_squared_error: 0.0132 - val_loss: 1.1479 - val_mae: 1.0460 - val_mean_squared_error: 1.1497\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0149 - mae: 0.0799 - mean_squared_error: 0.0149 - val_loss: 0.9401 - val_mae: 0.9434 - val_mean_squared_error: 0.9416\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0128 - mae: 0.0753 - mean_squared_error: 0.0128 - val_loss: 1.2932 - val_mae: 1.1181 - val_mean_squared_error: 1.2950\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0132 - mae: 0.0781 - mean_squared_error: 0.0132 - val_loss: 1.0111 - val_mae: 0.9779 - val_mean_squared_error: 1.0125\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0142 - mae: 0.0788 - mean_squared_error: 0.0142 - val_loss: 1.3214 - val_mae: 1.1297 - val_mean_squared_error: 1.3233\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0126 - mae: 0.0750 - mean_squared_error: 0.0126 - val_loss: 0.8006 - val_mae: 0.8672 - val_mean_squared_error: 0.8018\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0138 - mae: 0.0802 - mean_squared_error: 0.0138 - val_loss: 1.0993 - val_mae: 1.0259 - val_mean_squared_error: 1.1010\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0138 - mae: 0.0792 - mean_squared_error: 0.0138 - val_loss: 0.9106 - val_mae: 0.9331 - val_mean_squared_error: 0.9120\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0140 - mae: 0.0811 - mean_squared_error: 0.0140 - val_loss: 0.6428 - val_mae: 0.7735 - val_mean_squared_error: 0.6439\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0132 - mae: 0.0788 - mean_squared_error: 0.0132 - val_loss: 0.5595 - val_mae: 0.7231 - val_mean_squared_error: 0.5604\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0133 - mae: 0.0780 - mean_squared_error: 0.0133 - val_loss: 0.6052 - val_mae: 0.7568 - val_mean_squared_error: 0.6059\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0131 - mae: 0.0773 - mean_squared_error: 0.0131 - val_loss: 1.1452 - val_mae: 1.0486 - val_mean_squared_error: 1.1469\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0139 - mae: 0.0794 - mean_squared_error: 0.0139 - val_loss: 0.8946 - val_mae: 0.9213 - val_mean_squared_error: 0.8959\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0132 - mae: 0.0776 - mean_squared_error: 0.0132 - val_loss: 1.3232 - val_mae: 1.1305 - val_mean_squared_error: 1.3251\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0141 - mae: 0.0797 - mean_squared_error: 0.0141 - val_loss: 1.7678 - val_mae: 1.3135 - val_mean_squared_error: 1.7702\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0133 - mae: 0.0787 - mean_squared_error: 0.0133 - val_loss: 0.5337 - val_mae: 0.7076 - val_mean_squared_error: 0.5344\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step\n",
      "> Model set up completed. Duration:  5: 17\n",
      "> Compilation completed. Duration:  28540524: 4\n",
      "> Training completed. Duration:  6: 57\n",
      "> Prediction completed. Duration:  28540524: 5\n",
      "> Evaluation completed. Duration:  6: 57\n",
      "Total time taken: 01:39\n",
      "\n",
      "\n",
      "Final Neural Network\n",
      "> Input shape: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesrezgui/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │        \u001b[38;5;34m15,934\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">409,981</span> (1.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m409,981\u001b[0m (1.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">409,981</span> (1.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m409,981\u001b[0m (1.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.3820 - mae: 1.1280 - mean_squared_error: 3.3820 - val_loss: 0.6893 - val_mae: 0.8000 - val_mean_squared_error: 0.6912\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4224 - mae: 0.5173 - mean_squared_error: 0.4224 - val_loss: 1.1109 - val_mae: 1.0308 - val_mean_squared_error: 1.1130\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3382 - mae: 0.4636 - mean_squared_error: 0.3382 - val_loss: 1.0689 - val_mae: 1.0118 - val_mean_squared_error: 1.0704\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2740 - mae: 0.4172 - mean_squared_error: 0.2740 - val_loss: 1.0486 - val_mae: 1.0028 - val_mean_squared_error: 1.0506\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2436 - mae: 0.3937 - mean_squared_error: 0.2436 - val_loss: 0.6768 - val_mae: 0.7989 - val_mean_squared_error: 0.6779\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2169 - mae: 0.3699 - mean_squared_error: 0.2169 - val_loss: 0.5899 - val_mae: 0.7445 - val_mean_squared_error: 0.5906\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1837 - mae: 0.3408 - mean_squared_error: 0.1837 - val_loss: 0.3954 - val_mae: 0.6032 - val_mean_squared_error: 0.3959\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1739 - mae: 0.3326 - mean_squared_error: 0.1739 - val_loss: 0.2278 - val_mae: 0.4477 - val_mean_squared_error: 0.2277\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1545 - mae: 0.3115 - mean_squared_error: 0.1545 - val_loss: 0.2495 - val_mae: 0.4716 - val_mean_squared_error: 0.2494\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1482 - mae: 0.3051 - mean_squared_error: 0.1482 - val_loss: 0.2584 - val_mae: 0.4811 - val_mean_squared_error: 0.2584\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1353 - mae: 0.2919 - mean_squared_error: 0.1353 - val_loss: 0.1640 - val_mae: 0.3733 - val_mean_squared_error: 0.1636\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1248 - mae: 0.2796 - mean_squared_error: 0.1248 - val_loss: 0.1349 - val_mae: 0.3335 - val_mean_squared_error: 0.1343\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1158 - mae: 0.2670 - mean_squared_error: 0.1158 - val_loss: 0.1077 - val_mae: 0.2901 - val_mean_squared_error: 0.1069\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1089 - mae: 0.2609 - mean_squared_error: 0.1089 - val_loss: 0.0992 - val_mae: 0.2763 - val_mean_squared_error: 0.0984\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0997 - mae: 0.2473 - mean_squared_error: 0.0997 - val_loss: 0.0991 - val_mae: 0.2758 - val_mean_squared_error: 0.0983\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0954 - mae: 0.2420 - mean_squared_error: 0.0954 - val_loss: 0.1352 - val_mae: 0.3342 - val_mean_squared_error: 0.1346\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0876 - mae: 0.2310 - mean_squared_error: 0.0876 - val_loss: 0.1303 - val_mae: 0.3273 - val_mean_squared_error: 0.1296\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0825 - mae: 0.2229 - mean_squared_error: 0.0825 - val_loss: 0.1195 - val_mae: 0.3100 - val_mean_squared_error: 0.1188\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0750 - mae: 0.2133 - mean_squared_error: 0.0750 - val_loss: 0.0862 - val_mae: 0.2516 - val_mean_squared_error: 0.0853\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0696 - mae: 0.2051 - mean_squared_error: 0.0696 - val_loss: 0.1113 - val_mae: 0.2958 - val_mean_squared_error: 0.1107\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0670 - mae: 0.1999 - mean_squared_error: 0.0670 - val_loss: 0.1375 - val_mae: 0.3379 - val_mean_squared_error: 0.1369\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0615 - mae: 0.1917 - mean_squared_error: 0.0615 - val_loss: 0.1446 - val_mae: 0.3473 - val_mean_squared_error: 0.1441\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0561 - mae: 0.1819 - mean_squared_error: 0.0561 - val_loss: 0.1599 - val_mae: 0.3676 - val_mean_squared_error: 0.1596\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0518 - mae: 0.1767 - mean_squared_error: 0.0518 - val_loss: 0.1813 - val_mae: 0.3958 - val_mean_squared_error: 0.1809\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0482 - mae: 0.1700 - mean_squared_error: 0.0482 - val_loss: 0.1931 - val_mae: 0.4063 - val_mean_squared_error: 0.1931\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0440 - mae: 0.1620 - mean_squared_error: 0.0440 - val_loss: 0.3900 - val_mae: 0.5990 - val_mean_squared_error: 0.3907\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0395 - mae: 0.1527 - mean_squared_error: 0.0395 - val_loss: 0.3108 - val_mae: 0.5231 - val_mean_squared_error: 0.3116\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0369 - mae: 0.1477 - mean_squared_error: 0.0369 - val_loss: 0.3188 - val_mae: 0.5325 - val_mean_squared_error: 0.3195\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0363 - mae: 0.1435 - mean_squared_error: 0.0363 - val_loss: 0.4791 - val_mae: 0.6661 - val_mean_squared_error: 0.4800\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0321 - mae: 0.1348 - mean_squared_error: 0.0321 - val_loss: 0.6426 - val_mae: 0.7797 - val_mean_squared_error: 0.6437\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0305 - mae: 0.1288 - mean_squared_error: 0.0305 - val_loss: 0.6783 - val_mae: 0.8010 - val_mean_squared_error: 0.6795\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0283 - mae: 0.1257 - mean_squared_error: 0.0283 - val_loss: 0.5057 - val_mae: 0.6830 - val_mean_squared_error: 0.5068\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0271 - mae: 0.1237 - mean_squared_error: 0.0271 - val_loss: 0.7863 - val_mae: 0.8663 - val_mean_squared_error: 0.7875\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0252 - mae: 0.1180 - mean_squared_error: 0.0252 - val_loss: 0.8912 - val_mae: 0.9249 - val_mean_squared_error: 0.8924\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0257 - mae: 0.1182 - mean_squared_error: 0.0257 - val_loss: 0.7905 - val_mae: 0.8691 - val_mean_squared_error: 0.7917\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0246 - mae: 0.1146 - mean_squared_error: 0.0246 - val_loss: 0.6428 - val_mae: 0.7795 - val_mean_squared_error: 0.6439\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0212 - mae: 0.1073 - mean_squared_error: 0.0212 - val_loss: 0.7830 - val_mae: 0.8649 - val_mean_squared_error: 0.7842\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0221 - mae: 0.1072 - mean_squared_error: 0.0221 - val_loss: 0.7743 - val_mae: 0.8600 - val_mean_squared_error: 0.7755\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0226 - mae: 0.1076 - mean_squared_error: 0.0226 - val_loss: 0.7832 - val_mae: 0.8650 - val_mean_squared_error: 0.7843\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0192 - mae: 0.1009 - mean_squared_error: 0.0192 - val_loss: 0.7394 - val_mae: 0.8396 - val_mean_squared_error: 0.7404\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0184 - mae: 0.0971 - mean_squared_error: 0.0184 - val_loss: 0.8114 - val_mae: 0.8813 - val_mean_squared_error: 0.8126\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0185 - mae: 0.0964 - mean_squared_error: 0.0185 - val_loss: 0.8160 - val_mae: 0.8833 - val_mean_squared_error: 0.8171\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0202 - mae: 0.1014 - mean_squared_error: 0.0202 - val_loss: 0.8368 - val_mae: 0.8955 - val_mean_squared_error: 0.8380\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0175 - mae: 0.0939 - mean_squared_error: 0.0175 - val_loss: 0.9027 - val_mae: 0.9310 - val_mean_squared_error: 0.9039\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0183 - mae: 0.0954 - mean_squared_error: 0.0183 - val_loss: 0.7663 - val_mae: 0.8550 - val_mean_squared_error: 0.7674\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0172 - mae: 0.0929 - mean_squared_error: 0.0172 - val_loss: 0.9074 - val_mae: 0.9335 - val_mean_squared_error: 0.9087\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0173 - mae: 0.0942 - mean_squared_error: 0.0173 - val_loss: 0.8951 - val_mae: 0.9270 - val_mean_squared_error: 0.8963\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0175 - mae: 0.0913 - mean_squared_error: 0.0175 - val_loss: 0.7515 - val_mae: 0.8467 - val_mean_squared_error: 0.7526\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0155 - mae: 0.0882 - mean_squared_error: 0.0155 - val_loss: 0.5328 - val_mae: 0.7061 - val_mean_squared_error: 0.5334\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0173 - mae: 0.0923 - mean_squared_error: 0.0173 - val_loss: 0.4155 - val_mae: 0.6197 - val_mean_squared_error: 0.4160\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0166 - mae: 0.0887 - mean_squared_error: 0.0166 - val_loss: 0.9667 - val_mae: 0.9645 - val_mean_squared_error: 0.9681\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0181 - mae: 0.0916 - mean_squared_error: 0.0181 - val_loss: 0.8500 - val_mae: 0.9018 - val_mean_squared_error: 0.8512\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0165 - mae: 0.0869 - mean_squared_error: 0.0165 - val_loss: 0.8075 - val_mae: 0.8784 - val_mean_squared_error: 0.8086\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0168 - mae: 0.0898 - mean_squared_error: 0.0168 - val_loss: 0.8180 - val_mae: 0.8844 - val_mean_squared_error: 0.8192\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0146 - mae: 0.0833 - mean_squared_error: 0.0146 - val_loss: 0.7588 - val_mae: 0.8503 - val_mean_squared_error: 0.7599\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0165 - mae: 0.0880 - mean_squared_error: 0.0165 - val_loss: 0.7890 - val_mae: 0.8683 - val_mean_squared_error: 0.7900\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0153 - mae: 0.0860 - mean_squared_error: 0.0153 - val_loss: 0.9405 - val_mae: 0.9504 - val_mean_squared_error: 0.9418\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0156 - mae: 0.0863 - mean_squared_error: 0.0156 - val_loss: 0.6416 - val_mae: 0.7780 - val_mean_squared_error: 0.6425\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0151 - mae: 0.0863 - mean_squared_error: 0.0151 - val_loss: 0.8763 - val_mae: 0.9168 - val_mean_squared_error: 0.8774\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0153 - mae: 0.0839 - mean_squared_error: 0.0153 - val_loss: 0.6755 - val_mae: 0.7988 - val_mean_squared_error: 0.6764\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0162 - mae: 0.0884 - mean_squared_error: 0.0162 - val_loss: 0.9633 - val_mae: 0.9614 - val_mean_squared_error: 0.9647\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0161 - mae: 0.0847 - mean_squared_error: 0.0161 - val_loss: 0.8588 - val_mae: 0.9067 - val_mean_squared_error: 0.8600\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0145 - mae: 0.0816 - mean_squared_error: 0.0145 - val_loss: 0.8562 - val_mae: 0.9051 - val_mean_squared_error: 0.8574\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0159 - mae: 0.0845 - mean_squared_error: 0.0159 - val_loss: 0.9153 - val_mae: 0.9375 - val_mean_squared_error: 0.9167\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0140 - mae: 0.0790 - mean_squared_error: 0.0140 - val_loss: 0.7395 - val_mae: 0.8364 - val_mean_squared_error: 0.7405\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0169 - mae: 0.0855 - mean_squared_error: 0.0169 - val_loss: 0.8442 - val_mae: 0.8970 - val_mean_squared_error: 0.8454\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0152 - mae: 0.0851 - mean_squared_error: 0.0152 - val_loss: 0.8592 - val_mae: 0.9063 - val_mean_squared_error: 0.8604\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0143 - mae: 0.0808 - mean_squared_error: 0.0143 - val_loss: 0.6516 - val_mae: 0.7815 - val_mean_squared_error: 0.6525\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0134 - mae: 0.0792 - mean_squared_error: 0.0134 - val_loss: 1.0116 - val_mae: 0.9864 - val_mean_squared_error: 1.0130\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0144 - mae: 0.0804 - mean_squared_error: 0.0144 - val_loss: 0.9715 - val_mae: 0.9659 - val_mean_squared_error: 0.9729\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0155 - mae: 0.0841 - mean_squared_error: 0.0155 - val_loss: 1.0585 - val_mae: 1.0098 - val_mean_squared_error: 1.0601\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0172 - mae: 0.0896 - mean_squared_error: 0.0172 - val_loss: 0.6609 - val_mae: 0.7897 - val_mean_squared_error: 0.6619\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0141 - mae: 0.0802 - mean_squared_error: 0.0141 - val_loss: 0.8210 - val_mae: 0.8844 - val_mean_squared_error: 0.8221\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0171 - mae: 0.0908 - mean_squared_error: 0.0171 - val_loss: 0.8877 - val_mae: 0.9207 - val_mean_squared_error: 0.8890\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0145 - mae: 0.0828 - mean_squared_error: 0.0145 - val_loss: 0.8232 - val_mae: 0.8862 - val_mean_squared_error: 0.8244\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0156 - mae: 0.0878 - mean_squared_error: 0.0156 - val_loss: 0.6573 - val_mae: 0.7881 - val_mean_squared_error: 0.6583\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0226 - mae: 0.1043 - mean_squared_error: 0.0226 - val_loss: 1.0708 - val_mae: 1.0159 - val_mean_squared_error: 1.0722\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0159 - mae: 0.0871 - mean_squared_error: 0.0159 - val_loss: 1.0375 - val_mae: 0.9997 - val_mean_squared_error: 1.0389\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0160 - mae: 0.0854 - mean_squared_error: 0.0160 - val_loss: 0.9138 - val_mae: 0.9352 - val_mean_squared_error: 0.9151\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0159 - mae: 0.0848 - mean_squared_error: 0.0159 - val_loss: 0.9007 - val_mae: 0.9290 - val_mean_squared_error: 0.9021\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0153 - mae: 0.0833 - mean_squared_error: 0.0153 - val_loss: 0.9165 - val_mae: 0.9377 - val_mean_squared_error: 0.9178\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0168 - mae: 0.0889 - mean_squared_error: 0.0168 - val_loss: 0.9504 - val_mae: 0.9560 - val_mean_squared_error: 0.9518\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0159 - mae: 0.0827 - mean_squared_error: 0.0159 - val_loss: 0.9910 - val_mae: 0.9763 - val_mean_squared_error: 0.9925\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0160 - mae: 0.0873 - mean_squared_error: 0.0160 - val_loss: 1.0233 - val_mae: 0.9932 - val_mean_squared_error: 1.0247\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0159 - mae: 0.0867 - mean_squared_error: 0.0159 - val_loss: 0.9172 - val_mae: 0.9381 - val_mean_squared_error: 0.9186\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0161 - mae: 0.0849 - mean_squared_error: 0.0161 - val_loss: 0.9166 - val_mae: 0.9375 - val_mean_squared_error: 0.9179\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0157 - mae: 0.0836 - mean_squared_error: 0.0157 - val_loss: 0.9988 - val_mae: 0.9810 - val_mean_squared_error: 1.0002\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0164 - mae: 0.0863 - mean_squared_error: 0.0164 - val_loss: 0.8140 - val_mae: 0.8824 - val_mean_squared_error: 0.8151\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0158 - mae: 0.0852 - mean_squared_error: 0.0158 - val_loss: 1.0002 - val_mae: 0.9817 - val_mean_squared_error: 1.0016\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0149 - mae: 0.0802 - mean_squared_error: 0.0149 - val_loss: 0.9064 - val_mae: 0.9329 - val_mean_squared_error: 0.9076\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0168 - mae: 0.0891 - mean_squared_error: 0.0168 - val_loss: 0.9671 - val_mae: 0.9649 - val_mean_squared_error: 0.9684\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0178 - mae: 0.0903 - mean_squared_error: 0.0178 - val_loss: 0.7938 - val_mae: 0.8706 - val_mean_squared_error: 0.7948\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0155 - mae: 0.0844 - mean_squared_error: 0.0155 - val_loss: 0.8173 - val_mae: 0.8826 - val_mean_squared_error: 0.8184\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0158 - mae: 0.0861 - mean_squared_error: 0.0158 - val_loss: 0.9128 - val_mae: 0.9364 - val_mean_squared_error: 0.9140\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0149 - mae: 0.0830 - mean_squared_error: 0.0149 - val_loss: 0.9109 - val_mae: 0.9353 - val_mean_squared_error: 0.9121\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0169 - mae: 0.0854 - mean_squared_error: 0.0169 - val_loss: 0.8156 - val_mae: 0.8831 - val_mean_squared_error: 0.8167\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0155 - mae: 0.0844 - mean_squared_error: 0.0155 - val_loss: 0.8085 - val_mae: 0.8785 - val_mean_squared_error: 0.8096\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0139 - mae: 0.0788 - mean_squared_error: 0.0139 - val_loss: 0.7033 - val_mae: 0.8158 - val_mean_squared_error: 0.7043\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0163 - mae: 0.0846 - mean_squared_error: 0.0163 - val_loss: 0.9185 - val_mae: 0.9391 - val_mean_squared_error: 0.9198\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0156 - mae: 0.0834 - mean_squared_error: 0.0156 - val_loss: 0.6848 - val_mae: 0.8049 - val_mean_squared_error: 0.6858\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step\n",
      "> Model set up completed. Duration:  5: 17\n",
      "> Compilation completed. Duration:  28540525: 44\n",
      "> Training completed. Duration:  7: 39\n",
      "> Prediction completed. Duration:  28540525: 45\n",
      "> Evaluation completed. Duration:  7: 39\n",
      "Total time taken: 04:02\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run model for different moodels\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics_NN.iterrows():\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "\n",
    "    input_shape = row['X_train'].shape[1]\n",
    "    print(f\"> Input shape: {input_shape}\")\n",
    "\n",
    "    # neurons number\n",
    "    n_neurons = 512\n",
    "\n",
    "### define a model\n",
    "    final_model = keras.Sequential()\n",
    "\n",
    "    # Add input layer\n",
    "    final_model.add(layers.Dense(\n",
    "                n_neurons, # number of neurons\n",
    "                input_dim = input_shape, # number of inputs \n",
    "                activation = 'relu' # activation faunction\n",
    "                ))\n",
    "\n",
    "    # Hidden - Layers\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.3, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "    final_model.add(layers.Dense(\n",
    "        256, \n",
    "        activation = \"relu\"))\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.2, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "    final_model.add(layers.Dense(\n",
    "        62, \n",
    "        activation = \"relu\"))\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.2, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "\n",
    "    # Final layer\n",
    "    final_model.add(layers.Dense(\n",
    "        1, \n",
    "        activation = 'linear'))\n",
    "\n",
    "    final_model.summary()\n",
    "\n",
    "    # Add model to table\n",
    "    row['model'] = final_model\n",
    "    \n",
    "### Compile the model\n",
    "    final_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mean_squared_error'], \n",
    "    metrics = ['mae', 'mean_squared_error']\n",
    "    )\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "\n",
    "### Train the model\n",
    "    epochs_hist = final_model.fit(\n",
    "    row['X_train'], # input\n",
    "    y_wr_train, # output\n",
    "    epochs=100, # number of iterations\n",
    "    batch_size=50, # number of observations taken to train the data - 1030 obs/50 -> there are 17 groups (observations are taken once for epoch) so model is trained 17 times in each epoch\n",
    "    verbose=1,\n",
    "    validation_data = (row['X_test'], y_wr_test),\n",
    "    shuffle = True\n",
    "    #validation_split=0.2,    \n",
    "    )\n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "\n",
    "\n",
    "# ### Predictions\n",
    "    y_pred = final_model.predict(row['X_test'])\n",
    "    # Store predictions\n",
    "    row['prediction'] = y_pred\n",
    "    # Time elapsed\n",
    "    elapsed_time_4 = time.time() - elapsed_time_3\n",
    "\n",
    "\n",
    "# ### Evaluation\n",
    "    mse = mean_squared_error(y_pred, y_wr_test)\n",
    "    mae = mean_absolute_error(y_pred, y_wr_test)\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_5 = time.time() - elapsed_time_4\n",
    "\n",
    "\n",
    "    # Timings\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Model set up completed. Duration: {minutes: 02d}:{seconds: 02d}\")\n",
    "\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Compilation completed. Duration: {minutes: 02d}:{seconds: 02d}\")\n",
    "\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    minutes = int(elapsed_time_4 // 60)\n",
    "    seconds = int(elapsed_time_4 % 60)\n",
    "    print(f\"> Prediction completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    minutes = int(elapsed_time_5 // 60)\n",
    "    seconds = int(elapsed_time_5 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\"Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_3, built=True&gt;</td>\n",
       "      <td>0         1         2         3  ...</td>\n",
       "      <td>0         1         2         3  ...</td>\n",
       "      <td>[[3.5686584], [3.5541081], [3.5608985], [3.536...</td>\n",
       "      <td>0.707585</td>\n",
       "      <td>0.534406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_4, built=True&gt;</td>\n",
       "      <td>0         1         2         3...</td>\n",
       "      <td>0         1         2         3...</td>\n",
       "      <td>[[3.4987206], [3.4354882], [3.4638855], [3.398...</td>\n",
       "      <td>0.804946</td>\n",
       "      <td>0.685819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              model  \\\n",
       "Baseline Neural Network  <Sequential name=sequential_3, built=True>   \n",
       "Final Neural Network     <Sequential name=sequential_4, built=True>   \n",
       "\n",
       "                                                                   X_train  \\\n",
       "Baseline Neural Network               0         1         2         3  ...   \n",
       "Final Neural Network                    0         1         2         3...   \n",
       "\n",
       "                                                                    X_test  \\\n",
       "Baseline Neural Network               0         1         2         3  ...   \n",
       "Final Neural Network                    0         1         2         3...   \n",
       "\n",
       "                                                                prediction  \\\n",
       "Baseline Neural Network  [[3.5686584], [3.5541081], [3.5608985], [3.536...   \n",
       "Final Neural Network     [[3.4987206], [3.4354882], [3.4638855], [3.398...   \n",
       "\n",
       "                              MAE       MSE  \n",
       "Baseline Neural Network  0.707585  0.534406  \n",
       "Final Neural Network     0.804946  0.685819  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_all = pd.concat([evaluation_metrics, evaluation_metrics_NN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>tSVD1     tSVD2     tSVD3     tSVD4...</td>\n",
       "      <td>[4.26317910221547, 4.263480602359525, 4.262913...</td>\n",
       "      <td>0.13147</td>\n",
       "      <td>0.045265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.269796558491903, 4.251500951120229, 4.26040...</td>\n",
       "      <td>0.131182</td>\n",
       "      <td>0.044787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_3, built=True&gt;</td>\n",
       "      <td>0         1         2         3  ...</td>\n",
       "      <td>0         1         2         3  ...</td>\n",
       "      <td>[[3.5686584], [3.5541081], [3.5608985], [3.536...</td>\n",
       "      <td>0.707585</td>\n",
       "      <td>0.534406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_4, built=True&gt;</td>\n",
       "      <td>0         1         2         3...</td>\n",
       "      <td>0         1         2         3...</td>\n",
       "      <td>[[3.4987206], [3.4354882], [3.4638855], [3.398...</td>\n",
       "      <td>0.804946</td>\n",
       "      <td>0.685819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         model  \\\n",
       "Baseline Support Vector Regression                                     (SVR())   \n",
       "Final Support Vector Regression                                        (SVR())   \n",
       "Baseline Neural Network             <Sequential name=sequential_3, built=True>   \n",
       "Final Neural Network                <Sequential name=sequential_4, built=True>   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                          0         1         2         3  ...   \n",
       "Final Neural Network                               0         1         2         3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression             tSVD1     tSVD2     tSVD3     tSVD4...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                          0         1         2         3  ...   \n",
       "Final Neural Network                               0         1         2         3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.26317910221547, 4.263480602359525, 4.262913...   \n",
       "Final Support Vector Regression     [4.269796558491903, 4.251500951120229, 4.26040...   \n",
       "Baseline Neural Network             [[3.5686584], [3.5541081], [3.5608985], [3.536...   \n",
       "Final Neural Network                [[3.4987206], [3.4354882], [3.4638855], [3.398...   \n",
       "\n",
       "                                         MAE       MSE  \n",
       "Baseline Support Vector Regression   0.13147  0.045265  \n",
       "Final Support Vector Regression     0.131182  0.044787  \n",
       "Baseline Neural Network             0.707585  0.534406  \n",
       "Final Neural Network                0.804946  0.685819  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise NN\n",
    "\n",
    "# # Plotting Loss And Mean Square Error For both Training And Test Sets\n",
    "# plt.plot(epochs_hist.history['mse'])\n",
    "# plt.plot(epochs_hist.history['val_mse'])\n",
    "# plt.title('MSE')\n",
    "# plt.ylabel('mae')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(output_folder, '{i} mse chart.png'))\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(epochs_hist.history['loss'])\n",
    "# plt.plot(epochs_hist.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.savefig('4.png')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(output_folder, '{i} summary chart.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
