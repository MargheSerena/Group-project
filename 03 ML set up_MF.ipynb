{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with datarames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charts\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# X, Y preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# Neural Network\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title-level dataset\n",
    "titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images\n",
    "covers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "To decide which observations to drop\n",
    "- non-English description\n",
    "- no date?\n",
    "- others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Note:\n",
    "- will have to create X including and X excluding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns for ML\n",
    "\n",
    "# Example code:\n",
    "columns_to_drop = []\n",
    "data = data.drop(columns_to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y\n",
    "\n",
    "# Example code\n",
    "x_vars = []\n",
    "y_var = ''\n",
    "\n",
    "X = data[x_vars]\n",
    "y = data[y_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "\n",
    "# Example code\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size= 0.2, \n",
    "    random_state= 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "Do we need to fit transform or normalise the data?    \n",
    "They are mostly categorical or non numerical variables so maybe not?  \n",
    "If yes:\n",
    "- same transformation for both models?\n",
    "- which type of scaler?\n",
    "- do we need to scale also y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, use code below\n",
    "\n",
    "# Choose type of scaler\n",
    "scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit_transform - get mean and std deviation and normalise the X_train data\n",
    "# Note: do this on X train data to avoid data leakage! would have this issue if we were doing this on X\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# transform test based on info of X train\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Also fit transform y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "- any hyperparameter we need to think about?\n",
    "- which metric are we using to evaluate the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up RF\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Note:\n",
    "- this will have to be done including and excluding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "rf_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the feature importance.\n",
    "\n",
    "# get importance of features and assign names\n",
    "rf_importances = rf.feature_importances_\n",
    "rf_importances = pd.DataFrame({'feature':X.columns, 'importance': rf_importances})\n",
    "# sort dataset by importance\n",
    "rf_importances = rf_importances.sort_values(by = 'importance', ascending = False)\n",
    "\n",
    "# Draw chart\n",
    "ax = sns.barplot(\n",
    "    data = rf_importances,\n",
    "    x = 'importance',\n",
    "    y = 'feature'\n",
    ")\n",
    "\n",
    "for index, value in enumerate(rf_importances['importance']):\n",
    "    ax.text(value, index, f'{value:.4f}', ha='left', va='center', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree of the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a decision tree with a max_leaf_nodes of 3, plot the decision tree\n",
    "\n",
    "dt_max_3 = DecisionTreeRegressor(max_depth=3)\n",
    "dt_max_3.fit(X_train,y_train)\n",
    "dt_max_3_predictions = dt_max_3.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=300)\n",
    "tree.plot_tree(\n",
    "    dt_max_3,\n",
    "    max_depth = 3, \n",
    "    feature_names=X_train.columns,  \n",
    "    class_names=True,\n",
    "    filled=True,\n",
    "    fontsize=8\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "Inputs to choose:\n",
    "- number of layers\n",
    "- activation functions\n",
    "- Use softmax in the last layer to obtain the probability distribution of the outcome?\n",
    "- optimizer: Adam? sdg?\n",
    "- loss function\n",
    "- add dense layers to avoid overfitting?\n",
    "- number of epochs\n",
    "- which metric to use to evaluate the model?\n",
    "\n",
    "- Use gridsearch to optimise hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of inputs - second element of shape (i.e. number of columns in X)\n",
    "input_shape = X.shape[1]\n",
    "\n",
    "# neurons number\n",
    "n_neurons = 512\n",
    "\n",
    "# define a model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(layers.Dense(\n",
    "            n_neurons, # number of neurons\n",
    "            input_dim = input_shape, # number of inputs \n",
    "            activation = 'tanh' # activation faunction\n",
    "            ))\n",
    "\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "\n",
    "# To change activation function: Output: only want one neuron in the last layer = no activation because we want an output that is a continuous variable (same as saying activation = linear)\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "# Concrete and wine datasets (output is continuous variable)\n",
    "model.compile(\n",
    "    optimizer='sgd', \n",
    "    loss='mean_squared_error', \n",
    "    metrics=['mae'])\n",
    "\n",
    "# Shallow Network (numbers recognition - output is categorical var)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=SGD(learning_rate=0.01),  # lr = learning rate\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Auto purchase dataset (y is continuous)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Note:\n",
    "- this will have to be done including and excluding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs_hist = model.fit(\n",
    "    X_train, # input\n",
    "    y_train, # output\n",
    "    epochs=100, # number of iterations\n",
    "    batch_size=50, # number of observations taken to train the data - 1030 obs/50 -> there are 17 groups (observations are taken once for epoch) so model is trained 17 times in each epoch\n",
    "    verbose=1,\n",
    "    validation_data = (X_test, y_test),\n",
    "    shuffle = True\n",
    "    #validation_split=0.2,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model (it will give the metric specified when model is compiled)\n",
    "score = model.evaluate(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise NN\n",
    "\n",
    "# Plotting Loss And Root Mean Square Error For both Training And Test Sets\n",
    "plt.plot(epochs_hist.history['mae'])\n",
    "plt.plot(epochs_hist.history['val_mae'])\n",
    "plt.title('MAE')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(epochs_hist.history['loss'])\n",
    "plt.plot(epochs_hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('4.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of model performance - example with y continuous\n",
    "test_predictions_ = model.predict(test_df).flatten()\n",
    "test_labels_ = test_labels.to_numpy().flatten()\n",
    "\n",
    "_, ax = plt.subplots(figsize=(14,8))\n",
    "plt.scatter(\n",
    "    test_labels_,\n",
    "    test_predictions_,\n",
    "    alpha=0.6,\n",
    "    color='#ff7043',\n",
    "    lw=1,\n",
    "    ec='black'\n",
    ")\n",
    "\n",
    "lims = [\n",
    "    0,\n",
    "    max(test_predictions_.max(), test_labels_.max())\n",
    "]\n",
    "\n",
    "plt.plot(lims, lims, lw=1, color='#00acc1')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "- how to integrate NN in the pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# Define pipeline steps\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Feature scaling\n",
    "    ('rf', rf)  # Random Forest classifier\n",
    "])\n",
    "\n",
    "nn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Feature scaling\n",
    "    ('nn', model)  # Neural Network classifier\n",
    "])\n",
    "\n",
    "# Fit Random Forest pipeline\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Fit Neural Network pipeline\n",
    "nn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "rf_accuracy = rf_pipeline.score(X_test, y_test)\n",
    "nn_accuracy = nn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Neural Network Accuracy:\", nn_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
