{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folders\n",
    "from EDA_functions import folders_set_up\n",
    "import os\n",
    "\n",
    "# Work with datarames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charts\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# X, Y preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# Neural Network\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Evaluate models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Time\n",
    "import time\n",
    "\n",
    "#from scipy.sparse import spmatrixc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers import Dense,Dropout\n",
    "# from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM\n",
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders\n",
    "Run the code below if you have the following structure:\n",
    "- Group-project: GitHub folder\n",
    "- 01 Input\n",
    "- 02 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_folder, input_folder, output_folder = folders_set_up.generate_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'description', 'authors', 'image', 'previewLink',\n",
       "       'publisher', 'infoLink', 'categories', 'reviews number',\n",
       "       'average rating', 'median rating', 'min review date', 'max review date',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Title-level dataset with embeddings\n",
    "title_embeddings_df = pd.read_pickle('English_fiction_pre_PCA_3_with_av_pool_embeddings.gz')\n",
    "\n",
    "title_embeddings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                     int64\n",
       "Title                    object\n",
       "description              object\n",
       "authors                  object\n",
       "image                    object\n",
       "previewLink              object\n",
       "publisher                object\n",
       "infoLink                 object\n",
       "categories               object\n",
       "reviews number            int64\n",
       "average rating          float64\n",
       "median rating           float64\n",
       "min review date          object\n",
       "max review date          object\n",
       "weighted rating         float64\n",
       "date                     object\n",
       "year                    float64\n",
       "description_language     object\n",
       "Embedding                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_columns = ['min review date', 'max review date', 'date']\n",
    "\n",
    "for date in dates_columns:\n",
    "    # get date from strings with time\n",
    "    title_embeddings_df[date] = title_embeddings_df[date].str.split().str[0]\n",
    "    # convert in datetime\n",
    "    title_embeddings_df[date] = pd.to_datetime(title_embeddings_df[date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min review date    0\n",
       "max review date    0\n",
       "date               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df[dates_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8228249664151188\n",
      "4.886083503427672\n"
     ]
    }
   ],
   "source": [
    "# what is the max and minimu of the ratings?\n",
    "print(title_embeddings_df['weighted rating'].min())\n",
    "print(title_embeddings_df['weighted rating'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we work on a subset of data for now to make the ML run faster\n",
    "#title_embeddings_df = title_embeddings_df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image embeddings\n",
    "These need may need to be transformed in from arrays to columns if the model we use is not NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>median rating</th>\n",
       "      <th>min review date</th>\n",
       "      <th>max review date</th>\n",
       "      <th>weighted rating</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>description_language</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>index_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-02-14</td>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>3.938400</td>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.5179044, -0.7533603, -1.1291503, -0.4418345...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>4.306145</td>\n",
       "      <td>2001-03-06</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.706188, -0.4773652, -0.17887038, 0.07989502...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-22</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>4.256189</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.294651, -0.24902871, -0.6188333, -0.7722471...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-04-16</td>\n",
       "      <td>2000-05-14</td>\n",
       "      <td>4.336313</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.37794992, -0.6178984, -0.81393754, -0.66795...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-08-07</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>4.701517</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.34032565, -2.1706967, -0.21470371, -0.10447...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26769</th>\n",
       "      <td>212361</td>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>4.137453</td>\n",
       "      <td>2009-03-17</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[1.1648176, 0.56768346, -0.22511423, -0.185316...</td>\n",
       "      <td>212361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26770</th>\n",
       "      <td>212365</td>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-05-17</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>4.466716</td>\n",
       "      <td>1998-01-27</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.023786038, -1.9050528, -0.38564998, 0.14921...</td>\n",
       "      <td>212365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26771</th>\n",
       "      <td>212394</td>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>4.260690</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.2700834, -0.11750376, -2.0253444, -1.039558...</td>\n",
       "      <td>212394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26772</th>\n",
       "      <td>212399</td>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-07-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>4.504800</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.6904726, -0.96442795, 0.093034565, -1.69420...</td>\n",
       "      <td>212399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773</th>\n",
       "      <td>212402</td>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2002-11-11</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>3.989408</td>\n",
       "      <td>2003-08-12</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.47349918, -0.8046489, -0.88566315, -0.04958...</td>\n",
       "      <td>212402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26774 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                              Title  \\\n",
       "0           3                      Whispers of the Wicked Saints   \n",
       "1          24           The Forbidden Stories of Marta Veneranda   \n",
       "2          42                            Tess and the Highlander   \n",
       "3          49  Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "4          73                 Night World: Daughters Of Darkness   \n",
       "...       ...                                                ...   \n",
       "26769  212361                                       Calder Pride   \n",
       "26770  212365                                      The Road Back   \n",
       "26771  212394                                       Final things   \n",
       "26772  212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "26773  212402                                  The Autograph Man   \n",
       "\n",
       "                                             description  \\\n",
       "0      Julia Thomas finds her life spinning out of co...   \n",
       "1      Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "2      In 1543, on a windswept isle off of Scotland, ...   \n",
       "3      Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "4      \"There’s something strange about the new girls...   \n",
       "...                                                  ...   \n",
       "26769  The Long-Awaited Addition to the Beloved Calde...   \n",
       "26770  The sequel to the masterpiece All Quiet on the...   \n",
       "26771  Grace's father believes in science and builds ...   \n",
       "26772  During a school trip to Ellis Island, Dominick...   \n",
       "26773  Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                        authors  \\\n",
       "0           ['Veronica Haddon']   \n",
       "1       ['Sonia Rivera-Valdes']   \n",
       "2            ['May Mcgoldrick']   \n",
       "3        ['Elizabeth Sinclair']   \n",
       "4                ['L.J. Smith']   \n",
       "...                         ...   \n",
       "26769          ['Janet Dailey']   \n",
       "26770  ['Erich Maria Remarque']   \n",
       "26771          ['Jenny Offill']   \n",
       "26772       ['Elvira Woodruff']   \n",
       "26773           ['Zadie Smith']   \n",
       "\n",
       "                                                   image  \\\n",
       "0      http://books.google.com/books/content?id=aRSIg...   \n",
       "1      http://books.google.com/books/content?id=A7aYb...   \n",
       "2      http://books.google.com/books/content?id=VmCRS...   \n",
       "3      http://books.google.com/books/content?id=Z6uzJ...   \n",
       "4      http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                  ...   \n",
       "26769  http://books.google.com/books/content?id=nlsgd...   \n",
       "26770  http://books.google.com/books/content?id=obZdA...   \n",
       "26771  http://books.google.com/books/content?id=UbSFB...   \n",
       "26772  http://books.google.com/books/content?id=J7M-N...   \n",
       "26773  http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                             previewLink  \\\n",
       "0      http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "1      http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "2      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "3      http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "4      http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                  ...   \n",
       "26769  http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "26770  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "26771  http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "26772  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "26773  http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                               publisher  \\\n",
       "0                                              iUniverse   \n",
       "1                                    Seven Stories Press   \n",
       "2                                         Harper Collins   \n",
       "3      Harlequin Treasury-Harlequin American Romance 90s   \n",
       "4                                     Simon and Schuster   \n",
       "...                                                  ...   \n",
       "26769                                     Harper Collins   \n",
       "26770                      Random House Trade Paperbacks   \n",
       "26771                                            Vintage   \n",
       "26772                              Scholastic Paperbacks   \n",
       "26773                                            Vintage   \n",
       "\n",
       "                                                infoLink  \\\n",
       "0      http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "1      http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "2      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "3      http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "4      http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                  ...   \n",
       "26769  https://play.google.com/store/books/details?id...   \n",
       "26770  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "26771  https://play.google.com/store/books/details?id...   \n",
       "26772  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "26773  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                 categories  reviews number  average rating  median rating  \\\n",
       "0               ['fiction']              32        3.718750            5.0   \n",
       "1               ['fiction']               1        5.000000            5.0   \n",
       "2      ['juvenile fiction']              17        4.235294            5.0   \n",
       "3               ['fiction']               2        5.000000            5.0   \n",
       "4      ['juvenile fiction']             134        4.768657            5.0   \n",
       "...                     ...             ...             ...            ...   \n",
       "26769           ['fiction']              28        4.035714            5.0   \n",
       "26770           ['fiction']              17        4.705882            5.0   \n",
       "26771           ['fiction']               1        4.000000            4.0   \n",
       "26772  ['juvenile fiction']              28        4.678571            5.0   \n",
       "26773           ['fiction']               4        2.500000            2.5   \n",
       "\n",
       "      min review date max review date  weighted rating       date    year  \\\n",
       "0          2005-02-14      2006-07-01         3.938400 2005-02-01  2005.0   \n",
       "1          2005-01-24      2005-01-24         4.306145 2001-03-06  2001.0   \n",
       "2          2002-10-22      2011-05-25         4.256189 2002-11-01  2002.0   \n",
       "3          1998-04-16      2000-05-14         4.336313 1997-01-01  1997.0   \n",
       "4          1996-08-07      2012-09-18         4.701517 2016-12-06  2016.0   \n",
       "...               ...             ...              ...        ...     ...   \n",
       "26769      1999-09-30      2012-04-04         4.137453 2009-03-17  2009.0   \n",
       "26770      1997-05-17      2012-01-23         4.466716 1998-01-27  1998.0   \n",
       "26771      2012-01-26      2012-01-26         4.260690 2015-03-17  2015.0   \n",
       "26772      1998-07-10      2011-12-31         4.504800 2000-06-01  2000.0   \n",
       "26773      2002-11-11      2005-11-14         3.989408 2003-08-12  2003.0   \n",
       "\n",
       "      description_language                                          Embedding  \\\n",
       "0                  English  [0.5179044, -0.7533603, -1.1291503, -0.4418345...   \n",
       "1                  English  [0.706188, -0.4773652, -0.17887038, 0.07989502...   \n",
       "2                  English  [2.294651, -0.24902871, -0.6188333, -0.7722471...   \n",
       "3                  English  [0.37794992, -0.6178984, -0.81393754, -0.66795...   \n",
       "4                  English  [0.34032565, -2.1706967, -0.21470371, -0.10447...   \n",
       "...                    ...                                                ...   \n",
       "26769              English  [1.1648176, 0.56768346, -0.22511423, -0.185316...   \n",
       "26770              English  [0.023786038, -1.9050528, -0.38564998, 0.14921...   \n",
       "26771              English  [2.2700834, -0.11750376, -2.0253444, -1.039558...   \n",
       "26772              English  [2.6904726, -0.96442795, 0.093034565, -1.69420...   \n",
       "26773              English  [0.47349918, -0.8046489, -0.88566315, -0.04958...   \n",
       "\n",
       "       index_key  \n",
       "0              3  \n",
       "1             24  \n",
       "2             42  \n",
       "3             49  \n",
       "4             73  \n",
       "...          ...  \n",
       "26769     212361  \n",
       "26770     212365  \n",
       "26771     212394  \n",
       "26772     212399  \n",
       "26773     212402  \n",
       "\n",
       "[26774 rows x 20 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df['index_key'] = title_embeddings_df['index']\n",
    "title_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings_df = title_embeddings_df.set_index('index_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>median rating</th>\n",
       "      <th>min review date</th>\n",
       "      <th>max review date</th>\n",
       "      <th>weighted rating</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>description_language</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-02-14</td>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>3.938400</td>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.5179044, -0.7533603, -1.1291503, -0.4418345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>2005-01-24</td>\n",
       "      <td>4.306145</td>\n",
       "      <td>2001-03-06</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.706188, -0.4773652, -0.17887038, 0.07989502...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-22</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>4.256189</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.294651, -0.24902871, -0.6188333, -0.7722471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-04-16</td>\n",
       "      <td>2000-05-14</td>\n",
       "      <td>4.336313</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.37794992, -0.6178984, -0.81393754, -0.66795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-08-07</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>4.701517</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.34032565, -2.1706967, -0.21470371, -0.10447...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>212361</td>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>4.137453</td>\n",
       "      <td>2009-03-17</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[1.1648176, 0.56768346, -0.22511423, -0.185316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>212365</td>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-05-17</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>4.466716</td>\n",
       "      <td>1998-01-27</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.023786038, -1.9050528, -0.38564998, 0.14921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>212394</td>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>4.260690</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.2700834, -0.11750376, -2.0253444, -1.039558...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>212399</td>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998-07-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>4.504800</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[2.6904726, -0.96442795, 0.093034565, -1.69420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>212402</td>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2002-11-11</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>3.989408</td>\n",
       "      <td>2003-08-12</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.47349918, -0.8046489, -0.88566315, -0.04958...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26774 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index                                              Title  \\\n",
       "index_key                                                              \n",
       "3               3                      Whispers of the Wicked Saints   \n",
       "24             24           The Forbidden Stories of Marta Veneranda   \n",
       "42             42                            Tess and the Highlander   \n",
       "49             49  Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "73             73                 Night World: Daughters Of Darkness   \n",
       "...           ...                                                ...   \n",
       "212361     212361                                       Calder Pride   \n",
       "212365     212365                                      The Road Back   \n",
       "212394     212394                                       Final things   \n",
       "212399     212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "212402     212402                                  The Autograph Man   \n",
       "\n",
       "                                                 description  \\\n",
       "index_key                                                      \n",
       "3          Julia Thomas finds her life spinning out of co...   \n",
       "24         Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "42         In 1543, on a windswept isle off of Scotland, ...   \n",
       "49         Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "73         \"There’s something strange about the new girls...   \n",
       "...                                                      ...   \n",
       "212361     The Long-Awaited Addition to the Beloved Calde...   \n",
       "212365     The sequel to the masterpiece All Quiet on the...   \n",
       "212394     Grace's father believes in science and builds ...   \n",
       "212399     During a school trip to Ellis Island, Dominick...   \n",
       "212402     Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                            authors  \\\n",
       "index_key                             \n",
       "3               ['Veronica Haddon']   \n",
       "24          ['Sonia Rivera-Valdes']   \n",
       "42               ['May Mcgoldrick']   \n",
       "49           ['Elizabeth Sinclair']   \n",
       "73                   ['L.J. Smith']   \n",
       "...                             ...   \n",
       "212361             ['Janet Dailey']   \n",
       "212365     ['Erich Maria Remarque']   \n",
       "212394             ['Jenny Offill']   \n",
       "212399          ['Elvira Woodruff']   \n",
       "212402              ['Zadie Smith']   \n",
       "\n",
       "                                                       image  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.com/books/content?id=aRSIg...   \n",
       "24         http://books.google.com/books/content?id=A7aYb...   \n",
       "42         http://books.google.com/books/content?id=VmCRS...   \n",
       "49         http://books.google.com/books/content?id=Z6uzJ...   \n",
       "73         http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                      ...   \n",
       "212361     http://books.google.com/books/content?id=nlsgd...   \n",
       "212365     http://books.google.com/books/content?id=obZdA...   \n",
       "212394     http://books.google.com/books/content?id=UbSFB...   \n",
       "212399     http://books.google.com/books/content?id=J7M-N...   \n",
       "212402     http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                                 previewLink  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24         http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "42         http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49         http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "73         http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                      ...   \n",
       "212361     http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "212365     http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394     http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "212399     http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402     http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                                   publisher  \\\n",
       "index_key                                                      \n",
       "3                                                  iUniverse   \n",
       "24                                       Seven Stories Press   \n",
       "42                                            Harper Collins   \n",
       "49         Harlequin Treasury-Harlequin American Romance 90s   \n",
       "73                                        Simon and Schuster   \n",
       "...                                                      ...   \n",
       "212361                                        Harper Collins   \n",
       "212365                         Random House Trade Paperbacks   \n",
       "212394                                               Vintage   \n",
       "212399                                 Scholastic Paperbacks   \n",
       "212402                                               Vintage   \n",
       "\n",
       "                                                    infoLink  \\\n",
       "index_key                                                      \n",
       "3          http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24         http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "42         http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49         http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "73         http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                      ...   \n",
       "212361     https://play.google.com/store/books/details?id...   \n",
       "212365     http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394     https://play.google.com/store/books/details?id...   \n",
       "212399     http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402     https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                     categories  reviews number  average rating  \\\n",
       "index_key                                                         \n",
       "3                   ['fiction']              32        3.718750   \n",
       "24                  ['fiction']               1        5.000000   \n",
       "42         ['juvenile fiction']              17        4.235294   \n",
       "49                  ['fiction']               2        5.000000   \n",
       "73         ['juvenile fiction']             134        4.768657   \n",
       "...                         ...             ...             ...   \n",
       "212361              ['fiction']              28        4.035714   \n",
       "212365              ['fiction']              17        4.705882   \n",
       "212394              ['fiction']               1        4.000000   \n",
       "212399     ['juvenile fiction']              28        4.678571   \n",
       "212402              ['fiction']               4        2.500000   \n",
       "\n",
       "           median rating min review date max review date  weighted rating  \\\n",
       "index_key                                                                   \n",
       "3                    5.0      2005-02-14      2006-07-01         3.938400   \n",
       "24                   5.0      2005-01-24      2005-01-24         4.306145   \n",
       "42                   5.0      2002-10-22      2011-05-25         4.256189   \n",
       "49                   5.0      1998-04-16      2000-05-14         4.336313   \n",
       "73                   5.0      1996-08-07      2012-09-18         4.701517   \n",
       "...                  ...             ...             ...              ...   \n",
       "212361               5.0      1999-09-30      2012-04-04         4.137453   \n",
       "212365               5.0      1997-05-17      2012-01-23         4.466716   \n",
       "212394               4.0      2012-01-26      2012-01-26         4.260690   \n",
       "212399               5.0      1998-07-10      2011-12-31         4.504800   \n",
       "212402               2.5      2002-11-11      2005-11-14         3.989408   \n",
       "\n",
       "                date    year description_language  \\\n",
       "index_key                                           \n",
       "3         2005-02-01  2005.0              English   \n",
       "24        2001-03-06  2001.0              English   \n",
       "42        2002-11-01  2002.0              English   \n",
       "49        1997-01-01  1997.0              English   \n",
       "73        2016-12-06  2016.0              English   \n",
       "...              ...     ...                  ...   \n",
       "212361    2009-03-17  2009.0              English   \n",
       "212365    1998-01-27  1998.0              English   \n",
       "212394    2015-03-17  2015.0              English   \n",
       "212399    2000-06-01  2000.0              English   \n",
       "212402    2003-08-12  2003.0              English   \n",
       "\n",
       "                                                   Embedding  \n",
       "index_key                                                     \n",
       "3          [0.5179044, -0.7533603, -1.1291503, -0.4418345...  \n",
       "24         [0.706188, -0.4773652, -0.17887038, 0.07989502...  \n",
       "42         [2.294651, -0.24902871, -0.6188333, -0.7722471...  \n",
       "49         [0.37794992, -0.6178984, -0.81393754, -0.66795...  \n",
       "73         [0.34032565, -2.1706967, -0.21470371, -0.10447...  \n",
       "...                                                      ...  \n",
       "212361     [1.1648176, 0.56768346, -0.22511423, -0.185316...  \n",
       "212365     [0.023786038, -1.9050528, -0.38564998, 0.14921...  \n",
       "212394     [2.2700834, -0.11750376, -2.0253444, -1.039558...  \n",
       "212399     [2.6904726, -0.96442795, 0.093034565, -1.69420...  \n",
       "212402     [0.47349918, -0.8046489, -0.88566315, -0.04958...  \n",
       "\n",
       "[26774 rows x 19 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "Most of the cleaning is done in '02 Consolidate books dataset':\n",
    "- English description\n",
    "- category containing the word 'fiction'\n",
    "- non-missing date\n",
    "- non-missing author\n",
    "- non-missing publisher\n",
    "- non-missing cover image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'description', 'authors', 'image', 'previewLink',\n",
       "       'publisher', 'infoLink', 'categories', 'reviews number',\n",
       "       'average rating', 'median rating', 'min review date', 'max review date',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y including all X features and all all teh possible target variables\n",
    "# NOTE: we will have to add the description PCA in X_features\n",
    "X_columns = ['year', 'Embedding', 'index', 'Title']\n",
    "\n",
    "X = title_embeddings_df[X_columns]\n",
    "y = title_embeddings_df[['average rating', 'weighted rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'Embedding', 'index', 'Title'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['average rating', 'weighted rating'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "\n",
    "# Need to create train test split for different combinations of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size= 0.2, \n",
    "    random_state= 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_indices.to_csv(\\n    os.path.join(output_folder, 'train_indices.csv')\\n)\\n\\n\\ntest_indices.to_csv(\\n    os.path.join(output_folder, 'test_indices.csv')\\n)\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store indices of train test split for the NLP of description\n",
    "train_indices = X_train[['Title', 'index']]\n",
    "test_indices = X_test[['Title', 'index']]\n",
    "\n",
    "# no longer required\n",
    "\"\"\"train_indices.to_csv(\n",
    "    os.path.join(output_folder, 'train_indices.csv')\n",
    ")\n",
    "\n",
    "\n",
    "test_indices.to_csv(\n",
    "    os.path.join(output_folder, 'test_indices.csv')\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y train with average rating\n",
    "y_avg_r_train = y_train['average rating']\n",
    "y_avg_r_test = y_test['average rating']\n",
    "\n",
    "# Y train with weighted rating\n",
    "y_wr_train = y_train['weighted rating']\n",
    "y_wr_test = y_test['weighted rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings X\n",
    "Transform the arrays into columns so that they can feed into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images_train = X_train['Embedding'].apply(pd.Series)\n",
    "X_images_test = X_test['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "X_images_train = X_images_train.add_prefix('image_')\n",
    "X_images_test = X_images_test.add_prefix('image_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_0</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "      <th>image_6</th>\n",
       "      <th>image_7</th>\n",
       "      <th>image_8</th>\n",
       "      <th>image_9</th>\n",
       "      <th>...</th>\n",
       "      <th>image_246</th>\n",
       "      <th>image_247</th>\n",
       "      <th>image_248</th>\n",
       "      <th>image_249</th>\n",
       "      <th>image_250</th>\n",
       "      <th>image_251</th>\n",
       "      <th>image_252</th>\n",
       "      <th>image_253</th>\n",
       "      <th>image_254</th>\n",
       "      <th>image_255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19757</th>\n",
       "      <td>1.310540</td>\n",
       "      <td>0.253238</td>\n",
       "      <td>-0.208362</td>\n",
       "      <td>-0.584103</td>\n",
       "      <td>-0.794551</td>\n",
       "      <td>-1.803357</td>\n",
       "      <td>-2.700018</td>\n",
       "      <td>-0.848385</td>\n",
       "      <td>0.949902</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475167</td>\n",
       "      <td>-0.116055</td>\n",
       "      <td>0.735540</td>\n",
       "      <td>-2.354414</td>\n",
       "      <td>0.956939</td>\n",
       "      <td>-1.065875</td>\n",
       "      <td>-0.428229</td>\n",
       "      <td>-0.285047</td>\n",
       "      <td>1.098027</td>\n",
       "      <td>-1.029737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111405</th>\n",
       "      <td>1.078856</td>\n",
       "      <td>-0.691140</td>\n",
       "      <td>-0.908770</td>\n",
       "      <td>-0.527087</td>\n",
       "      <td>-1.044688</td>\n",
       "      <td>-0.904328</td>\n",
       "      <td>0.210946</td>\n",
       "      <td>-1.238919</td>\n",
       "      <td>2.290273</td>\n",
       "      <td>-0.155667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833435</td>\n",
       "      <td>-1.053398</td>\n",
       "      <td>-2.031961</td>\n",
       "      <td>-2.716383</td>\n",
       "      <td>0.817275</td>\n",
       "      <td>-0.434370</td>\n",
       "      <td>-1.456125</td>\n",
       "      <td>0.112614</td>\n",
       "      <td>0.199815</td>\n",
       "      <td>-2.946273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12269</th>\n",
       "      <td>0.350093</td>\n",
       "      <td>-0.135313</td>\n",
       "      <td>0.512653</td>\n",
       "      <td>0.466054</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>0.014968</td>\n",
       "      <td>-2.224684</td>\n",
       "      <td>-0.740723</td>\n",
       "      <td>0.951188</td>\n",
       "      <td>0.978717</td>\n",
       "      <td>...</td>\n",
       "      <td>1.401945</td>\n",
       "      <td>-0.270900</td>\n",
       "      <td>-1.967142</td>\n",
       "      <td>-0.814089</td>\n",
       "      <td>0.170715</td>\n",
       "      <td>0.335253</td>\n",
       "      <td>-0.030882</td>\n",
       "      <td>-0.557203</td>\n",
       "      <td>0.037506</td>\n",
       "      <td>-1.951925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186303</th>\n",
       "      <td>4.560193</td>\n",
       "      <td>0.550376</td>\n",
       "      <td>-0.331547</td>\n",
       "      <td>-2.215812</td>\n",
       "      <td>0.023501</td>\n",
       "      <td>-0.370375</td>\n",
       "      <td>-0.838666</td>\n",
       "      <td>-0.905349</td>\n",
       "      <td>2.655245</td>\n",
       "      <td>0.461321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073267</td>\n",
       "      <td>0.354717</td>\n",
       "      <td>-1.745163</td>\n",
       "      <td>-2.610591</td>\n",
       "      <td>-0.239567</td>\n",
       "      <td>-1.714204</td>\n",
       "      <td>-0.914772</td>\n",
       "      <td>-0.354720</td>\n",
       "      <td>0.098914</td>\n",
       "      <td>-3.272672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134045</th>\n",
       "      <td>2.236698</td>\n",
       "      <td>-0.498289</td>\n",
       "      <td>-1.946595</td>\n",
       "      <td>-0.618339</td>\n",
       "      <td>-2.150359</td>\n",
       "      <td>1.257699</td>\n",
       "      <td>-1.547900</td>\n",
       "      <td>-1.492419</td>\n",
       "      <td>2.133226</td>\n",
       "      <td>1.513557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192120</td>\n",
       "      <td>-0.731990</td>\n",
       "      <td>-3.023546</td>\n",
       "      <td>-2.410438</td>\n",
       "      <td>-0.701821</td>\n",
       "      <td>-0.403381</td>\n",
       "      <td>0.522945</td>\n",
       "      <td>0.256027</td>\n",
       "      <td>1.826598</td>\n",
       "      <td>-3.091393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167206</th>\n",
       "      <td>0.998582</td>\n",
       "      <td>-1.018484</td>\n",
       "      <td>-0.603367</td>\n",
       "      <td>-0.467136</td>\n",
       "      <td>0.090738</td>\n",
       "      <td>-1.786356</td>\n",
       "      <td>-1.359131</td>\n",
       "      <td>-0.057187</td>\n",
       "      <td>-0.699324</td>\n",
       "      <td>-0.277699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967072</td>\n",
       "      <td>-0.407894</td>\n",
       "      <td>0.675619</td>\n",
       "      <td>-2.690143</td>\n",
       "      <td>0.811195</td>\n",
       "      <td>0.162421</td>\n",
       "      <td>-0.319673</td>\n",
       "      <td>-0.655105</td>\n",
       "      <td>1.837679</td>\n",
       "      <td>-2.292945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40262</th>\n",
       "      <td>0.993674</td>\n",
       "      <td>-0.236895</td>\n",
       "      <td>-1.161556</td>\n",
       "      <td>-0.831213</td>\n",
       "      <td>-0.535987</td>\n",
       "      <td>1.596271</td>\n",
       "      <td>-1.988977</td>\n",
       "      <td>-0.020737</td>\n",
       "      <td>0.298276</td>\n",
       "      <td>0.706597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235213</td>\n",
       "      <td>-0.146208</td>\n",
       "      <td>-1.484007</td>\n",
       "      <td>-1.698481</td>\n",
       "      <td>-0.935188</td>\n",
       "      <td>-0.480799</td>\n",
       "      <td>0.807755</td>\n",
       "      <td>0.346888</td>\n",
       "      <td>1.523457</td>\n",
       "      <td>-1.739759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>0.622462</td>\n",
       "      <td>-0.793748</td>\n",
       "      <td>-0.837395</td>\n",
       "      <td>-0.745482</td>\n",
       "      <td>-0.137351</td>\n",
       "      <td>0.551706</td>\n",
       "      <td>0.313828</td>\n",
       "      <td>-0.853430</td>\n",
       "      <td>0.280148</td>\n",
       "      <td>-0.483826</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114822</td>\n",
       "      <td>-0.845614</td>\n",
       "      <td>-1.312257</td>\n",
       "      <td>-0.366303</td>\n",
       "      <td>-0.078197</td>\n",
       "      <td>1.005683</td>\n",
       "      <td>0.627290</td>\n",
       "      <td>-1.296975</td>\n",
       "      <td>1.453560</td>\n",
       "      <td>-0.719741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124694</th>\n",
       "      <td>1.551901</td>\n",
       "      <td>0.323544</td>\n",
       "      <td>-1.019795</td>\n",
       "      <td>-0.265791</td>\n",
       "      <td>-0.342785</td>\n",
       "      <td>0.346762</td>\n",
       "      <td>0.363122</td>\n",
       "      <td>-0.480910</td>\n",
       "      <td>0.268287</td>\n",
       "      <td>-0.495149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396447</td>\n",
       "      <td>0.472859</td>\n",
       "      <td>-1.687484</td>\n",
       "      <td>-2.504541</td>\n",
       "      <td>-0.655285</td>\n",
       "      <td>0.751038</td>\n",
       "      <td>0.302664</td>\n",
       "      <td>-0.154664</td>\n",
       "      <td>1.174814</td>\n",
       "      <td>-2.710322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185394</th>\n",
       "      <td>1.494337</td>\n",
       "      <td>-0.428976</td>\n",
       "      <td>-0.336699</td>\n",
       "      <td>-0.481797</td>\n",
       "      <td>-1.894806</td>\n",
       "      <td>0.845233</td>\n",
       "      <td>-1.619887</td>\n",
       "      <td>-0.521737</td>\n",
       "      <td>-0.534672</td>\n",
       "      <td>-2.321126</td>\n",
       "      <td>...</td>\n",
       "      <td>1.549637</td>\n",
       "      <td>-0.056938</td>\n",
       "      <td>-0.942204</td>\n",
       "      <td>-1.869788</td>\n",
       "      <td>0.764314</td>\n",
       "      <td>-1.347982</td>\n",
       "      <td>-0.955746</td>\n",
       "      <td>-0.672004</td>\n",
       "      <td>1.069264</td>\n",
       "      <td>-1.114935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_0   image_1   image_2   image_3   image_4   image_5  \\\n",
       "index_key                                                               \n",
       "19757      1.310540  0.253238 -0.208362 -0.584103 -0.794551 -1.803357   \n",
       "111405     1.078856 -0.691140 -0.908770 -0.527087 -1.044688 -0.904328   \n",
       "12269      0.350093 -0.135313  0.512653  0.466054  0.326599  0.014968   \n",
       "186303     4.560193  0.550376 -0.331547 -2.215812  0.023501 -0.370375   \n",
       "134045     2.236698 -0.498289 -1.946595 -0.618339 -2.150359  1.257699   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "167206     0.998582 -1.018484 -0.603367 -0.467136  0.090738 -1.786356   \n",
       "40262      0.993674 -0.236895 -1.161556 -0.831213 -0.535987  1.596271   \n",
       "7002       0.622462 -0.793748 -0.837395 -0.745482 -0.137351  0.551706   \n",
       "124694     1.551901  0.323544 -1.019795 -0.265791 -0.342785  0.346762   \n",
       "185394     1.494337 -0.428976 -0.336699 -0.481797 -1.894806  0.845233   \n",
       "\n",
       "            image_6   image_7   image_8   image_9  ...  image_246  image_247  \\\n",
       "index_key                                          ...                         \n",
       "19757     -2.700018 -0.848385  0.949902  0.030431  ...  -0.475167  -0.116055   \n",
       "111405     0.210946 -1.238919  2.290273 -0.155667  ...   0.833435  -1.053398   \n",
       "12269     -2.224684 -0.740723  0.951188  0.978717  ...   1.401945  -0.270900   \n",
       "186303    -0.838666 -0.905349  2.655245  0.461321  ...   1.073267   0.354717   \n",
       "134045    -1.547900 -1.492419  2.133226  1.513557  ...   0.192120  -0.731990   \n",
       "...             ...       ...       ...       ...  ...        ...        ...   \n",
       "167206    -1.359131 -0.057187 -0.699324 -0.277699  ...   0.967072  -0.407894   \n",
       "40262     -1.988977 -0.020737  0.298276  0.706597  ...   1.235213  -0.146208   \n",
       "7002       0.313828 -0.853430  0.280148 -0.483826  ...   1.114822  -0.845614   \n",
       "124694     0.363122 -0.480910  0.268287 -0.495149  ...   1.396447   0.472859   \n",
       "185394    -1.619887 -0.521737 -0.534672 -2.321126  ...   1.549637  -0.056938   \n",
       "\n",
       "           image_248  image_249  image_250  image_251  image_252  image_253  \\\n",
       "index_key                                                                     \n",
       "19757       0.735540  -2.354414   0.956939  -1.065875  -0.428229  -0.285047   \n",
       "111405     -2.031961  -2.716383   0.817275  -0.434370  -1.456125   0.112614   \n",
       "12269      -1.967142  -0.814089   0.170715   0.335253  -0.030882  -0.557203   \n",
       "186303     -1.745163  -2.610591  -0.239567  -1.714204  -0.914772  -0.354720   \n",
       "134045     -3.023546  -2.410438  -0.701821  -0.403381   0.522945   0.256027   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "167206      0.675619  -2.690143   0.811195   0.162421  -0.319673  -0.655105   \n",
       "40262      -1.484007  -1.698481  -0.935188  -0.480799   0.807755   0.346888   \n",
       "7002       -1.312257  -0.366303  -0.078197   1.005683   0.627290  -1.296975   \n",
       "124694     -1.687484  -2.504541  -0.655285   0.751038   0.302664  -0.154664   \n",
       "185394     -0.942204  -1.869788   0.764314  -1.347982  -0.955746  -0.672004   \n",
       "\n",
       "           image_254  image_255  \n",
       "index_key                        \n",
       "19757       1.098027  -1.029737  \n",
       "111405      0.199815  -2.946273  \n",
       "12269       0.037506  -1.951925  \n",
       "186303      0.098914  -3.272672  \n",
       "134045      1.826598  -3.091393  \n",
       "...              ...        ...  \n",
       "167206      1.837679  -2.292945  \n",
       "40262       1.523457  -1.739759  \n",
       "7002        1.453560  -0.719741  \n",
       "124694      1.174814  -2.710322  \n",
       "185394      1.069264  -1.114935  \n",
       "\n",
       "[21419 rows x 256 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description from NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       'median rating', 'min review date_x', 'min review date_y',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'tokens',\n",
       "       'nostalgia', 'self-published/debut', 'story/anthology',\n",
       "       'womens_fiction', 'childrens_books', 'classic', 'family_drama',\n",
       "       'digital_books/recreations', 'reproduced', 'murder_mystery', 'reprint',\n",
       "       'bestselling_author', 'romance', 'unkonwn', 'teen', 'novel',\n",
       "       'world/war/historical_fiction', 'unknown', 'young_adult',\n",
       "       'coming_of_age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP test\n",
    "NMF_df_test = pd.read_csv('X_test_NMF_topics.csv')\n",
    "\n",
    "# Set indices as in train test split\n",
    "NMF_df_test = NMF_df_test.set_index('index')\n",
    "\n",
    "NMF_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       'median rating', 'min review date_x', 'min review date_y',\n",
       "       'weighted rating', 'date', 'year', 'description_language', 'tokens',\n",
       "       'nostalgia', 'self-published/debut', 'story/anthology',\n",
       "       'womens_fiction', 'childrens_books', 'classic', 'family_drama',\n",
       "       'digital_books/recreations', 'reproduced', 'murder_mystery', 'reprint',\n",
       "       'bestselling_author', 'romance', 'unkonwn', 'teen', 'novel',\n",
       "       'world/war/historical_fiction', 'unknown', 'young_adult',\n",
       "       'coming_of_age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP train\n",
    "NMF_df_train = pd.read_csv('X_train_NMF_topics.csv')\n",
    "\n",
    "NMF_df_train = NMF_df_train.set_index('index')\n",
    "\n",
    "NMF_df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LP - please check!\n",
    "columns_to_keep = ['nostalgia', 'self-published/debut', 'story/anthology',\n",
    "       'womens_fiction', 'childrens_books', 'classic', 'family_drama',\n",
    "       'digital_books/recreations', 'reproduced', 'murder_mystery', 'reprint',\n",
    "       'bestselling_author', 'romance', 'unkonwn', 'teen', 'novel',\n",
    "       'world/war/historical_fiction', 'unknown', 'young_adult',\n",
    "       'coming_of_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_df_train = NMF_df_train[columns_to_keep]\n",
    "NMF_df_test = NMF_df_test[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21419, 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5355, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description from tSDV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'tSVD2991', 'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996',\n",
       "       'tSVD2997', 'tSVD2998', 'tSVD2999', 'tSVD3000'],\n",
       "      dtype='object', length=3018)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP test\n",
    "# LP 8/4 I haven't run this again\n",
    "NLP_df_test = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_test_tSVD_3000.csv')\n",
    ")\n",
    "\n",
    "# Set indices as in train test split\n",
    "NLP_df_test = NLP_df_test.set_index('index')\n",
    "\n",
    "NLP_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>From Potter's Field</td>\n",
       "      <td>The sixth book in the Kay Scarpetta series, fr...</td>\n",
       "      <td>['Patricia Cornwell']</td>\n",
       "      <td>http://books.google.com/books/content?id=prefg...</td>\n",
       "      <td>http://books.google.nl/books?id=prefgSxnGOwC&amp;p...</td>\n",
       "      <td>Hachette UK</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>157</td>\n",
       "      <td>3.783439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.011035</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>-0.009888</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.021092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Riverworld and Other Stories</td>\n",
       "      <td>Three stories of a world shared by resurrected...</td>\n",
       "      <td>['Philip José Farmer']</td>\n",
       "      <td>http://books.google.com/books/content?id=TP4oD...</td>\n",
       "      <td>http://books.google.nl/books?id=TP4oDwAAQBAJ&amp;p...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>7</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.007662</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>-0.007470</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>0.012055</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>0.011430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Kenny Doin' Just Fine</td>\n",
       "      <td>KENNY DOIN' JUST FINE Miriam Greenfield, a pro...</td>\n",
       "      <td>['Sadie Wernick Hurwitz']</td>\n",
       "      <td>http://books.google.com/books/content?id=D6Wgi...</td>\n",
       "      <td>http://books.google.nl/books?id=D6WgitXrr8sC&amp;p...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=D6WgitXrr8sC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>-0.004538</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>-0.015920</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>-0.003591</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>-0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Harry on the Rocks</td>\n",
       "      <td>Harry and his boat become stranded on an islan...</td>\n",
       "      <td>['Susan Meddaugh']</td>\n",
       "      <td>http://books.google.com/books/content?id=u5r79...</td>\n",
       "      <td>http://books.google.nl/books?id=u5r79DAUeIYC&amp;q...</td>\n",
       "      <td>Houghton Mifflin Harcourt</td>\n",
       "      <td>http://books.google.nl/books?id=u5r79DAUeIYC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.003397</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.011397</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>-0.004257</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>-0.002277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>The National Review Treasury of Classic Childr...</td>\n",
       "      <td>A collection of over forty stories, tales, poe...</td>\n",
       "      <td>['William F. Buckley, Jr.']</td>\n",
       "      <td>http://books.google.com/books/content?id=NZm7P...</td>\n",
       "      <td>http://books.google.nl/books?id=NZm7PAAACAAJ&amp;d...</td>\n",
       "      <td>Isi Books</td>\n",
       "      <td>http://books.google.nl/books?id=NZm7PAAACAAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002185</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>-0.000960</td>\n",
       "      <td>-0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212041</th>\n",
       "      <td>Man For Maggie Moore (Montana Matchmakers) (Ha...</td>\n",
       "      <td>You don't know what love is until you have los...</td>\n",
       "      <td>['Steven Labree']</td>\n",
       "      <td>http://books.google.com/books/content?id=NZpeJ...</td>\n",
       "      <td>http://books.google.com/books?id=NZpeJhtmGo8C&amp;...</td>\n",
       "      <td>Steven LaBree</td>\n",
       "      <td>http://books.google.com/books?id=NZpeJhtmGo8C&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>3</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>-0.010747</td>\n",
       "      <td>-0.012375</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>-0.006146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212144</th>\n",
       "      <td>Prancing Tiger</td>\n",
       "      <td>To clear the name of his ex-girlfriend's son, ...</td>\n",
       "      <td>['Philip Singerman']</td>\n",
       "      <td>http://books.google.com/books/content?id=68R7S...</td>\n",
       "      <td>http://books.google.com/books?id=68R7SppHYHcC&amp;...</td>\n",
       "      <td>William Morrow</td>\n",
       "      <td>http://books.google.com/books?id=68R7SppHYHcC&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011748</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>-0.006230</td>\n",
       "      <td>-0.008297</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>-0.002252</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>-0.001916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212256</th>\n",
       "      <td>Nude Men: A Novel</td>\n",
       "      <td>The internationally acclaimed debut of a novel...</td>\n",
       "      <td>['Amanda Filipacchi']</td>\n",
       "      <td>http://books.google.com/books/content?id=uM-1A...</td>\n",
       "      <td>http://books.google.com/books?id=uM-1AwAAQBAJ&amp;...</td>\n",
       "      <td>Open Road Media</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>23</td>\n",
       "      <td>3.739130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010690</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>-0.001923</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>-0.002761</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>0.018299</td>\n",
       "      <td>-0.007997</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.007875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212260</th>\n",
       "      <td>The Tale of Digby</td>\n",
       "      <td>In Digby, Willy Wink finds himself in the midd...</td>\n",
       "      <td>['Timothy Lee Bonnette, Jr.']</td>\n",
       "      <td>http://books.google.com/books/content?id=pcgBA...</td>\n",
       "      <td>http://books.google.com/books?id=pcgBAAAACAAJ&amp;...</td>\n",
       "      <td>Publishamerica Incorporated</td>\n",
       "      <td>http://books.google.com/books?id=pcgBAAAACAAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>0.008438</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>-0.006659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212333</th>\n",
       "      <td>Blueprint For Revenge</td>\n",
       "      <td>Johanna is devastated when her beloved grandmo...</td>\n",
       "      <td>['Martine Jardin']</td>\n",
       "      <td>http://books.google.com/books/content?id=I96XB...</td>\n",
       "      <td>http://books.google.com/books?id=I96XBQAAQBAJ&amp;...</td>\n",
       "      <td>Devine Destinies</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001040</td>\n",
       "      <td>-0.004460</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.008113</td>\n",
       "      <td>-0.002739</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>-0.008496</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5355 rows × 3018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "index                                                       \n",
       "115                                   From Potter's Field   \n",
       "209                          Riverworld and Other Stories   \n",
       "330                                 Kenny Doin' Just Fine   \n",
       "333                                    Harry on the Rocks   \n",
       "371     The National Review Treasury of Classic Childr...   \n",
       "...                                                   ...   \n",
       "212041  Man For Maggie Moore (Montana Matchmakers) (Ha...   \n",
       "212144                                     Prancing Tiger   \n",
       "212256                                  Nude Men: A Novel   \n",
       "212260                                  The Tale of Digby   \n",
       "212333                              Blueprint For Revenge   \n",
       "\n",
       "                                              description  \\\n",
       "index                                                       \n",
       "115     The sixth book in the Kay Scarpetta series, fr...   \n",
       "209     Three stories of a world shared by resurrected...   \n",
       "330     KENNY DOIN' JUST FINE Miriam Greenfield, a pro...   \n",
       "333     Harry and his boat become stranded on an islan...   \n",
       "371     A collection of over forty stories, tales, poe...   \n",
       "...                                                   ...   \n",
       "212041  You don't know what love is until you have los...   \n",
       "212144  To clear the name of his ex-girlfriend's son, ...   \n",
       "212256  The internationally acclaimed debut of a novel...   \n",
       "212260  In Digby, Willy Wink finds himself in the midd...   \n",
       "212333  Johanna is devastated when her beloved grandmo...   \n",
       "\n",
       "                              authors  \\\n",
       "index                                   \n",
       "115             ['Patricia Cornwell']   \n",
       "209            ['Philip José Farmer']   \n",
       "330         ['Sadie Wernick Hurwitz']   \n",
       "333                ['Susan Meddaugh']   \n",
       "371       ['William F. Buckley, Jr.']   \n",
       "...                               ...   \n",
       "212041              ['Steven Labree']   \n",
       "212144           ['Philip Singerman']   \n",
       "212256          ['Amanda Filipacchi']   \n",
       "212260  ['Timothy Lee Bonnette, Jr.']   \n",
       "212333             ['Martine Jardin']   \n",
       "\n",
       "                                                    image  \\\n",
       "index                                                       \n",
       "115     http://books.google.com/books/content?id=prefg...   \n",
       "209     http://books.google.com/books/content?id=TP4oD...   \n",
       "330     http://books.google.com/books/content?id=D6Wgi...   \n",
       "333     http://books.google.com/books/content?id=u5r79...   \n",
       "371     http://books.google.com/books/content?id=NZm7P...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books/content?id=NZpeJ...   \n",
       "212144  http://books.google.com/books/content?id=68R7S...   \n",
       "212256  http://books.google.com/books/content?id=uM-1A...   \n",
       "212260  http://books.google.com/books/content?id=pcgBA...   \n",
       "212333  http://books.google.com/books/content?id=I96XB...   \n",
       "\n",
       "                                              previewLink  \\\n",
       "index                                                       \n",
       "115     http://books.google.nl/books?id=prefgSxnGOwC&p...   \n",
       "209     http://books.google.nl/books?id=TP4oDwAAQBAJ&p...   \n",
       "330     http://books.google.nl/books?id=D6WgitXrr8sC&p...   \n",
       "333     http://books.google.nl/books?id=u5r79DAUeIYC&q...   \n",
       "371     http://books.google.nl/books?id=NZm7PAAACAAJ&d...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books?id=NZpeJhtmGo8C&...   \n",
       "212144  http://books.google.com/books?id=68R7SppHYHcC&...   \n",
       "212256  http://books.google.com/books?id=uM-1AwAAQBAJ&...   \n",
       "212260  http://books.google.com/books?id=pcgBAAAACAAJ&...   \n",
       "212333  http://books.google.com/books?id=I96XBQAAQBAJ&...   \n",
       "\n",
       "                          publisher  \\\n",
       "index                                 \n",
       "115                     Hachette UK   \n",
       "209                 Open Road Media   \n",
       "330                       iUniverse   \n",
       "333       Houghton Mifflin Harcourt   \n",
       "371                       Isi Books   \n",
       "...                             ...   \n",
       "212041                Steven LaBree   \n",
       "212144               William Morrow   \n",
       "212256              Open Road Media   \n",
       "212260  Publishamerica Incorporated   \n",
       "212333             Devine Destinies   \n",
       "\n",
       "                                                 infoLink  \\\n",
       "index                                                       \n",
       "115     https://play.google.com/store/books/details?id...   \n",
       "209     https://play.google.com/store/books/details?id...   \n",
       "330     http://books.google.nl/books?id=D6WgitXrr8sC&d...   \n",
       "333     http://books.google.nl/books?id=u5r79DAUeIYC&d...   \n",
       "371     http://books.google.nl/books?id=NZm7PAAACAAJ&d...   \n",
       "...                                                   ...   \n",
       "212041  http://books.google.com/books?id=NZpeJhtmGo8C&...   \n",
       "212144  http://books.google.com/books?id=68R7SppHYHcC&...   \n",
       "212256  https://play.google.com/store/books/details?id...   \n",
       "212260  http://books.google.com/books?id=pcgBAAAACAAJ&...   \n",
       "212333  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                  categories  reviews number  average rating  ...  tSVD2991  \\\n",
       "index                                                         ...             \n",
       "115              ['fiction']             157        3.783439  ...  0.004673   \n",
       "209              ['fiction']               7        4.285714  ...  0.000682   \n",
       "330              ['fiction']               1        5.000000  ... -0.005573   \n",
       "333     ['juvenile fiction']               2        5.000000  ...  0.003240   \n",
       "371     ['juvenile fiction']               3        5.000000  ... -0.002185   \n",
       "...                      ...             ...             ...  ...       ...   \n",
       "212041           ['fiction']               3        4.666667  ...  0.004356   \n",
       "212144           ['fiction']               4        4.250000  ...  0.011748   \n",
       "212256           ['fiction']              23        3.739130  ... -0.010690   \n",
       "212260           ['fiction']               2        4.000000  ...  0.002488   \n",
       "212333           ['fiction']               1        5.000000  ... -0.001040   \n",
       "\n",
       "        tSVD2992  tSVD2993  tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  \\\n",
       "index                                                                          \n",
       "115     0.011035  0.000885  0.011355  0.003110 -0.009888  0.001707 -0.000568   \n",
       "209    -0.000072 -0.007662  0.010220  0.001084 -0.007470 -0.006064  0.012055   \n",
       "330    -0.004538  0.000019  0.002786 -0.015920  0.004716 -0.000231 -0.003591   \n",
       "333    -0.001592 -0.003397  0.001563  0.008503  0.011397 -0.003413 -0.004257   \n",
       "371    -0.001413  0.003879  0.004376  0.000110  0.002358  0.005980  0.005749   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212041  0.004014  0.000388 -0.010747 -0.012375 -0.001525  0.006544  0.013144   \n",
       "212144 -0.000504  0.006213 -0.006230 -0.008297 -0.009560 -0.002252  0.006993   \n",
       "212256  0.012784 -0.001923  0.008254 -0.002761 -0.003366  0.018299 -0.007997   \n",
       "212260  0.002354  0.004941 -0.000441  0.008438  0.012563  0.002361  0.008794   \n",
       "212333 -0.004460 -0.001457 -0.008113 -0.002739 -0.001419  0.011348 -0.008496   \n",
       "\n",
       "        tSVD2999  tSVD3000  \n",
       "index                       \n",
       "115     0.000850  0.021092  \n",
       "209    -0.007764  0.011430  \n",
       "330     0.000867 -0.001421  \n",
       "333     0.006653 -0.002277  \n",
       "371    -0.000960 -0.003924  \n",
       "...          ...       ...  \n",
       "212041  0.003183 -0.006146  \n",
       "212144  0.007963 -0.001916  \n",
       "212256  0.004508  0.007875  \n",
       "212260  0.000451 -0.006659  \n",
       "212333  0.005376  0.000832  \n",
       "\n",
       "[5355 rows x 3018 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'infoLink', 'categories', 'reviews number', 'average rating',\n",
       "       ...\n",
       "       'tSVD2991', 'tSVD2992', 'tSVD2993', 'tSVD2994', 'tSVD2995', 'tSVD2996',\n",
       "       'tSVD2997', 'tSVD2998', 'tSVD2999', 'tSVD3000'],\n",
       "      dtype='object', length=3018)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description NLP train\n",
    "NLP_df_train = pd.read_csv(\n",
    "    os.path.join(output_folder, 'X_train_tSVD_3000.csv')\n",
    ")\n",
    "\n",
    "NLP_df_train = NLP_df_train.set_index('index')\n",
    "\n",
    "NLP_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews number</th>\n",
       "      <th>average rating</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>32</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Forbidden Stories of Marta Veneranda</td>\n",
       "      <td>Marta Veneranda, a Latina neoyorkina, finds th...</td>\n",
       "      <td>['Sonia Rivera-Valdes']</td>\n",
       "      <td>http://books.google.com/books/content?id=A7aYb...</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;p...</td>\n",
       "      <td>Seven Stories Press</td>\n",
       "      <td>http://books.google.nl/books?id=A7aYbAvagu8C&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Tess and the Highlander</td>\n",
       "      <td>In 1543, on a windswept isle off of Scotland, ...</td>\n",
       "      <td>['May Mcgoldrick']</td>\n",
       "      <td>http://books.google.com/books/content?id=VmCRS...</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>http://books.google.nl/books?id=VmCRSPmY3WkC&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.235294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Eight Men And A Lady (Elizabeth Sinclair, Harl...</td>\n",
       "      <td>Eight Men And A Lady by Elizabeth Sinclair rel...</td>\n",
       "      <td>['Elizabeth Sinclair']</td>\n",
       "      <td>http://books.google.com/books/content?id=Z6uzJ...</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;q...</td>\n",
       "      <td>Harlequin Treasury-Harlequin American Romance 90s</td>\n",
       "      <td>http://books.google.nl/books?id=Z6uzJgLWViUC&amp;d...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Night World: Daughters Of Darkness</td>\n",
       "      <td>\"There’s something strange about the new girls...</td>\n",
       "      <td>['L.J. Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=c9icD...</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;p...</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>http://books.google.nl/books?id=c9icDQAAQBAJ&amp;d...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>134</td>\n",
       "      <td>4.768657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>Calder Pride</td>\n",
       "      <td>The Long-Awaited Addition to the Beloved Calde...</td>\n",
       "      <td>['Janet Dailey']</td>\n",
       "      <td>http://books.google.com/books/content?id=nlsgd...</td>\n",
       "      <td>http://books.google.com/books?id=nlsgd2-kGq4C&amp;...</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>The Road Back</td>\n",
       "      <td>The sequel to the masterpiece All Quiet on the...</td>\n",
       "      <td>['Erich Maria Remarque']</td>\n",
       "      <td>http://books.google.com/books/content?id=obZdA...</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>Random House Trade Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=obZdAAAAQBAJ&amp;...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>17</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>Final things</td>\n",
       "      <td>Grace's father believes in science and builds ...</td>\n",
       "      <td>['Jenny Offill']</td>\n",
       "      <td>http://books.google.com/books/content?id=UbSFB...</td>\n",
       "      <td>http://books.google.com/books?id=UbSFBAAAQBAJ&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>The Orphan Of Ellis Island (Time Travel Advent...</td>\n",
       "      <td>During a school trip to Ellis Island, Dominick...</td>\n",
       "      <td>['Elvira Woodruff']</td>\n",
       "      <td>http://books.google.com/books/content?id=J7M-N...</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://books.google.com/books?id=J7M-NwAACAAJ&amp;...</td>\n",
       "      <td>['juvenile fiction']</td>\n",
       "      <td>28</td>\n",
       "      <td>4.678571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>The Autograph Man</td>\n",
       "      <td>Alex-Li Tandem sells autographs. His business ...</td>\n",
       "      <td>['Zadie Smith']</td>\n",
       "      <td>http://books.google.com/books/content?id=JM6YV...</td>\n",
       "      <td>http://books.google.com/books?id=JM6YVPx_clMC&amp;...</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "index                                                       \n",
       "3                           Whispers of the Wicked Saints   \n",
       "24               The Forbidden Stories of Marta Veneranda   \n",
       "42                                Tess and the Highlander   \n",
       "49      Eight Men And A Lady (Elizabeth Sinclair, Harl...   \n",
       "73                     Night World: Daughters Of Darkness   \n",
       "...                                                   ...   \n",
       "212361                                       Calder Pride   \n",
       "212365                                      The Road Back   \n",
       "212394                                       Final things   \n",
       "212399  The Orphan Of Ellis Island (Time Travel Advent...   \n",
       "212402                                  The Autograph Man   \n",
       "\n",
       "                                              description  \\\n",
       "index                                                       \n",
       "3       Julia Thomas finds her life spinning out of co...   \n",
       "24      Marta Veneranda, a Latina neoyorkina, finds th...   \n",
       "42      In 1543, on a windswept isle off of Scotland, ...   \n",
       "49      Eight Men And A Lady by Elizabeth Sinclair rel...   \n",
       "73      \"There’s something strange about the new girls...   \n",
       "...                                                   ...   \n",
       "212361  The Long-Awaited Addition to the Beloved Calde...   \n",
       "212365  The sequel to the masterpiece All Quiet on the...   \n",
       "212394  Grace's father believes in science and builds ...   \n",
       "212399  During a school trip to Ellis Island, Dominick...   \n",
       "212402  Alex-Li Tandem sells autographs. His business ...   \n",
       "\n",
       "                         authors  \\\n",
       "index                              \n",
       "3            ['Veronica Haddon']   \n",
       "24       ['Sonia Rivera-Valdes']   \n",
       "42            ['May Mcgoldrick']   \n",
       "49        ['Elizabeth Sinclair']   \n",
       "73                ['L.J. Smith']   \n",
       "...                          ...   \n",
       "212361          ['Janet Dailey']   \n",
       "212365  ['Erich Maria Remarque']   \n",
       "212394          ['Jenny Offill']   \n",
       "212399       ['Elvira Woodruff']   \n",
       "212402           ['Zadie Smith']   \n",
       "\n",
       "                                                    image  \\\n",
       "index                                                       \n",
       "3       http://books.google.com/books/content?id=aRSIg...   \n",
       "24      http://books.google.com/books/content?id=A7aYb...   \n",
       "42      http://books.google.com/books/content?id=VmCRS...   \n",
       "49      http://books.google.com/books/content?id=Z6uzJ...   \n",
       "73      http://books.google.com/books/content?id=c9icD...   \n",
       "...                                                   ...   \n",
       "212361  http://books.google.com/books/content?id=nlsgd...   \n",
       "212365  http://books.google.com/books/content?id=obZdA...   \n",
       "212394  http://books.google.com/books/content?id=UbSFB...   \n",
       "212399  http://books.google.com/books/content?id=J7M-N...   \n",
       "212402  http://books.google.com/books/content?id=JM6YV...   \n",
       "\n",
       "                                              previewLink  \\\n",
       "index                                                       \n",
       "3       http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24      http://books.google.nl/books?id=A7aYbAvagu8C&p...   \n",
       "42      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49      http://books.google.nl/books?id=Z6uzJgLWViUC&q...   \n",
       "73      http://books.google.nl/books?id=c9icDQAAQBAJ&p...   \n",
       "...                                                   ...   \n",
       "212361  http://books.google.com/books?id=nlsgd2-kGq4C&...   \n",
       "212365  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394  http://books.google.com/books?id=UbSFBAAAQBAJ&...   \n",
       "212399  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402  http://books.google.com/books?id=JM6YVPx_clMC&...   \n",
       "\n",
       "                                                publisher  \\\n",
       "index                                                       \n",
       "3                                               iUniverse   \n",
       "24                                    Seven Stories Press   \n",
       "42                                         Harper Collins   \n",
       "49      Harlequin Treasury-Harlequin American Romance 90s   \n",
       "73                                     Simon and Schuster   \n",
       "...                                                   ...   \n",
       "212361                                     Harper Collins   \n",
       "212365                      Random House Trade Paperbacks   \n",
       "212394                                            Vintage   \n",
       "212399                              Scholastic Paperbacks   \n",
       "212402                                            Vintage   \n",
       "\n",
       "                                                 infoLink  \\\n",
       "index                                                       \n",
       "3       http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "24      http://books.google.nl/books?id=A7aYbAvagu8C&d...   \n",
       "42      http://books.google.nl/books?id=VmCRSPmY3WkC&d...   \n",
       "49      http://books.google.nl/books?id=Z6uzJgLWViUC&d...   \n",
       "73      http://books.google.nl/books?id=c9icDQAAQBAJ&d...   \n",
       "...                                                   ...   \n",
       "212361  https://play.google.com/store/books/details?id...   \n",
       "212365  http://books.google.com/books?id=obZdAAAAQBAJ&...   \n",
       "212394  https://play.google.com/store/books/details?id...   \n",
       "212399  http://books.google.com/books?id=J7M-NwAACAAJ&...   \n",
       "212402  https://play.google.com/store/books/details?id...   \n",
       "\n",
       "                  categories  reviews number  average rating  ...  tSVD2991  \\\n",
       "index                                                         ...             \n",
       "3                ['fiction']              32        3.718750  ...  0.001714   \n",
       "24               ['fiction']               1        5.000000  ...  0.000900   \n",
       "42      ['juvenile fiction']              17        4.235294  ... -0.002241   \n",
       "49               ['fiction']               2        5.000000  ...  0.004947   \n",
       "73      ['juvenile fiction']             134        4.768657  ...  0.000070   \n",
       "...                      ...             ...             ...  ...       ...   \n",
       "212361           ['fiction']              28        4.035714  ... -0.003849   \n",
       "212365           ['fiction']              17        4.705882  ...  0.014579   \n",
       "212394           ['fiction']               1        4.000000  ...  0.007651   \n",
       "212399  ['juvenile fiction']              28        4.678571  ... -0.012930   \n",
       "212402           ['fiction']               4        2.500000  ... -0.007902   \n",
       "\n",
       "        tSVD2992  tSVD2993  tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  \\\n",
       "index                                                                          \n",
       "3      -0.012096  0.012755  0.004345  0.016297  0.022422  0.026395 -0.001808   \n",
       "24      0.002294 -0.002052 -0.013243 -0.002329 -0.005818 -0.000012  0.007939   \n",
       "42     -0.007230 -0.005164  0.000416 -0.007837 -0.002487 -0.004066  0.010140   \n",
       "49     -0.003206  0.011330 -0.004898 -0.002988  0.001996 -0.008123 -0.000639   \n",
       "73      0.003473 -0.013758  0.000954 -0.011287 -0.008795  0.003780  0.002349   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361 -0.000571  0.001393 -0.001693 -0.012675 -0.000349  0.002634 -0.004115   \n",
       "212365 -0.010071 -0.008840  0.011693  0.006862 -0.001960  0.004995 -0.007247   \n",
       "212394  0.005601  0.000670 -0.010949 -0.012086  0.002205 -0.004838 -0.008230   \n",
       "212399  0.005578 -0.000719 -0.004401  0.003777 -0.001121 -0.006025 -0.007464   \n",
       "212402  0.017672  0.005686  0.007956 -0.003892 -0.007589 -0.002541  0.005038   \n",
       "\n",
       "        tSVD2999  tSVD3000  \n",
       "index                       \n",
       "3      -0.028102  0.011717  \n",
       "24     -0.024247 -0.006868  \n",
       "42     -0.007082  0.001236  \n",
       "49     -0.016424 -0.004971  \n",
       "73     -0.013346  0.001237  \n",
       "...          ...       ...  \n",
       "212361  0.004868 -0.006720  \n",
       "212365 -0.013905 -0.001202  \n",
       "212394  0.000084 -0.002611  \n",
       "212399 -0.008325  0.005250  \n",
       "212402 -0.010622 -0.009332  \n",
       "\n",
       "[21419 rows x 3018 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [col for col in NLP_df_test.columns if col.startswith('tSVD')]\n",
    "\n",
    "NLP_df_train = NLP_df_train[columns_to_keep]\n",
    "NLP_df_test = NLP_df_test[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tSVD1</th>\n",
       "      <th>tSVD2</th>\n",
       "      <th>tSVD3</th>\n",
       "      <th>tSVD4</th>\n",
       "      <th>tSVD5</th>\n",
       "      <th>tSVD6</th>\n",
       "      <th>tSVD7</th>\n",
       "      <th>tSVD8</th>\n",
       "      <th>tSVD9</th>\n",
       "      <th>tSVD10</th>\n",
       "      <th>...</th>\n",
       "      <th>tSVD2991</th>\n",
       "      <th>tSVD2992</th>\n",
       "      <th>tSVD2993</th>\n",
       "      <th>tSVD2994</th>\n",
       "      <th>tSVD2995</th>\n",
       "      <th>tSVD2996</th>\n",
       "      <th>tSVD2997</th>\n",
       "      <th>tSVD2998</th>\n",
       "      <th>tSVD2999</th>\n",
       "      <th>tSVD3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129603</td>\n",
       "      <td>-0.038296</td>\n",
       "      <td>-0.063023</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.033247</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.028102</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105464</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.013301</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110316</td>\n",
       "      <td>-0.032971</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>-0.066746</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.049538</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>-0.049904</td>\n",
       "      <td>-0.034815</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.008123</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.112533</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>-0.013758</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.182936</td>\n",
       "      <td>-0.055312</td>\n",
       "      <td>-0.095027</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.006720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.169263</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.156620</td>\n",
       "      <td>-0.021632</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.044151</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.085933</td>\n",
       "      <td>-0.024211</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.005250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.098494</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.009332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tSVD1     tSVD2     tSVD3     tSVD4     tSVD5     tSVD6     tSVD7  \\\n",
       "index                                                                          \n",
       "3       0.129603 -0.038296 -0.063023  0.002653 -0.035225  0.004487 -0.033247   \n",
       "24      0.105464 -0.019206 -0.016426 -0.003522  0.027940 -0.001382 -0.020538   \n",
       "42      0.110316 -0.032971 -0.038233 -0.010302  0.009022 -0.016867 -0.066746   \n",
       "49      0.043620 -0.010660 -0.049538  0.428472 -0.049904 -0.034815 -0.042501   \n",
       "73      0.112533 -0.023843 -0.024143 -0.009996  0.020481  0.020480  0.000434   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  0.182936 -0.055312 -0.095027  0.006525 -0.008528  0.031805 -0.063892   \n",
       "212365  0.169263 -0.042064  0.007502 -0.012853 -0.016271 -0.000023  0.023768   \n",
       "212394  0.156620 -0.021632 -0.010062 -0.017675  0.014586 -0.020211 -0.044151   \n",
       "212399  0.085933 -0.024211 -0.004185 -0.009729  0.047267 -0.017817 -0.034370   \n",
       "212402  0.098494 -0.025208  0.004084 -0.003334  0.038047 -0.005458  0.009703   \n",
       "\n",
       "           tSVD8     tSVD9    tSVD10  ...  tSVD2991  tSVD2992  tSVD2993  \\\n",
       "index                                 ...                                 \n",
       "3       0.006327  0.040408  0.035931  ...  0.001714 -0.012096  0.012755   \n",
       "24      0.003146 -0.013301 -0.015466  ...  0.000900  0.002294 -0.002052   \n",
       "42      0.042483  0.019502  0.016915  ... -0.002241 -0.007230 -0.005164   \n",
       "49     -0.027805 -0.038459 -0.016390  ...  0.004947 -0.003206  0.011330   \n",
       "73      0.000725 -0.014176 -0.008300  ...  0.000070  0.003473 -0.013758   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  0.001145  0.053651 -0.010439  ... -0.003849 -0.000571  0.001393   \n",
       "212365 -0.028389  0.026683  0.003763  ...  0.014579 -0.010071 -0.008840   \n",
       "212394 -0.082319  0.016963  0.027734  ...  0.007651  0.005601  0.000670   \n",
       "212399 -0.028692 -0.017410  0.020350  ... -0.012930  0.005578 -0.000719   \n",
       "212402 -0.004967 -0.007246 -0.009244  ... -0.007902  0.017672  0.005686   \n",
       "\n",
       "        tSVD2994  tSVD2995  tSVD2996  tSVD2997  tSVD2998  tSVD2999  tSVD3000  \n",
       "index                                                                         \n",
       "3       0.004345  0.016297  0.022422  0.026395 -0.001808 -0.028102  0.011717  \n",
       "24     -0.013243 -0.002329 -0.005818 -0.000012  0.007939 -0.024247 -0.006868  \n",
       "42      0.000416 -0.007837 -0.002487 -0.004066  0.010140 -0.007082  0.001236  \n",
       "49     -0.004898 -0.002988  0.001996 -0.008123 -0.000639 -0.016424 -0.004971  \n",
       "73      0.000954 -0.011287 -0.008795  0.003780  0.002349 -0.013346  0.001237  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "212361 -0.001693 -0.012675 -0.000349  0.002634 -0.004115  0.004868 -0.006720  \n",
       "212365  0.011693  0.006862 -0.001960  0.004995 -0.007247 -0.013905 -0.001202  \n",
       "212394 -0.010949 -0.012086  0.002205 -0.004838 -0.008230  0.000084 -0.002611  \n",
       "212399 -0.004401  0.003777 -0.001121 -0.006025 -0.007464 -0.008325  0.005250  \n",
       "212402  0.007956 -0.003892 -0.007589 -0.002541  0.005038 -0.010622 -0.009332  \n",
       "\n",
       "[21419 rows x 3000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description dimension reduction NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Questions/notes:\n",
    "Inputs to choose:\n",
    "- number of layers:\n",
    "    - Description NN\n",
    "        - input\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - final layer\n",
    "    - Description and image embeddings NN\n",
    "        - input\n",
    "        - noise\n",
    "        - hidden layer\n",
    "        - noise\n",
    "        - final layer\n",
    "    Too many?   \n",
    "- add dense layers to avoid overfitting?\n",
    "- activation functions\n",
    "    - ReLu (Rectified linear activation function): piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. Simple but effective.\n",
    "- Use linear in the last layer to obtain a continuous variable\n",
    "- optimizer: \n",
    "    - Adam; works with momentums of first and second order. \n",
    "    - sdg: variant of Gradient Descent (Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. It's easy and works well but there is the risk that the model gets stuck in local minima)\n",
    "- loss function\n",
    "    - MSE?\n",
    "- number of epochs\n",
    "- which metric to use to evaluate the model?\n",
    "    - MSE\n",
    "    - MAE\n",
    "\n",
    "- Use gridsearch to optimise hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,752</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m10,752\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">142,080</span> (555.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m142,080\u001b[0m (555.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">142,080</span> (555.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m142,080\u001b[0m (555.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get number of inputs - second element of shape (i.e. number of columns in X)\n",
    "input_shape = NMF_df_train.shape[1]\n",
    "\n",
    "\n",
    "# neurons number\n",
    "n_neurons = 512\n",
    "\n",
    "# define a model\n",
    "baseline_model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "baseline_model.add(layers.Dense(\n",
    "            n_neurons, # number of neurons\n",
    "            input_dim = input_shape, # number of inputs \n",
    "            activation = 'relu' # activation faunction\n",
    "            ))\n",
    "\n",
    "# Hidden - Layers\n",
    "baseline_model.add(layers.Dense(\n",
    "                    256, \n",
    "                    activation = \"linear\"))\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mean_squared_error'], \n",
    "    metrics = ['mae', 'mean_squared_error']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.0495 - mae: 1.7191 - mean_squared_error: 6.0495 - val_loss: 0.0541 - val_mae: 0.1541 - val_mean_squared_error: 0.0524\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0482 - mae: 0.1492 - mean_squared_error: 0.0482 - val_loss: 0.0483 - val_mae: 0.1429 - val_mean_squared_error: 0.0467\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0439 - mae: 0.1393 - mean_squared_error: 0.0439 - val_loss: 0.0470 - val_mae: 0.1407 - val_mean_squared_error: 0.0454\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1370 - mean_squared_error: 0.0442 - val_loss: 0.0473 - val_mae: 0.1319 - val_mean_squared_error: 0.0454\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1376 - mean_squared_error: 0.0442 - val_loss: 0.0467 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0458 - mae: 0.1379 - mean_squared_error: 0.0458 - val_loss: 0.0468 - val_mae: 0.1390 - val_mean_squared_error: 0.0450\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0430 - mae: 0.1351 - mean_squared_error: 0.0430 - val_loss: 0.0473 - val_mae: 0.1311 - val_mean_squared_error: 0.0455\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0446 - mae: 0.1387 - mean_squared_error: 0.0446 - val_loss: 0.0471 - val_mae: 0.1322 - val_mean_squared_error: 0.0453\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0438 - mae: 0.1368 - mean_squared_error: 0.0438 - val_loss: 0.0467 - val_mae: 0.1359 - val_mean_squared_error: 0.0450\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0423 - mae: 0.1343 - mean_squared_error: 0.0423 - val_loss: 0.0475 - val_mae: 0.1311 - val_mean_squared_error: 0.0457\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0439 - mae: 0.1369 - mean_squared_error: 0.0439 - val_loss: 0.0490 - val_mae: 0.1553 - val_mean_squared_error: 0.0474\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0448 - mae: 0.1388 - mean_squared_error: 0.0448 - val_loss: 0.0470 - val_mae: 0.1335 - val_mean_squared_error: 0.0452\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0442 - mae: 0.1382 - mean_squared_error: 0.0442 - val_loss: 0.0478 - val_mae: 0.1310 - val_mean_squared_error: 0.0459\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0440 - mae: 0.1382 - mean_squared_error: 0.0440 - val_loss: 0.0471 - val_mae: 0.1335 - val_mean_squared_error: 0.0453\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0428 - mae: 0.1348 - mean_squared_error: 0.0428 - val_loss: 0.0474 - val_mae: 0.1461 - val_mean_squared_error: 0.0458\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0425 - mae: 0.1343 - mean_squared_error: 0.0425 - val_loss: 0.0470 - val_mae: 0.1404 - val_mean_squared_error: 0.0453\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0454 - mae: 0.1391 - mean_squared_error: 0.0454 - val_loss: 0.0492 - val_mae: 0.1302 - val_mean_squared_error: 0.0473\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0448 - mae: 0.1385 - mean_squared_error: 0.0448 - val_loss: 0.0468 - val_mae: 0.1385 - val_mean_squared_error: 0.0451\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0437 - mae: 0.1361 - mean_squared_error: 0.0437 - val_loss: 0.0470 - val_mae: 0.1321 - val_mean_squared_error: 0.0452\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0439 - mae: 0.1375 - mean_squared_error: 0.0439 - val_loss: 0.0471 - val_mae: 0.1439 - val_mean_squared_error: 0.0455\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0448 - mae: 0.1392 - mean_squared_error: 0.0448 - val_loss: 0.0470 - val_mae: 0.1425 - val_mean_squared_error: 0.0453\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1389 - mean_squared_error: 0.0442 - val_loss: 0.0467 - val_mae: 0.1386 - val_mean_squared_error: 0.0450\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0437 - mae: 0.1367 - mean_squared_error: 0.0437 - val_loss: 0.0489 - val_mae: 0.1550 - val_mean_squared_error: 0.0474\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0441 - mae: 0.1373 - mean_squared_error: 0.0441 - val_loss: 0.0501 - val_mae: 0.1607 - val_mean_squared_error: 0.0485\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0431 - mae: 0.1364 - mean_squared_error: 0.0431 - val_loss: 0.0467 - val_mae: 0.1374 - val_mean_squared_error: 0.0450\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0452 - mae: 0.1386 - mean_squared_error: 0.0452 - val_loss: 0.0499 - val_mae: 0.1307 - val_mean_squared_error: 0.0480\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1356 - mean_squared_error: 0.0425 - val_loss: 0.0490 - val_mae: 0.1305 - val_mean_squared_error: 0.0471\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1376 - mean_squared_error: 0.0442 - val_loss: 0.0469 - val_mae: 0.1329 - val_mean_squared_error: 0.0451\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0459 - mae: 0.1383 - mean_squared_error: 0.0459 - val_loss: 0.0468 - val_mae: 0.1389 - val_mean_squared_error: 0.0451\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0435 - mae: 0.1370 - mean_squared_error: 0.0435 - val_loss: 0.0468 - val_mae: 0.1353 - val_mean_squared_error: 0.0450\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0441 - mae: 0.1379 - mean_squared_error: 0.0441 - val_loss: 0.0499 - val_mae: 0.1307 - val_mean_squared_error: 0.0480\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0444 - mae: 0.1380 - mean_squared_error: 0.0444 - val_loss: 0.0519 - val_mae: 0.1678 - val_mean_squared_error: 0.0504\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0443 - mae: 0.1389 - mean_squared_error: 0.0443 - val_loss: 0.0476 - val_mae: 0.1312 - val_mean_squared_error: 0.0457\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0434 - mae: 0.1377 - mean_squared_error: 0.0434 - val_loss: 0.0471 - val_mae: 0.1422 - val_mean_squared_error: 0.0454\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0435 - mae: 0.1376 - mean_squared_error: 0.0435 - val_loss: 0.0468 - val_mae: 0.1345 - val_mean_squared_error: 0.0450\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0451 - mae: 0.1392 - mean_squared_error: 0.0451 - val_loss: 0.0469 - val_mae: 0.1352 - val_mean_squared_error: 0.0451\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0432 - mae: 0.1366 - mean_squared_error: 0.0432 - val_loss: 0.0498 - val_mae: 0.1310 - val_mean_squared_error: 0.0478\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0429 - mae: 0.1353 - mean_squared_error: 0.0429 - val_loss: 0.0476 - val_mae: 0.1322 - val_mean_squared_error: 0.0458\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0443 - mae: 0.1379 - mean_squared_error: 0.0443 - val_loss: 0.0473 - val_mae: 0.1448 - val_mean_squared_error: 0.0456\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0432 - mae: 0.1374 - mean_squared_error: 0.0432 - val_loss: 0.0475 - val_mae: 0.1312 - val_mean_squared_error: 0.0457\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0459 - mae: 0.1388 - mean_squared_error: 0.0459 - val_loss: 0.0471 - val_mae: 0.1324 - val_mean_squared_error: 0.0454\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0431 - mae: 0.1355 - mean_squared_error: 0.0431 - val_loss: 0.0485 - val_mae: 0.1303 - val_mean_squared_error: 0.0466\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0463 - mae: 0.1389 - mean_squared_error: 0.0463 - val_loss: 0.0482 - val_mae: 0.1306 - val_mean_squared_error: 0.0463\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0452 - mae: 0.1400 - mean_squared_error: 0.0452 - val_loss: 0.0470 - val_mae: 0.1335 - val_mean_squared_error: 0.0452\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0462 - mae: 0.1402 - mean_squared_error: 0.0462 - val_loss: 0.0468 - val_mae: 0.1396 - val_mean_squared_error: 0.0451\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0435 - mae: 0.1373 - mean_squared_error: 0.0435 - val_loss: 0.0468 - val_mae: 0.1350 - val_mean_squared_error: 0.0450\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1367 - mean_squared_error: 0.0440 - val_loss: 0.0481 - val_mae: 0.1310 - val_mean_squared_error: 0.0463\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0438 - mae: 0.1375 - mean_squared_error: 0.0438 - val_loss: 0.0470 - val_mae: 0.1397 - val_mean_squared_error: 0.0453\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0447 - mae: 0.1379 - mean_squared_error: 0.0447 - val_loss: 0.0474 - val_mae: 0.1454 - val_mean_squared_error: 0.0458\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.1377 - mean_squared_error: 0.0441 - val_loss: 0.0471 - val_mae: 0.1432 - val_mean_squared_error: 0.0454\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0437 - mae: 0.1367 - mean_squared_error: 0.0437 - val_loss: 0.0469 - val_mae: 0.1406 - val_mean_squared_error: 0.0452\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.1367 - mean_squared_error: 0.0441 - val_loss: 0.0502 - val_mae: 0.1602 - val_mean_squared_error: 0.0486\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0428 - mae: 0.1362 - mean_squared_error: 0.0428 - val_loss: 0.0476 - val_mae: 0.1319 - val_mean_squared_error: 0.0458\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.1370 - mean_squared_error: 0.0441 - val_loss: 0.0470 - val_mae: 0.1404 - val_mean_squared_error: 0.0452\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0446 - mae: 0.1382 - mean_squared_error: 0.0446 - val_loss: 0.0477 - val_mae: 0.1316 - val_mean_squared_error: 0.0459\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.1373 - mean_squared_error: 0.0441 - val_loss: 0.0470 - val_mae: 0.1401 - val_mean_squared_error: 0.0453\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1382 - mean_squared_error: 0.0442 - val_loss: 0.0491 - val_mae: 0.1305 - val_mean_squared_error: 0.0472\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0423 - mae: 0.1349 - mean_squared_error: 0.0423 - val_loss: 0.0470 - val_mae: 0.1346 - val_mean_squared_error: 0.0452\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1362 - mean_squared_error: 0.0429 - val_loss: 0.0505 - val_mae: 0.1314 - val_mean_squared_error: 0.0486\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0451 - mae: 0.1393 - mean_squared_error: 0.0451 - val_loss: 0.0471 - val_mae: 0.1433 - val_mean_squared_error: 0.0455\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0452 - mae: 0.1389 - mean_squared_error: 0.0452 - val_loss: 0.0469 - val_mae: 0.1370 - val_mean_squared_error: 0.0452\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0415 - mae: 0.1345 - mean_squared_error: 0.0415 - val_loss: 0.0496 - val_mae: 0.1309 - val_mean_squared_error: 0.0477\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0426 - mae: 0.1355 - mean_squared_error: 0.0426 - val_loss: 0.0485 - val_mae: 0.1523 - val_mean_squared_error: 0.0469\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0456 - mae: 0.1408 - mean_squared_error: 0.0456 - val_loss: 0.0474 - val_mae: 0.1437 - val_mean_squared_error: 0.0457\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0439 - mae: 0.1364 - mean_squared_error: 0.0439 - val_loss: 0.0479 - val_mae: 0.1314 - val_mean_squared_error: 0.0460\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0424 - mae: 0.1354 - mean_squared_error: 0.0424 - val_loss: 0.0469 - val_mae: 0.1395 - val_mean_squared_error: 0.0452\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0455 - mae: 0.1399 - mean_squared_error: 0.0455 - val_loss: 0.0471 - val_mae: 0.1353 - val_mean_squared_error: 0.0453\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0439 - mae: 0.1388 - mean_squared_error: 0.0439 - val_loss: 0.0470 - val_mae: 0.1396 - val_mean_squared_error: 0.0453\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0446 - mae: 0.1384 - mean_squared_error: 0.0446 - val_loss: 0.0470 - val_mae: 0.1374 - val_mean_squared_error: 0.0452\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0454 - mae: 0.1383 - mean_squared_error: 0.0454 - val_loss: 0.0471 - val_mae: 0.1396 - val_mean_squared_error: 0.0453\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0439 - mae: 0.1386 - mean_squared_error: 0.0439 - val_loss: 0.0469 - val_mae: 0.1374 - val_mean_squared_error: 0.0452\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0450 - mae: 0.1373 - mean_squared_error: 0.0450 - val_loss: 0.0469 - val_mae: 0.1372 - val_mean_squared_error: 0.0452\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0464 - mae: 0.1390 - mean_squared_error: 0.0464 - val_loss: 0.0477 - val_mae: 0.1475 - val_mean_squared_error: 0.0461\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0451 - mae: 0.1402 - mean_squared_error: 0.0451 - val_loss: 0.0474 - val_mae: 0.1431 - val_mean_squared_error: 0.0456\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1377 - mean_squared_error: 0.0440 - val_loss: 0.0469 - val_mae: 0.1361 - val_mean_squared_error: 0.0451\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0403 - mae: 0.1327 - mean_squared_error: 0.0403 - val_loss: 0.0478 - val_mae: 0.1317 - val_mean_squared_error: 0.0460\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0444 - mae: 0.1377 - mean_squared_error: 0.0444 - val_loss: 0.0487 - val_mae: 0.1311 - val_mean_squared_error: 0.0468\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1364 - mean_squared_error: 0.0427 - val_loss: 0.0473 - val_mae: 0.1341 - val_mean_squared_error: 0.0456\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.1388 - mean_squared_error: 0.0441 - val_loss: 0.0473 - val_mae: 0.1342 - val_mean_squared_error: 0.0456\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0457 - mae: 0.1373 - mean_squared_error: 0.0457 - val_loss: 0.0514 - val_mae: 0.1331 - val_mean_squared_error: 0.0495\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0461 - mae: 0.1398 - mean_squared_error: 0.0461 - val_loss: 0.0477 - val_mae: 0.1467 - val_mean_squared_error: 0.0460\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0430 - mae: 0.1359 - mean_squared_error: 0.0430 - val_loss: 0.0470 - val_mae: 0.1362 - val_mean_squared_error: 0.0453\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0430 - mae: 0.1363 - mean_squared_error: 0.0430 - val_loss: 0.0483 - val_mae: 0.1312 - val_mean_squared_error: 0.0464\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0445 - mae: 0.1380 - mean_squared_error: 0.0445 - val_loss: 0.0475 - val_mae: 0.1441 - val_mean_squared_error: 0.0458\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0426 - mae: 0.1364 - mean_squared_error: 0.0426 - val_loss: 0.0491 - val_mae: 0.1310 - val_mean_squared_error: 0.0472\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0430 - mae: 0.1363 - mean_squared_error: 0.0430 - val_loss: 0.0478 - val_mae: 0.1321 - val_mean_squared_error: 0.0460\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0443 - mae: 0.1384 - mean_squared_error: 0.0443 - val_loss: 0.0479 - val_mae: 0.1319 - val_mean_squared_error: 0.0461\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0421 - mae: 0.1360 - mean_squared_error: 0.0421 - val_loss: 0.0481 - val_mae: 0.1482 - val_mean_squared_error: 0.0464\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0436 - mae: 0.1381 - mean_squared_error: 0.0436 - val_loss: 0.0478 - val_mae: 0.1317 - val_mean_squared_error: 0.0460\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0433 - mae: 0.1359 - mean_squared_error: 0.0433 - val_loss: 0.0474 - val_mae: 0.1431 - val_mean_squared_error: 0.0457\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0425 - mae: 0.1365 - mean_squared_error: 0.0425 - val_loss: 0.0471 - val_mae: 0.1375 - val_mean_squared_error: 0.0454\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0452 - mae: 0.1408 - mean_squared_error: 0.0452 - val_loss: 0.0486 - val_mae: 0.1313 - val_mean_squared_error: 0.0467\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.1366 - mean_squared_error: 0.0441 - val_loss: 0.0472 - val_mae: 0.1422 - val_mean_squared_error: 0.0455\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0438 - mae: 0.1385 - mean_squared_error: 0.0438 - val_loss: 0.0479 - val_mae: 0.1324 - val_mean_squared_error: 0.0460\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0449 - mae: 0.1384 - mean_squared_error: 0.0449 - val_loss: 0.0498 - val_mae: 0.1313 - val_mean_squared_error: 0.0479\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0442 - mae: 0.1389 - mean_squared_error: 0.0442 - val_loss: 0.0471 - val_mae: 0.1369 - val_mean_squared_error: 0.0454\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1374 - mean_squared_error: 0.0442 - val_loss: 0.0478 - val_mae: 0.1322 - val_mean_squared_error: 0.0460\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0432 - mae: 0.1374 - mean_squared_error: 0.0432 - val_loss: 0.0489 - val_mae: 0.1318 - val_mean_squared_error: 0.0470\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1364 - mean_squared_error: 0.0427 - val_loss: 0.0474 - val_mae: 0.1335 - val_mean_squared_error: 0.0456\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0464 - mae: 0.1392 - mean_squared_error: 0.0464 - val_loss: 0.0524 - val_mae: 0.1688 - val_mean_squared_error: 0.0509\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs_hist = baseline_model.fit(\n",
    "    NMF_df_train, # input\n",
    "    y_wr_train, # output\n",
    "    epochs=100, # number of iterations\n",
    "    batch_size=50, # number of observations taken to train the data\n",
    "    verbose=1,\n",
    "    validation_data = (NMF_df_test, y_wr_test),\n",
    "    shuffle = True\n",
    "    #validation_split=0.2,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate intermediate description features with lower dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m670/670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step\n"
     ]
    }
   ],
   "source": [
    "# Predict baseline X train and X test \n",
    "\n",
    "NMF_intermediate_train = baseline_model.predict(NMF_df_train)\n",
    "NMF_intermediate_test = baseline_model.predict(NMF_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.188931 , 4.187715 , 4.19012  , ..., 4.1894956, 4.189237 ,\n",
       "        4.1885624],\n",
       "       [4.1719155, 4.171172 , 4.17062  , ..., 4.1713405, 4.170967 ,\n",
       "        4.1710534],\n",
       "       [4.1882606, 4.188523 , 4.1909156, ..., 4.1885986, 4.187873 ,\n",
       "        4.190593 ],\n",
       "       ...,\n",
       "       [4.213178 , 4.210014 , 4.2128134, ..., 4.211203 , 4.21     ,\n",
       "        4.2129335],\n",
       "       [4.199598 , 4.2013216, 4.200653 , ..., 4.197797 , 4.198517 ,\n",
       "        4.201688 ],\n",
       "       [4.166425 , 4.16632  , 4.165205 , ..., 4.1662045, 4.166108 ,\n",
       "        4.166419 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_intermediate_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NMF_intermediate_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store these into a dataframe\n",
    "NMF_intermediate_train_df = pd.DataFrame(NMF_intermediate_train, index=NMF_df_train.index)\n",
    "NMF_intermediate_test_df = pd.DataFrame(NMF_intermediate_test, index=NMF_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.188931</td>\n",
       "      <td>4.187715</td>\n",
       "      <td>4.190120</td>\n",
       "      <td>4.189871</td>\n",
       "      <td>4.188741</td>\n",
       "      <td>4.191274</td>\n",
       "      <td>4.187556</td>\n",
       "      <td>4.189681</td>\n",
       "      <td>4.189250</td>\n",
       "      <td>4.190693</td>\n",
       "      <td>...</td>\n",
       "      <td>4.188763</td>\n",
       "      <td>4.188751</td>\n",
       "      <td>4.188924</td>\n",
       "      <td>4.188408</td>\n",
       "      <td>4.188720</td>\n",
       "      <td>4.190204</td>\n",
       "      <td>4.190247</td>\n",
       "      <td>4.189496</td>\n",
       "      <td>4.189237</td>\n",
       "      <td>4.188562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.171916</td>\n",
       "      <td>4.171172</td>\n",
       "      <td>4.170620</td>\n",
       "      <td>4.172662</td>\n",
       "      <td>4.171742</td>\n",
       "      <td>4.171898</td>\n",
       "      <td>4.171084</td>\n",
       "      <td>4.170638</td>\n",
       "      <td>4.171104</td>\n",
       "      <td>4.171372</td>\n",
       "      <td>...</td>\n",
       "      <td>4.171973</td>\n",
       "      <td>4.171215</td>\n",
       "      <td>4.171245</td>\n",
       "      <td>4.172563</td>\n",
       "      <td>4.171942</td>\n",
       "      <td>4.171228</td>\n",
       "      <td>4.171238</td>\n",
       "      <td>4.171340</td>\n",
       "      <td>4.170967</td>\n",
       "      <td>4.171053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.188261</td>\n",
       "      <td>4.188523</td>\n",
       "      <td>4.190916</td>\n",
       "      <td>4.189618</td>\n",
       "      <td>4.189355</td>\n",
       "      <td>4.191080</td>\n",
       "      <td>4.190304</td>\n",
       "      <td>4.188797</td>\n",
       "      <td>4.190788</td>\n",
       "      <td>4.190187</td>\n",
       "      <td>...</td>\n",
       "      <td>4.188482</td>\n",
       "      <td>4.191442</td>\n",
       "      <td>4.189374</td>\n",
       "      <td>4.187205</td>\n",
       "      <td>4.188237</td>\n",
       "      <td>4.192234</td>\n",
       "      <td>4.189757</td>\n",
       "      <td>4.188599</td>\n",
       "      <td>4.187873</td>\n",
       "      <td>4.190593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.132412</td>\n",
       "      <td>4.128396</td>\n",
       "      <td>4.129284</td>\n",
       "      <td>4.131897</td>\n",
       "      <td>4.131412</td>\n",
       "      <td>4.134237</td>\n",
       "      <td>4.129202</td>\n",
       "      <td>4.130387</td>\n",
       "      <td>4.128264</td>\n",
       "      <td>4.132311</td>\n",
       "      <td>...</td>\n",
       "      <td>4.127559</td>\n",
       "      <td>4.132206</td>\n",
       "      <td>4.130312</td>\n",
       "      <td>4.131269</td>\n",
       "      <td>4.130145</td>\n",
       "      <td>4.131634</td>\n",
       "      <td>4.133309</td>\n",
       "      <td>4.128151</td>\n",
       "      <td>4.128461</td>\n",
       "      <td>4.131330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4.175608</td>\n",
       "      <td>4.176218</td>\n",
       "      <td>4.175508</td>\n",
       "      <td>4.175902</td>\n",
       "      <td>4.176369</td>\n",
       "      <td>4.177127</td>\n",
       "      <td>4.176533</td>\n",
       "      <td>4.175392</td>\n",
       "      <td>4.176219</td>\n",
       "      <td>4.176759</td>\n",
       "      <td>...</td>\n",
       "      <td>4.176504</td>\n",
       "      <td>4.175831</td>\n",
       "      <td>4.176373</td>\n",
       "      <td>4.176002</td>\n",
       "      <td>4.175429</td>\n",
       "      <td>4.177463</td>\n",
       "      <td>4.176657</td>\n",
       "      <td>4.175189</td>\n",
       "      <td>4.176006</td>\n",
       "      <td>4.175946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>4.205028</td>\n",
       "      <td>4.204352</td>\n",
       "      <td>4.204837</td>\n",
       "      <td>4.206960</td>\n",
       "      <td>4.205595</td>\n",
       "      <td>4.208562</td>\n",
       "      <td>4.204079</td>\n",
       "      <td>4.204986</td>\n",
       "      <td>4.205691</td>\n",
       "      <td>4.205834</td>\n",
       "      <td>...</td>\n",
       "      <td>4.205660</td>\n",
       "      <td>4.205583</td>\n",
       "      <td>4.205417</td>\n",
       "      <td>4.206025</td>\n",
       "      <td>4.205146</td>\n",
       "      <td>4.206812</td>\n",
       "      <td>4.206141</td>\n",
       "      <td>4.204774</td>\n",
       "      <td>4.204865</td>\n",
       "      <td>4.204173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>4.143547</td>\n",
       "      <td>4.144094</td>\n",
       "      <td>4.144294</td>\n",
       "      <td>4.144543</td>\n",
       "      <td>4.145777</td>\n",
       "      <td>4.145216</td>\n",
       "      <td>4.145878</td>\n",
       "      <td>4.145141</td>\n",
       "      <td>4.145458</td>\n",
       "      <td>4.143955</td>\n",
       "      <td>...</td>\n",
       "      <td>4.144623</td>\n",
       "      <td>4.144730</td>\n",
       "      <td>4.144679</td>\n",
       "      <td>4.144664</td>\n",
       "      <td>4.144484</td>\n",
       "      <td>4.145404</td>\n",
       "      <td>4.143905</td>\n",
       "      <td>4.144335</td>\n",
       "      <td>4.144312</td>\n",
       "      <td>4.145149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>4.213178</td>\n",
       "      <td>4.210014</td>\n",
       "      <td>4.212813</td>\n",
       "      <td>4.213948</td>\n",
       "      <td>4.213171</td>\n",
       "      <td>4.214641</td>\n",
       "      <td>4.213249</td>\n",
       "      <td>4.212882</td>\n",
       "      <td>4.212253</td>\n",
       "      <td>4.215130</td>\n",
       "      <td>...</td>\n",
       "      <td>4.212443</td>\n",
       "      <td>4.212058</td>\n",
       "      <td>4.211862</td>\n",
       "      <td>4.211416</td>\n",
       "      <td>4.210738</td>\n",
       "      <td>4.213229</td>\n",
       "      <td>4.212811</td>\n",
       "      <td>4.211203</td>\n",
       "      <td>4.210000</td>\n",
       "      <td>4.212934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>4.199598</td>\n",
       "      <td>4.201322</td>\n",
       "      <td>4.200653</td>\n",
       "      <td>4.202355</td>\n",
       "      <td>4.201696</td>\n",
       "      <td>4.202088</td>\n",
       "      <td>4.202820</td>\n",
       "      <td>4.197803</td>\n",
       "      <td>4.202850</td>\n",
       "      <td>4.203968</td>\n",
       "      <td>...</td>\n",
       "      <td>4.200421</td>\n",
       "      <td>4.200665</td>\n",
       "      <td>4.202033</td>\n",
       "      <td>4.199351</td>\n",
       "      <td>4.197876</td>\n",
       "      <td>4.204221</td>\n",
       "      <td>4.200559</td>\n",
       "      <td>4.197797</td>\n",
       "      <td>4.198517</td>\n",
       "      <td>4.201688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>4.166425</td>\n",
       "      <td>4.166320</td>\n",
       "      <td>4.165205</td>\n",
       "      <td>4.167020</td>\n",
       "      <td>4.166490</td>\n",
       "      <td>4.165708</td>\n",
       "      <td>4.166113</td>\n",
       "      <td>4.165818</td>\n",
       "      <td>4.166570</td>\n",
       "      <td>4.165198</td>\n",
       "      <td>...</td>\n",
       "      <td>4.167098</td>\n",
       "      <td>4.166414</td>\n",
       "      <td>4.166605</td>\n",
       "      <td>4.166739</td>\n",
       "      <td>4.166846</td>\n",
       "      <td>4.165773</td>\n",
       "      <td>4.165913</td>\n",
       "      <td>4.166204</td>\n",
       "      <td>4.166108</td>\n",
       "      <td>4.166419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "index                                                                          \n",
       "3       4.188931  4.187715  4.190120  4.189871  4.188741  4.191274  4.187556   \n",
       "24      4.171916  4.171172  4.170620  4.172662  4.171742  4.171898  4.171084   \n",
       "42      4.188261  4.188523  4.190916  4.189618  4.189355  4.191080  4.190304   \n",
       "49      4.132412  4.128396  4.129284  4.131897  4.131412  4.134237  4.129202   \n",
       "73      4.175608  4.176218  4.175508  4.175902  4.176369  4.177127  4.176533   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "212361  4.205028  4.204352  4.204837  4.206960  4.205595  4.208562  4.204079   \n",
       "212365  4.143547  4.144094  4.144294  4.144543  4.145777  4.145216  4.145878   \n",
       "212394  4.213178  4.210014  4.212813  4.213948  4.213171  4.214641  4.213249   \n",
       "212399  4.199598  4.201322  4.200653  4.202355  4.201696  4.202088  4.202820   \n",
       "212402  4.166425  4.166320  4.165205  4.167020  4.166490  4.165708  4.166113   \n",
       "\n",
       "             7         8         9    ...       246       247       248  \\\n",
       "index                                 ...                                 \n",
       "3       4.189681  4.189250  4.190693  ...  4.188763  4.188751  4.188924   \n",
       "24      4.170638  4.171104  4.171372  ...  4.171973  4.171215  4.171245   \n",
       "42      4.188797  4.190788  4.190187  ...  4.188482  4.191442  4.189374   \n",
       "49      4.130387  4.128264  4.132311  ...  4.127559  4.132206  4.130312   \n",
       "73      4.175392  4.176219  4.176759  ...  4.176504  4.175831  4.176373   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "212361  4.204986  4.205691  4.205834  ...  4.205660  4.205583  4.205417   \n",
       "212365  4.145141  4.145458  4.143955  ...  4.144623  4.144730  4.144679   \n",
       "212394  4.212882  4.212253  4.215130  ...  4.212443  4.212058  4.211862   \n",
       "212399  4.197803  4.202850  4.203968  ...  4.200421  4.200665  4.202033   \n",
       "212402  4.165818  4.166570  4.165198  ...  4.167098  4.166414  4.166605   \n",
       "\n",
       "             249       250       251       252       253       254       255  \n",
       "index                                                                         \n",
       "3       4.188408  4.188720  4.190204  4.190247  4.189496  4.189237  4.188562  \n",
       "24      4.172563  4.171942  4.171228  4.171238  4.171340  4.170967  4.171053  \n",
       "42      4.187205  4.188237  4.192234  4.189757  4.188599  4.187873  4.190593  \n",
       "49      4.131269  4.130145  4.131634  4.133309  4.128151  4.128461  4.131330  \n",
       "73      4.176002  4.175429  4.177463  4.176657  4.175189  4.176006  4.175946  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "212361  4.206025  4.205146  4.206812  4.206141  4.204774  4.204865  4.204173  \n",
       "212365  4.144664  4.144484  4.145404  4.143905  4.144335  4.144312  4.145149  \n",
       "212394  4.211416  4.210738  4.213229  4.212811  4.211203  4.210000  4.212934  \n",
       "212399  4.199351  4.197876  4.204221  4.200559  4.197797  4.198517  4.201688  \n",
       "212402  4.166739  4.166846  4.165773  4.165913  4.166204  4.166108  4.166419  \n",
       "\n",
       "[21419 rows x 256 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that indices are correct\n",
    "NMF_intermediate_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y cuts\n",
    "We are going to run two models for two target variables\n",
    "- Target variable: Average rating\n",
    "  - baseline (i.e. excluding image embeddings)\n",
    "  - including image embeddings\n",
    "- Target variable: weighted rating\n",
    "  - baseline (i.e. excluding image embeddings)\n",
    "  - including image embeddings\n",
    "\n",
    "We therefore need to create the following datsets\n",
    "- X train and X test with embeddings\n",
    "- X train and X text without embeddings\n",
    "- y train and y test using average rating\n",
    "- y train and y test using weighted rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\t Dense\t Dropout\t NMF_df_test\t NMF_df_train\t NMF_intermediate_test\t NMF_intermediate_test_df\t NMF_intermediate_train\t NMF_intermediate_train_df\t \n",
      "Pipeline\t RandomForestRegressor\t SGD\t SVR\t StandardScaler\t X\t X_columns\t X_images_test\t X_images_train\t \n",
      "X_test\t X_train\t baseline_model\t columns_to_keep\t date\t dates_columns\t epochs_hist\t folders_set_up\t input_shape\t \n",
      "keras\t layers\t mean_absolute_error\t mean_squared_error\t n_neurons\t np\t os\t pd\t plt\t \n",
      "sns\t test_indices\t time\t title_embeddings_df\t train_indices\t train_test_split\t tree\t y\t y_avg_r_test\t \n",
      "y_avg_r_train\t y_test\t y_train\t y_wr_test\t y_wr_train\t \n"
     ]
    }
   ],
   "source": [
    "%who\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model data for SVR\n",
    "X_baseline_train = pd.merge(\n",
    "    NMF_df_train,\n",
    "    X_train['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "\n",
    "X_baseline_test = pd.merge(\n",
    "    NMF_df_test,\n",
    "    X_test['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nostalgia</th>\n",
       "      <th>self-published/debut</th>\n",
       "      <th>story/anthology</th>\n",
       "      <th>womens_fiction</th>\n",
       "      <th>childrens_books</th>\n",
       "      <th>classic</th>\n",
       "      <th>family_drama</th>\n",
       "      <th>digital_books/recreations</th>\n",
       "      <th>reproduced</th>\n",
       "      <th>murder_mystery</th>\n",
       "      <th>...</th>\n",
       "      <th>bestselling_author</th>\n",
       "      <th>romance</th>\n",
       "      <th>unkonwn</th>\n",
       "      <th>teen</th>\n",
       "      <th>novel</th>\n",
       "      <th>world/war/historical_fiction</th>\n",
       "      <th>unknown</th>\n",
       "      <th>young_adult</th>\n",
       "      <th>coming_of_age</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.016551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006398</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032729</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.163073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.008190</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.029208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.022995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>0.050027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029441</td>\n",
       "      <td>0.015154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019377</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.010284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070030</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009232</td>\n",
       "      <td>0.029732</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.013522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        nostalgia  self-published/debut  story/anthology  womens_fiction  \\\n",
       "3        0.007321              0.000000         0.000000        0.000000   \n",
       "24       0.017931              0.000181         0.016551        0.000000   \n",
       "42       0.000572              0.000000         0.000000        0.000000   \n",
       "49       0.000000              0.000000         0.000627        0.163073   \n",
       "73       0.016888              0.000000         0.002722        0.000000   \n",
       "...           ...                   ...              ...             ...   \n",
       "212361   0.022995              0.000000         0.000000        0.000000   \n",
       "212365   0.018890              0.000000         0.000000        0.000000   \n",
       "212394   0.001170              0.003592         0.002157        0.000000   \n",
       "212399   0.000202              0.000000         0.002030        0.000049   \n",
       "212402   0.013522              0.000000         0.003726        0.000000   \n",
       "\n",
       "        childrens_books   classic  family_drama  digital_books/recreations  \\\n",
       "3              0.000000  0.000000      0.017810                   0.000000   \n",
       "24             0.011345  0.000000      0.004711                   0.000000   \n",
       "42             0.012089  0.000000      0.000000                   0.015269   \n",
       "49             0.000000  0.000000      0.000000                   0.000000   \n",
       "73             0.000000  0.006077      0.008190                   0.000415   \n",
       "...                 ...       ...           ...                        ...   \n",
       "212361         0.001835  0.000000      0.022703                   0.000000   \n",
       "212365         0.004152  0.000000      0.002923                   0.000000   \n",
       "212394         0.000000  0.000000      0.024553                   0.000000   \n",
       "212399         0.009378  0.000000      0.000000                   0.000000   \n",
       "212402         0.018500  0.000000      0.002072                   0.000342   \n",
       "\n",
       "        reproduced  murder_mystery  ...  bestselling_author   romance  \\\n",
       "3         0.009671        0.015191  ...            0.000000  0.040636   \n",
       "24        0.000000        0.005422  ...            0.000000  0.009457   \n",
       "42        0.000000        0.006971  ...            0.000000  0.027148   \n",
       "49        0.000000        0.000000  ...            0.000000  0.031143   \n",
       "73        0.002079        0.000000  ...            0.011133  0.000000   \n",
       "...            ...             ...  ...                 ...       ...   \n",
       "212361    0.000000        0.000000  ...            0.009513  0.050027   \n",
       "212365    0.002153        0.000000  ...            0.031135  0.000000   \n",
       "212394    0.001729        0.000000  ...            0.001626  0.000000   \n",
       "212399    0.000000        0.000000  ...            0.000000  0.000000   \n",
       "212402    0.000000        0.000575  ...            0.009417  0.000000   \n",
       "\n",
       "         unkonwn      teen     novel  world/war/historical_fiction   unknown  \\\n",
       "3       0.000000  0.000214  0.000000                      0.010339  0.000000   \n",
       "24      0.000000  0.000000  0.006398                      0.000197  0.000000   \n",
       "42      0.000000  0.056535  0.000000                      0.009953  0.000000   \n",
       "49      0.000000  0.000470  0.006613                      0.000000  0.000000   \n",
       "73      0.015559  0.029208  0.000000                      0.006137  0.007823   \n",
       "...          ...       ...       ...                           ...       ...   \n",
       "212361  0.000000  0.000000  0.000000                      0.012747  0.000000   \n",
       "212365  0.000000  0.000000  0.029441                      0.015154  0.000000   \n",
       "212394  0.000000  0.032400  0.022798                      0.009248  0.010284   \n",
       "212399  0.000000  0.070030  0.003718                      0.005264  0.000000   \n",
       "212402  0.000000  0.000000  0.016340                      0.000000  0.000000   \n",
       "\n",
       "        young_adult  coming_of_age    year  \n",
       "3          0.000000       0.000000  2005.0  \n",
       "24         0.000000       0.000621  2001.0  \n",
       "42         0.000000       0.032729  2002.0  \n",
       "49         0.019162       0.000000  1997.0  \n",
       "73         0.000000       0.000000  2016.0  \n",
       "...             ...            ...     ...  \n",
       "212361     0.000000       0.000050  2009.0  \n",
       "212365     0.000000       0.019377  1998.0  \n",
       "212394     0.000000       0.027765  2015.0  \n",
       "212399     0.009232       0.029732  2000.0  \n",
       "212402     0.000000       0.002843  2003.0  \n",
       "\n",
       "[21419 rows x 21 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21419, 21)\n",
      "(5355, 21)\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline_train.shape)\n",
    "print(X_baseline_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nostalgia</th>\n",
       "      <th>self-published/debut</th>\n",
       "      <th>story/anthology</th>\n",
       "      <th>womens_fiction</th>\n",
       "      <th>childrens_books</th>\n",
       "      <th>classic</th>\n",
       "      <th>family_drama</th>\n",
       "      <th>digital_books/recreations</th>\n",
       "      <th>reproduced</th>\n",
       "      <th>murder_mystery</th>\n",
       "      <th>...</th>\n",
       "      <th>bestselling_author</th>\n",
       "      <th>romance</th>\n",
       "      <th>unkonwn</th>\n",
       "      <th>teen</th>\n",
       "      <th>novel</th>\n",
       "      <th>world/war/historical_fiction</th>\n",
       "      <th>unknown</th>\n",
       "      <th>young_adult</th>\n",
       "      <th>coming_of_age</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.016551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006398</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032729</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.163073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.008190</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.029208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212361</th>\n",
       "      <td>0.022995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>0.050027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212365</th>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029441</td>\n",
       "      <td>0.015154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019377</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.010284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070030</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009232</td>\n",
       "      <td>0.029732</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>0.013522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21419 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        nostalgia  self-published/debut  story/anthology  womens_fiction  \\\n",
       "3        0.007321              0.000000         0.000000        0.000000   \n",
       "24       0.017931              0.000181         0.016551        0.000000   \n",
       "42       0.000572              0.000000         0.000000        0.000000   \n",
       "49       0.000000              0.000000         0.000627        0.163073   \n",
       "73       0.016888              0.000000         0.002722        0.000000   \n",
       "...           ...                   ...              ...             ...   \n",
       "212361   0.022995              0.000000         0.000000        0.000000   \n",
       "212365   0.018890              0.000000         0.000000        0.000000   \n",
       "212394   0.001170              0.003592         0.002157        0.000000   \n",
       "212399   0.000202              0.000000         0.002030        0.000049   \n",
       "212402   0.013522              0.000000         0.003726        0.000000   \n",
       "\n",
       "        childrens_books   classic  family_drama  digital_books/recreations  \\\n",
       "3              0.000000  0.000000      0.017810                   0.000000   \n",
       "24             0.011345  0.000000      0.004711                   0.000000   \n",
       "42             0.012089  0.000000      0.000000                   0.015269   \n",
       "49             0.000000  0.000000      0.000000                   0.000000   \n",
       "73             0.000000  0.006077      0.008190                   0.000415   \n",
       "...                 ...       ...           ...                        ...   \n",
       "212361         0.001835  0.000000      0.022703                   0.000000   \n",
       "212365         0.004152  0.000000      0.002923                   0.000000   \n",
       "212394         0.000000  0.000000      0.024553                   0.000000   \n",
       "212399         0.009378  0.000000      0.000000                   0.000000   \n",
       "212402         0.018500  0.000000      0.002072                   0.000342   \n",
       "\n",
       "        reproduced  murder_mystery  ...  bestselling_author   romance  \\\n",
       "3         0.009671        0.015191  ...            0.000000  0.040636   \n",
       "24        0.000000        0.005422  ...            0.000000  0.009457   \n",
       "42        0.000000        0.006971  ...            0.000000  0.027148   \n",
       "49        0.000000        0.000000  ...            0.000000  0.031143   \n",
       "73        0.002079        0.000000  ...            0.011133  0.000000   \n",
       "...            ...             ...  ...                 ...       ...   \n",
       "212361    0.000000        0.000000  ...            0.009513  0.050027   \n",
       "212365    0.002153        0.000000  ...            0.031135  0.000000   \n",
       "212394    0.001729        0.000000  ...            0.001626  0.000000   \n",
       "212399    0.000000        0.000000  ...            0.000000  0.000000   \n",
       "212402    0.000000        0.000575  ...            0.009417  0.000000   \n",
       "\n",
       "         unkonwn      teen     novel  world/war/historical_fiction   unknown  \\\n",
       "3       0.000000  0.000214  0.000000                      0.010339  0.000000   \n",
       "24      0.000000  0.000000  0.006398                      0.000197  0.000000   \n",
       "42      0.000000  0.056535  0.000000                      0.009953  0.000000   \n",
       "49      0.000000  0.000470  0.006613                      0.000000  0.000000   \n",
       "73      0.015559  0.029208  0.000000                      0.006137  0.007823   \n",
       "...          ...       ...       ...                           ...       ...   \n",
       "212361  0.000000  0.000000  0.000000                      0.012747  0.000000   \n",
       "212365  0.000000  0.000000  0.029441                      0.015154  0.000000   \n",
       "212394  0.000000  0.032400  0.022798                      0.009248  0.010284   \n",
       "212399  0.000000  0.070030  0.003718                      0.005264  0.000000   \n",
       "212402  0.000000  0.000000  0.016340                      0.000000  0.000000   \n",
       "\n",
       "        young_adult  coming_of_age    year  \n",
       "3          0.000000       0.000000  2005.0  \n",
       "24         0.000000       0.000621  2001.0  \n",
       "42         0.000000       0.032729  2002.0  \n",
       "49         0.019162       0.000000  1997.0  \n",
       "73         0.000000       0.000000  2016.0  \n",
       "...             ...            ...     ...  \n",
       "212361     0.000000       0.000050  2009.0  \n",
       "212365     0.000000       0.019377  1998.0  \n",
       "212394     0.000000       0.027765  2015.0  \n",
       "212399     0.009232       0.029732  2000.0  \n",
       "212402     0.000000       0.002843  2003.0  \n",
       "\n",
       "[21419 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With image embeddings\n",
    "X_final_train = pd.merge(\n",
    "    X_images_train,\n",
    "    X_baseline_train,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "    \n",
    "X_final_test = pd.merge(\n",
    "    X_images_test,\n",
    "    X_baseline_test,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_0', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5',\n",
       "       'image_6', 'image_7', 'image_8', 'image_9',\n",
       "       ...\n",
       "       'bestselling_author', 'romance', 'unkonwn', 'teen', 'novel',\n",
       "       'world/war/historical_fiction', 'unknown', 'young_adult',\n",
       "       'coming_of_age', 'year'],\n",
       "      dtype='object', length=277)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model data for NN\n",
    "X_baseline_train_NN = NMF_intermediate_train_df\n",
    "X_baseline_test_NN = NMF_intermediate_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack description + publish year and images embeddings\n",
    "\n",
    "X_final_train_NN = pd.merge(\n",
    "    X_baseline_train_NN, \n",
    "    X_images_train, \n",
    "    left_index = True, \n",
    "    right_index = True)\n",
    "\n",
    "X_final_test_NN = pd.merge(\n",
    "    X_baseline_test_NN, \n",
    "    X_images_test, \n",
    "    left_index = True, \n",
    "    right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using NMF output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "X_baseline_train_NMF = pd.merge(\n",
    "    NMF_df_train,\n",
    "    X_train['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "\n",
    "X_baseline_test_NMF = pd.merge(\n",
    "    NMF_df_test,\n",
    "    X_test['year'],\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With image embeddings\n",
    "X_final_train_NMF = pd.merge(\n",
    "    X_images_train,\n",
    "    X_baseline_train_NMF,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')\n",
    "    \n",
    "X_final_test_NMF = pd.merge(\n",
    "    X_images_test,\n",
    "    X_baseline_test_NMF,\n",
    "    right_index = True,\n",
    "    left_index = True,\n",
    "    how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression & co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "\n",
    "# Random forest\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# SVR\n",
    "svr_model = SVR(kernel='rbf')  # 'rbf' for radial basis function kernel\n",
    "\n",
    "# Lightgbm\n",
    "\n",
    "\n",
    "# Define pipeline steps\n",
    "rf_pipeline = Pipeline([\n",
    "    ('rf', rf)  # Random Forest classifier\n",
    "])\n",
    "\n",
    "svr_pipeline = Pipeline([\n",
    "    ('svr', svr_model)  # Neural Network classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  \\\n",
       "Baseline Support Vector Regression  (SVR())   \n",
       "Final Support Vector Regression     (SVR())   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression          nostalgia  self-published/debut  story...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression          nostalgia  self-published/debut  story...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                   prediction   MAE   MSE  \n",
       "Baseline Support Vector Regression       None  None  None  \n",
       "Final Support Vector Regression          None  None  None  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "evaluation_metrics = pd.DataFrame({\n",
    "    #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "    'Baseline Support Vector Regression': {'model': svr_pipeline, 'X_train': X_baseline_train_NMF, 'X_test' : X_baseline_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Support Vector Regression': {'model': svr_pipeline, 'X_train': X_final_train_NMF, 'X_test' : X_final_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "}).transpose()\n",
    "\n",
    "evaluation_metrics = evaluation_metrics.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Support Vector Regression\n",
      "> Training completed. Duration: 00:11\n",
      "> Predictions completed. Duration: 28542986:23\n",
      "> Evaluation completed. Duration: 00:11\n",
      ">> Total time taken: 00:17\n",
      "\n",
      "\n",
      "Final Support Vector Regression\n",
      "> Training completed. Duration: 00:30\n",
      "> Predictions completed. Duration: 28542986:45\n",
      "> Evaluation completed. Duration: 00:30\n",
      ">> Total time taken: 00:42\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics.iterrows():\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "    # Call model\n",
    "    model = row['model']\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(row['X_train'], y_wr_train)\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate predictions\n",
    "    y_wr_pred = model.predict(row['X_test'])\n",
    "\n",
    "    # save predictions\n",
    "    row['prediction'] = y_wr_pred\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Predictions completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_wr_test, y_wr_pred)\n",
    "    mae = mean_absolute_error(y_wr_test, y_wr_pred)\n",
    "\n",
    "    # Save metrics\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\">> Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>[4.26307895387321, 4.2635497937961855, 4.26266...</td>\n",
       "      <td>0.131488</td>\n",
       "      <td>0.045261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.269757943778172, 4.251419682223208, 4.26035...</td>\n",
       "      <td>0.131189</td>\n",
       "      <td>0.044785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  \\\n",
       "Baseline Support Vector Regression  (SVR())   \n",
       "Final Support Vector Regression     (SVR())   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression          nostalgia  self-published/debut  story...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression          nostalgia  self-published/debut  story...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.26307895387321, 4.2635497937961855, 4.26266...   \n",
       "Final Support Vector Regression     [4.269757943778172, 4.251419682223208, 4.26035...   \n",
       "\n",
       "                                         MAE       MSE  \n",
       "Baseline Support Vector Regression  0.131488  0.045261  \n",
       "Final Support Vector Regression     0.131189  0.044785  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>None</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>None</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  \\\n",
       "Baseline Neural Network  None   \n",
       "Final Neural Network     None   \n",
       "\n",
       "                                                                   X_train  \\\n",
       "Baseline Neural Network          nostalgia  self-published/debut  story...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                    X_test  \\\n",
       "Baseline Neural Network          nostalgia  self-published/debut  story...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                        prediction   MAE   MSE  \n",
       "Baseline Neural Network       None  None  None  \n",
       "Final Neural Network          None  None  None  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up table to run different variations and store the results\n",
    "\n",
    "# evaluation_metrics_NN = pd.DataFrame({\n",
    "#     #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "#     'Baseline Neural Network': {'model': None, 'X_train': X_baseline_train_NN, 'X_test' : X_baseline_test_NN, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "#     'Final Neural Network': {'model': None, 'X_train': X_final_train_NN, 'X_test' : X_final_test_NN, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "# }).transpose()\n",
    "\n",
    "# NOTE: trying to run the NN without the dimension reduction of the NLP output\n",
    "evaluation_metrics_NN = pd.DataFrame({\n",
    "    #'Random Forest': {'model': rf_pipeline, 'prediction' : None, 'MAE' : None, 'MSE' : None},\n",
    "    'Baseline Neural Network': {'model': None, 'X_train': X_baseline_train_NMF, 'X_test' : X_baseline_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None},\n",
    "    'Final Neural Network': {'model': None, 'X_train': X_final_train_NMF, 'X_test' : X_final_test_NMF, 'prediction': None, 'MAE' : None, 'MSE' : None}\n",
    "}).transpose()\n",
    "\n",
    "evaluation_metrics_NN = evaluation_metrics_NN.rename(\n",
    "    columns  = {'index' : 'model name'}\n",
    ")\n",
    "\n",
    "evaluation_metrics_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Neural Network\n",
      "> Input shape: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m11,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │        \u001b[38;5;34m15,934\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">158,589</span> (619.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m158,589\u001b[0m (619.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">158,589</span> (619.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m158,589\u001b[0m (619.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1592.1772 - mae: 17.1470 - mean_squared_error: 1592.1802 - val_loss: 0.2935 - val_mae: 0.5165 - val_mean_squared_error: 0.2933\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.3043 - mae: 1.2641 - mean_squared_error: 3.3043 - val_loss: 0.2947 - val_mae: 0.5177 - val_mean_squared_error: 0.2946\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0754 - mae: 1.0722 - mean_squared_error: 2.0754 - val_loss: 0.2902 - val_mae: 0.5134 - val_mean_squared_error: 0.2901\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7049 - mae: 0.9890 - mean_squared_error: 1.7049 - val_loss: 0.0794 - val_mae: 0.2404 - val_mean_squared_error: 0.0783\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5746 - mae: 0.9727 - mean_squared_error: 1.5746 - val_loss: 0.3250 - val_mae: 0.5455 - val_mean_squared_error: 0.3249\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3994 - mae: 0.9365 - mean_squared_error: 1.3994 - val_loss: 0.2527 - val_mae: 0.4766 - val_mean_squared_error: 0.2525\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3593 - mae: 0.9194 - mean_squared_error: 1.3593 - val_loss: 0.0543 - val_mae: 0.1761 - val_mean_squared_error: 0.0529\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3006 - mae: 0.8984 - mean_squared_error: 1.3006 - val_loss: 0.2267 - val_mae: 0.4493 - val_mean_squared_error: 0.2264\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.2827 - mae: 0.8841 - mean_squared_error: 1.2827 - val_loss: 0.2552 - val_mae: 0.4792 - val_mean_squared_error: 0.2549\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1588 - mae: 0.8518 - mean_squared_error: 1.1589 - val_loss: 0.2947 - val_mae: 0.5178 - val_mean_squared_error: 0.2946\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1044 - mae: 0.8212 - mean_squared_error: 1.1044 - val_loss: 0.0842 - val_mae: 0.2506 - val_mean_squared_error: 0.0831\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0136 - mae: 0.7903 - mean_squared_error: 1.0136 - val_loss: 0.4533 - val_mae: 0.6508 - val_mean_squared_error: 0.4536\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9317 - mae: 0.7547 - mean_squared_error: 0.9317 - val_loss: 0.2761 - val_mae: 0.5000 - val_mean_squared_error: 0.2759\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8423 - mae: 0.7248 - mean_squared_error: 0.8423 - val_loss: 0.0742 - val_mae: 0.2294 - val_mean_squared_error: 0.0730\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7459 - mae: 0.6859 - mean_squared_error: 0.7459 - val_loss: 0.1517 - val_mae: 0.3593 - val_mean_squared_error: 0.1510\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6761 - mae: 0.6479 - mean_squared_error: 0.6761 - val_loss: 0.2078 - val_mae: 0.4286 - val_mean_squared_error: 0.2073\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5804 - mae: 0.5990 - mean_squared_error: 0.5804 - val_loss: 0.1465 - val_mae: 0.3523 - val_mean_squared_error: 0.1458\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5056 - mae: 0.5604 - mean_squared_error: 0.5056 - val_loss: 0.1649 - val_mae: 0.3768 - val_mean_squared_error: 0.1642\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.4111 - mae: 0.5077 - mean_squared_error: 0.4111 - val_loss: 0.1795 - val_mae: 0.3952 - val_mean_squared_error: 0.1789\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3529 - mae: 0.4694 - mean_squared_error: 0.3529 - val_loss: 0.1717 - val_mae: 0.3855 - val_mean_squared_error: 0.1710\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2937 - mae: 0.4272 - mean_squared_error: 0.2937 - val_loss: 0.1268 - val_mae: 0.3242 - val_mean_squared_error: 0.1260\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2333 - mae: 0.3796 - mean_squared_error: 0.2333 - val_loss: 0.0480 - val_mae: 0.1501 - val_mean_squared_error: 0.0464\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1924 - mae: 0.3455 - mean_squared_error: 0.1924 - val_loss: 0.0993 - val_mae: 0.2794 - val_mean_squared_error: 0.0983\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1535 - mae: 0.3063 - mean_squared_error: 0.1535 - val_loss: 0.0747 - val_mae: 0.2308 - val_mean_squared_error: 0.0735\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1214 - mae: 0.2681 - mean_squared_error: 0.1214 - val_loss: 0.0765 - val_mae: 0.2347 - val_mean_squared_error: 0.0753\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0943 - mae: 0.2345 - mean_squared_error: 0.0943 - val_loss: 0.0580 - val_mae: 0.1883 - val_mean_squared_error: 0.0566\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1247 - mae: 0.2151 - mean_squared_error: 0.1247 - val_loss: 0.0743 - val_mae: 0.2298 - val_mean_squared_error: 0.0730\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1433 - mae: 0.2107 - mean_squared_error: 0.1433 - val_loss: 0.0470 - val_mae: 0.1433 - val_mean_squared_error: 0.0453\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0579 - mae: 0.1670 - mean_squared_error: 0.0579 - val_loss: 0.0480 - val_mae: 0.1501 - val_mean_squared_error: 0.0463\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0481 - mae: 0.1526 - mean_squared_error: 0.0481 - val_loss: 0.0469 - val_mae: 0.1417 - val_mean_squared_error: 0.0451\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0457 - mae: 0.1457 - mean_squared_error: 0.0457 - val_loss: 0.0467 - val_mae: 0.1398 - val_mean_squared_error: 0.0450\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0432 - mae: 0.1391 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0472 - mae: 0.1380 - mean_squared_error: 0.0472 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0431 - mae: 0.1339 - mean_squared_error: 0.0431 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0433 - mae: 0.1353 - mean_squared_error: 0.0433 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0442 - mae: 0.1361 - mean_squared_error: 0.0442 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0415 - mae: 0.1327 - mean_squared_error: 0.0415 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1119 - mae: 0.1357 - mean_squared_error: 0.1119 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2312 - mae: 0.1392 - mean_squared_error: 0.2312 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0443 - mae: 0.1372 - mean_squared_error: 0.0443 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0441 - mae: 0.1357 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0955 - mae: 0.1385 - mean_squared_error: 0.0955 - val_loss: 0.0467 - val_mae: 0.1337 - val_mean_squared_error: 0.0449\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0443 - mae: 0.1348 - mean_squared_error: 0.0443 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0451 - mae: 0.1361 - mean_squared_error: 0.0451 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0432 - mae: 0.1351 - mean_squared_error: 0.0432 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0956 - mae: 0.1366 - mean_squared_error: 0.0956 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0424 - mae: 0.1342 - mean_squared_error: 0.0424 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0435 - mae: 0.1355 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1092 - mae: 0.1385 - mean_squared_error: 0.1092 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0426 - mae: 0.1331 - mean_squared_error: 0.0426 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0418 - mae: 0.1344 - mean_squared_error: 0.0418 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0432 - mae: 0.1354 - mean_squared_error: 0.0432 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0457 - mae: 0.1373 - mean_squared_error: 0.0457 - val_loss: 0.0467 - val_mae: 0.1340 - val_mean_squared_error: 0.0449\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0433 - mae: 0.1347 - mean_squared_error: 0.0433 - val_loss: 0.0468 - val_mae: 0.1334 - val_mean_squared_error: 0.0450\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0433 - mae: 0.1341 - mean_squared_error: 0.0433 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0443 - mae: 0.1353 - mean_squared_error: 0.0443 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0502 - mae: 0.1365 - mean_squared_error: 0.0502 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0453 - mae: 0.1355 - mean_squared_error: 0.0453 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0407 - mae: 0.1318 - mean_squared_error: 0.0407 - val_loss: 0.0466 - val_mae: 0.1370 - val_mean_squared_error: 0.0449\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0437 - mae: 0.1365 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1360 - val_mean_squared_error: 0.0449\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0438 - mae: 0.1353 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0432 - mae: 0.1356 - mean_squared_error: 0.0432 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0427 - mae: 0.1334 - mean_squared_error: 0.0427 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0417 - mae: 0.1334 - mean_squared_error: 0.0417 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1112 - mae: 0.1372 - mean_squared_error: 0.1112 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0438 - mae: 0.1366 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1141 - mae: 0.1400 - mean_squared_error: 0.1141 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0413 - mae: 0.1335 - mean_squared_error: 0.0413 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0534 - mae: 0.1352 - mean_squared_error: 0.0534 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0895 - mae: 0.1363 - mean_squared_error: 0.0895 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0422 - mae: 0.1340 - mean_squared_error: 0.0422 - val_loss: 0.0467 - val_mae: 0.1340 - val_mean_squared_error: 0.0449\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0438 - mae: 0.1345 - mean_squared_error: 0.0438 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0482 - mae: 0.1350 - mean_squared_error: 0.0482 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0426 - mae: 0.1341 - mean_squared_error: 0.0426 - val_loss: 0.0467 - val_mae: 0.1336 - val_mean_squared_error: 0.0449\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0425 - mae: 0.1341 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0449 - mae: 0.1359 - mean_squared_error: 0.0449 - val_loss: 0.0466 - val_mae: 0.1362 - val_mean_squared_error: 0.0449\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0417 - mae: 0.1330 - mean_squared_error: 0.0417 - val_loss: 0.0466 - val_mae: 0.1366 - val_mean_squared_error: 0.0449\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0439 - mae: 0.1361 - mean_squared_error: 0.0439 - val_loss: 0.0467 - val_mae: 0.1340 - val_mean_squared_error: 0.0449\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0430 - mae: 0.1346 - mean_squared_error: 0.0430 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0436 - mae: 0.1350 - mean_squared_error: 0.0436 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0420 - mae: 0.1321 - mean_squared_error: 0.0420 - val_loss: 0.0466 - val_mae: 0.1383 - val_mean_squared_error: 0.0449\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0429 - mae: 0.1353 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0428 - mae: 0.1345 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1364 - val_mean_squared_error: 0.0449\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0417 - mae: 0.1339 - mean_squared_error: 0.0417 - val_loss: 0.0466 - val_mae: 0.1360 - val_mean_squared_error: 0.0449\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0428 - mae: 0.1355 - mean_squared_error: 0.0428 - val_loss: 0.0467 - val_mae: 0.1337 - val_mean_squared_error: 0.0449\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0433 - mae: 0.1351 - mean_squared_error: 0.0433 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0436 - mae: 0.1355 - mean_squared_error: 0.0436 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0435 - mae: 0.1352 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0445 - mae: 0.1367 - mean_squared_error: 0.0445 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0436 - mae: 0.1333 - mean_squared_error: 0.0436 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0438 - mae: 0.1359 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mae: 0.1367 - val_mean_squared_error: 0.0449\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2108 - mae: 0.1384 - mean_squared_error: 0.2108 - val_loss: 0.0466 - val_mae: 0.1366 - val_mean_squared_error: 0.0449\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0444 - mae: 0.1377 - mean_squared_error: 0.0444 - val_loss: 0.0467 - val_mae: 0.1339 - val_mean_squared_error: 0.0449\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0429 - mae: 0.1352 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1369 - val_mean_squared_error: 0.0449\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0436 - mae: 0.1350 - mean_squared_error: 0.0436 - val_loss: 0.0467 - val_mae: 0.1337 - val_mean_squared_error: 0.0449\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0426 - mae: 0.1348 - mean_squared_error: 0.0426 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0433 - mae: 0.1350 - mean_squared_error: 0.0433 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0443 - mae: 0.1361 - mean_squared_error: 0.0443 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0438 - mae: 0.1367 - mean_squared_error: 0.0438 - val_loss: 0.0467 - val_mae: 0.1338 - val_mean_squared_error: 0.0449\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0431 - mae: 0.1340 - mean_squared_error: 0.0431 - val_loss: 0.0466 - val_mae: 0.1366 - val_mean_squared_error: 0.0449\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "> Model set up completed. Duration:  0: 30\n",
      "> Compilation completed. Duration:  28542990: 45\n",
      "> Training completed. Duration:  2: 57\n",
      "> Prediction completed. Duration:  28542990: 45\n",
      "> Evaluation completed. Duration:  2: 57\n",
      "Total time taken: 02:27\n",
      "\n",
      "\n",
      "Final Neural Network\n",
      "> Input shape: 277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">142,336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m142,336\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │        \u001b[38;5;34m15,934\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">289,661</span> (1.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m289,661\u001b[0m (1.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">289,661</span> (1.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m289,661\u001b[0m (1.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 756.9845 - mae: 10.6443 - mean_squared_error: 756.9856 - val_loss: 0.3269 - val_mae: 0.5473 - val_mean_squared_error: 0.3268\n",
      "Epoch 2/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.1302 - mae: 1.0731 - mean_squared_error: 2.1302 - val_loss: 0.1937 - val_mae: 0.4118 - val_mean_squared_error: 0.1932\n",
      "Epoch 3/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.7779 - mae: 0.9917 - mean_squared_error: 1.7778 - val_loss: 0.0491 - val_mae: 0.1319 - val_mean_squared_error: 0.0472\n",
      "Epoch 4/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.5181 - mae: 0.9696 - mean_squared_error: 1.5181 - val_loss: 0.1799 - val_mae: 0.3952 - val_mean_squared_error: 0.1793\n",
      "Epoch 5/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.3946 - mae: 0.9311 - mean_squared_error: 1.3946 - val_loss: 0.1968 - val_mae: 0.4155 - val_mean_squared_error: 0.1963\n",
      "Epoch 6/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.3134 - mae: 0.9038 - mean_squared_error: 1.3134 - val_loss: 0.1102 - val_mae: 0.2973 - val_mean_squared_error: 0.1093\n",
      "Epoch 7/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.2607 - mae: 0.8881 - mean_squared_error: 1.2607 - val_loss: 0.1740 - val_mae: 0.3878 - val_mean_squared_error: 0.1733\n",
      "Epoch 8/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.1531 - mae: 0.8511 - mean_squared_error: 1.1531 - val_loss: 0.0692 - val_mae: 0.2170 - val_mean_squared_error: 0.0679\n",
      "Epoch 9/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.1263 - mae: 0.8329 - mean_squared_error: 1.1263 - val_loss: 0.4150 - val_mae: 0.6213 - val_mean_squared_error: 0.4152\n",
      "Epoch 10/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.0675 - mae: 0.8190 - mean_squared_error: 1.0675 - val_loss: 0.5764 - val_mae: 0.7381 - val_mean_squared_error: 0.5769\n",
      "Epoch 11/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.9639 - mae: 0.7784 - mean_squared_error: 0.9639 - val_loss: 0.3609 - val_mae: 0.5770 - val_mean_squared_error: 0.3608\n",
      "Epoch 12/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.8899 - mae: 0.7443 - mean_squared_error: 0.8899 - val_loss: 0.1647 - val_mae: 0.3762 - val_mean_squared_error: 0.1640\n",
      "Epoch 13/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.7911 - mae: 0.7084 - mean_squared_error: 0.7911 - val_loss: 0.1965 - val_mae: 0.4154 - val_mean_squared_error: 0.1959\n",
      "Epoch 14/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.7196 - mae: 0.6762 - mean_squared_error: 0.7196 - val_loss: 0.1817 - val_mae: 0.3977 - val_mean_squared_error: 0.1811\n",
      "Epoch 15/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6306 - mae: 0.6306 - mean_squared_error: 0.6306 - val_loss: 0.1697 - val_mae: 0.3828 - val_mean_squared_error: 0.1690\n",
      "Epoch 16/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.5318 - mae: 0.5778 - mean_squared_error: 0.5318 - val_loss: 0.2059 - val_mae: 0.4264 - val_mean_squared_error: 0.2054\n",
      "Epoch 17/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4715 - mae: 0.5447 - mean_squared_error: 0.4715 - val_loss: 0.2770 - val_mae: 0.5012 - val_mean_squared_error: 0.2767\n",
      "Epoch 18/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.4071 - mae: 0.5050 - mean_squared_error: 0.4071 - val_loss: 0.2248 - val_mae: 0.4476 - val_mean_squared_error: 0.2244\n",
      "Epoch 19/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.3260 - mae: 0.4517 - mean_squared_error: 0.3260 - val_loss: 0.1386 - val_mae: 0.3413 - val_mean_squared_error: 0.1378\n",
      "Epoch 20/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.3084 - mae: 0.4209 - mean_squared_error: 0.3084 - val_loss: 0.1440 - val_mae: 0.3489 - val_mean_squared_error: 0.1432\n",
      "Epoch 21/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2291 - mae: 0.3737 - mean_squared_error: 0.2291 - val_loss: 0.1274 - val_mae: 0.3250 - val_mean_squared_error: 0.1265\n",
      "Epoch 22/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1801 - mae: 0.3300 - mean_squared_error: 0.1801 - val_loss: 0.0483 - val_mae: 0.1524 - val_mean_squared_error: 0.0467\n",
      "Epoch 23/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1720 - mae: 0.3026 - mean_squared_error: 0.1720 - val_loss: 0.0485 - val_mae: 0.1532 - val_mean_squared_error: 0.0468\n",
      "Epoch 24/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1220 - mae: 0.2667 - mean_squared_error: 0.1220 - val_loss: 0.0520 - val_mae: 0.1684 - val_mean_squared_error: 0.0504\n",
      "Epoch 25/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0967 - mae: 0.2278 - mean_squared_error: 0.0967 - val_loss: 0.0545 - val_mae: 0.1773 - val_mean_squared_error: 0.0530\n",
      "Epoch 26/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0784 - mae: 0.2062 - mean_squared_error: 0.0784 - val_loss: 0.0575 - val_mae: 0.1870 - val_mean_squared_error: 0.0561\n",
      "Epoch 27/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0657 - mae: 0.1827 - mean_squared_error: 0.0657 - val_loss: 0.0613 - val_mae: 0.1977 - val_mean_squared_error: 0.0599\n",
      "Epoch 28/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0572 - mae: 0.1729 - mean_squared_error: 0.0572 - val_loss: 0.0627 - val_mae: 0.2015 - val_mean_squared_error: 0.0613\n",
      "Epoch 29/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0535 - mae: 0.1631 - mean_squared_error: 0.0535 - val_loss: 0.0508 - val_mae: 0.1639 - val_mean_squared_error: 0.0493\n",
      "Epoch 30/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0772 - mae: 0.1528 - mean_squared_error: 0.0772 - val_loss: 0.0495 - val_mae: 0.1584 - val_mean_squared_error: 0.0480\n",
      "Epoch 31/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0613 - mae: 0.1409 - mean_squared_error: 0.0613 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 32/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0443 - mae: 0.1342 - mean_squared_error: 0.0443 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 33/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0448 - mae: 0.1362 - mean_squared_error: 0.0448 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 34/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0435 - mae: 0.1358 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 35/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0422 - mae: 0.1343 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 36/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0428 - mae: 0.1350 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 37/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0451 - mae: 0.1365 - mean_squared_error: 0.0451 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 38/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0418 - mae: 0.1335 - mean_squared_error: 0.0418 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 39/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0435 - mae: 0.1356 - mean_squared_error: 0.0435 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 40/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0446 - mae: 0.1365 - mean_squared_error: 0.0446 - val_loss: 0.0467 - val_mae: 0.1340 - val_mean_squared_error: 0.0449\n",
      "Epoch 41/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0429 - mae: 0.1348 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 42/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0409 - mae: 0.1325 - mean_squared_error: 0.0409 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 43/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0429 - mae: 0.1344 - mean_squared_error: 0.0429 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 44/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0433 - mae: 0.1364 - mean_squared_error: 0.0433 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 45/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0449 - mae: 0.1351 - mean_squared_error: 0.0449 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 46/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0427 - mae: 0.1351 - mean_squared_error: 0.0427 - val_loss: 0.0467 - val_mae: 0.1342 - val_mean_squared_error: 0.0449\n",
      "Epoch 47/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0444 - mae: 0.1350 - mean_squared_error: 0.0444 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 48/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0418 - mae: 0.1341 - mean_squared_error: 0.0418 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 49/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0422 - mae: 0.1327 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 50/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0439 - mae: 0.1333 - mean_squared_error: 0.0439 - val_loss: 0.0467 - val_mae: 0.1344 - val_mean_squared_error: 0.0449\n",
      "Epoch 51/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0441 - mae: 0.1344 - mean_squared_error: 0.0441 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 52/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0419 - mae: 0.1340 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1368 - val_mean_squared_error: 0.0449\n",
      "Epoch 53/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.6349 - mae: 0.1421 - mean_squared_error: 0.6349 - val_loss: 0.0466 - val_mae: 0.1350 - val_mean_squared_error: 0.0449\n",
      "Epoch 54/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0500 - mae: 0.1347 - mean_squared_error: 0.0500 - val_loss: 0.0466 - val_mae: 0.1361 - val_mean_squared_error: 0.0449\n",
      "Epoch 55/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0879 - mae: 0.1381 - mean_squared_error: 0.0879 - val_loss: 0.0469 - val_mae: 0.1325 - val_mean_squared_error: 0.0451\n",
      "Epoch 56/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0961 - mae: 0.1375 - mean_squared_error: 0.0961 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 57/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0419 - mae: 0.1344 - mean_squared_error: 0.0419 - val_loss: 0.0466 - val_mae: 0.1364 - val_mean_squared_error: 0.0449\n",
      "Epoch 58/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0430 - mae: 0.1357 - mean_squared_error: 0.0430 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 59/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0413 - mae: 0.1331 - mean_squared_error: 0.0413 - val_loss: 0.0466 - val_mae: 0.1371 - val_mean_squared_error: 0.0449\n",
      "Epoch 60/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0457 - mae: 0.1368 - mean_squared_error: 0.0457 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 61/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0445 - mae: 0.1353 - mean_squared_error: 0.0445 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "Epoch 62/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0434 - mae: 0.1361 - mean_squared_error: 0.0434 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 63/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0439 - mae: 0.1351 - mean_squared_error: 0.0439 - val_loss: 0.0467 - val_mae: 0.1346 - val_mean_squared_error: 0.0449\n",
      "Epoch 64/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0425 - mae: 0.1341 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 65/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - mae: 0.1339 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1368 - val_mean_squared_error: 0.0449\n",
      "Epoch 66/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0421 - mae: 0.1344 - mean_squared_error: 0.0421 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 67/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0422 - mae: 0.1339 - mean_squared_error: 0.0422 - val_loss: 0.0466 - val_mae: 0.1354 - val_mean_squared_error: 0.0449\n",
      "Epoch 68/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0433 - mae: 0.1356 - mean_squared_error: 0.0433 - val_loss: 0.0467 - val_mae: 0.1345 - val_mean_squared_error: 0.0449\n",
      "Epoch 69/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0408 - mae: 0.1323 - mean_squared_error: 0.0408 - val_loss: 0.0466 - val_mae: 0.1358 - val_mean_squared_error: 0.0449\n",
      "Epoch 70/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0425 - mae: 0.1349 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 71/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0443 - mae: 0.1367 - mean_squared_error: 0.0443 - val_loss: 0.0467 - val_mae: 0.1343 - val_mean_squared_error: 0.0449\n",
      "Epoch 72/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0446 - mae: 0.1358 - mean_squared_error: 0.0446 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 73/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0442 - mae: 0.1360 - mean_squared_error: 0.0442 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 74/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0430 - mae: 0.1355 - mean_squared_error: 0.0430 - val_loss: 0.0466 - val_mae: 0.1360 - val_mean_squared_error: 0.0449\n",
      "Epoch 75/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0434 - mae: 0.1347 - mean_squared_error: 0.0434 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 76/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0445 - mae: 0.1364 - mean_squared_error: 0.0445 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 77/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0446 - mae: 0.1362 - mean_squared_error: 0.0446 - val_loss: 0.0467 - val_mae: 0.1341 - val_mean_squared_error: 0.0449\n",
      "Epoch 78/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0424 - mae: 0.1333 - mean_squared_error: 0.0424 - val_loss: 0.0466 - val_mae: 0.1359 - val_mean_squared_error: 0.0449\n",
      "Epoch 79/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0427 - mae: 0.1340 - mean_squared_error: 0.0427 - val_loss: 0.0466 - val_mae: 0.1356 - val_mean_squared_error: 0.0449\n",
      "Epoch 80/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - mae: 0.1341 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1357 - val_mean_squared_error: 0.0449\n",
      "Epoch 81/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0430 - mae: 0.1348 - mean_squared_error: 0.0430 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 82/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2205 - mae: 0.1388 - mean_squared_error: 0.2205 - val_loss: 0.0467 - val_mae: 0.1345 - val_mean_squared_error: 0.0449\n",
      "Epoch 83/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0431 - mae: 0.1354 - mean_squared_error: 0.0431 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 84/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0438 - mae: 0.1337 - mean_squared_error: 0.0438 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 85/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - mae: 0.1341 - mean_squared_error: 0.0420 - val_loss: 0.0467 - val_mae: 0.1348 - val_mean_squared_error: 0.0449\n",
      "Epoch 86/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0429 - mae: 0.1346 - mean_squared_error: 0.0429 - val_loss: 0.0467 - val_mae: 0.1345 - val_mean_squared_error: 0.0449\n",
      "Epoch 87/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0414 - mae: 0.1332 - mean_squared_error: 0.0414 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 88/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0439 - mae: 0.1362 - mean_squared_error: 0.0439 - val_loss: 0.0466 - val_mae: 0.1363 - val_mean_squared_error: 0.0449\n",
      "Epoch 89/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0430 - mae: 0.1345 - mean_squared_error: 0.0430 - val_loss: 0.0467 - val_mae: 0.1336 - val_mean_squared_error: 0.0449\n",
      "Epoch 90/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0432 - mae: 0.1347 - mean_squared_error: 0.0432 - val_loss: 0.0467 - val_mae: 0.1347 - val_mean_squared_error: 0.0449\n",
      "Epoch 91/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0415 - mae: 0.1318 - mean_squared_error: 0.0415 - val_loss: 0.0466 - val_mae: 0.1365 - val_mean_squared_error: 0.0449\n",
      "Epoch 92/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0427 - mae: 0.1357 - mean_squared_error: 0.0427 - val_loss: 0.0466 - val_mae: 0.1352 - val_mean_squared_error: 0.0449\n",
      "Epoch 93/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0423 - mae: 0.1337 - mean_squared_error: 0.0423 - val_loss: 0.0466 - val_mae: 0.1362 - val_mean_squared_error: 0.0449\n",
      "Epoch 94/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0452 - mae: 0.1371 - mean_squared_error: 0.0452 - val_loss: 0.0466 - val_mae: 0.1349 - val_mean_squared_error: 0.0449\n",
      "Epoch 95/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0425 - mae: 0.1343 - mean_squared_error: 0.0425 - val_loss: 0.0466 - val_mae: 0.1361 - val_mean_squared_error: 0.0449\n",
      "Epoch 96/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - mae: 0.1341 - mean_squared_error: 0.0428 - val_loss: 0.0466 - val_mae: 0.1355 - val_mean_squared_error: 0.0449\n",
      "Epoch 97/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0431 - mae: 0.1348 - mean_squared_error: 0.0431 - val_loss: 0.0467 - val_mae: 0.1341 - val_mean_squared_error: 0.0449\n",
      "Epoch 98/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0437 - mae: 0.1351 - mean_squared_error: 0.0437 - val_loss: 0.0466 - val_mae: 0.1353 - val_mean_squared_error: 0.0449\n",
      "Epoch 99/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0437 - mae: 0.1359 - mean_squared_error: 0.0437 - val_loss: 0.0468 - val_mae: 0.1334 - val_mean_squared_error: 0.0450\n",
      "Epoch 100/100\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0416 - mae: 0.1322 - mean_squared_error: 0.0416 - val_loss: 0.0466 - val_mae: 0.1351 - val_mean_squared_error: 0.0449\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "> Model set up completed. Duration:  0: 30\n",
      "> Compilation completed. Duration:  28542993: 12\n",
      "> Training completed. Duration:  3: 57\n",
      "> Prediction completed. Duration:  28542993: 12\n",
      "> Evaluation completed. Duration:  3: 57\n",
      "Total time taken: 03:27\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run model for different moodels\n",
    "\n",
    "\n",
    "for i, row in evaluation_metrics_NN.iterrows():\n",
    "    start_time = time.time()\n",
    "    print(i)\n",
    "\n",
    "    input_shape = row['X_train'].shape[1]\n",
    "    print(f\"> Input shape: {input_shape}\")\n",
    "\n",
    "    # neurons number\n",
    "    n_neurons = 512\n",
    "\n",
    "### define a model\n",
    "    final_model = keras.Sequential()\n",
    "\n",
    "    # Add input layer\n",
    "    final_model.add(layers.Dense(\n",
    "                n_neurons, # number of neurons\n",
    "                input_dim = input_shape, # number of inputs \n",
    "                activation = 'relu' # activation faunction\n",
    "                ))\n",
    "\n",
    "    # Hidden - Layers\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.3, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "    final_model.add(layers.Dense(\n",
    "        256, \n",
    "        activation = \"relu\"))\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.2, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "    final_model.add(layers.Dense(\n",
    "        62, \n",
    "        activation = \"relu\"))\n",
    "    final_model.add(layers.Dropout(\n",
    "                        0.2, \n",
    "                        noise_shape=None, \n",
    "                        seed=None))\n",
    "\n",
    "    # Final layer\n",
    "    final_model.add(layers.Dense(\n",
    "        1, \n",
    "        activation = 'linear'))\n",
    "\n",
    "    final_model.summary()\n",
    "\n",
    "    # Add model to table\n",
    "    row['model'] = final_model\n",
    "    \n",
    "### Compile the model\n",
    "    final_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mean_squared_error'], \n",
    "    metrics = ['mae', 'mean_squared_error']\n",
    "    )\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_2 = time.time() - elapsed_time\n",
    "\n",
    "### Train the model\n",
    "    epochs_hist = final_model.fit(\n",
    "    row['X_train'], # input\n",
    "    y_wr_train, # output\n",
    "    epochs=100, # number of iterations\n",
    "    batch_size=50, # number of observations taken to train the data - 1030 obs/50 -> there are 17 groups (observations are taken once for epoch) so model is trained 17 times in each epoch\n",
    "    verbose=1,\n",
    "    validation_data = (row['X_test'], y_wr_test),\n",
    "    shuffle = True\n",
    "    #validation_split=0.2,    \n",
    "    )\n",
    "    # Time elapsed\n",
    "    elapsed_time_3 = time.time() - elapsed_time_2\n",
    "\n",
    "\n",
    "# ### Predictions\n",
    "    y_pred = final_model.predict(row['X_test'])\n",
    "    # Store predictions\n",
    "    row['prediction'] = y_pred\n",
    "    # Time elapsed\n",
    "    elapsed_time_4 = time.time() - elapsed_time_3\n",
    "\n",
    "\n",
    "# ### Evaluation\n",
    "    mse = mean_squared_error(y_pred, y_wr_test)\n",
    "    mae = mean_absolute_error(y_pred, y_wr_test)\n",
    "    row['MAE'] = mae\n",
    "    row['MSE'] = mse\n",
    "    \n",
    "    # Time elapsed\n",
    "    elapsed_time_5 = time.time() - elapsed_time_4\n",
    "\n",
    "\n",
    "    # Timings\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"> Model set up completed. Duration: {minutes: 02d}:{seconds: 02d}\")\n",
    "\n",
    "    minutes = int(elapsed_time_2 // 60)\n",
    "    seconds = int(elapsed_time_2 % 60)\n",
    "    print(f\"> Compilation completed. Duration: {minutes: 02d}:{seconds: 02d}\")\n",
    "\n",
    "    minutes = int(elapsed_time_3 // 60)\n",
    "    seconds = int(elapsed_time_3 % 60)\n",
    "    print(f\"> Training completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    minutes = int(elapsed_time_4 // 60)\n",
    "    seconds = int(elapsed_time_4 % 60)\n",
    "    print(f\"> Prediction completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    minutes = int(elapsed_time_5 // 60)\n",
    "    seconds = int(elapsed_time_5 % 60)\n",
    "    print(f\"> Evaluation completed. Duration: {minutes: 02d}:{seconds: 02d}\") \n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Print the time in minutes and seconds\n",
    "    print(f\"Total time taken: {minutes:02d}:{seconds:02d}\")\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_2, built=True&gt;</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>[[4.2411165], [4.2411165], [4.2411165], [4.241...</td>\n",
       "      <td>0.136608</td>\n",
       "      <td>0.044854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_3, built=True&gt;</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[[4.2464857], [4.2464857], [4.2464857], [4.246...</td>\n",
       "      <td>0.135082</td>\n",
       "      <td>0.044865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              model  \\\n",
       "Baseline Neural Network  <Sequential name=sequential_2, built=True>   \n",
       "Final Neural Network     <Sequential name=sequential_3, built=True>   \n",
       "\n",
       "                                                                   X_train  \\\n",
       "Baseline Neural Network          nostalgia  self-published/debut  story...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                    X_test  \\\n",
       "Baseline Neural Network          nostalgia  self-published/debut  story...   \n",
       "Final Neural Network              image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                prediction  \\\n",
       "Baseline Neural Network  [[4.2411165], [4.2411165], [4.2411165], [4.241...   \n",
       "Final Neural Network     [[4.2464857], [4.2464857], [4.2464857], [4.246...   \n",
       "\n",
       "                              MAE       MSE  \n",
       "Baseline Neural Network  0.136608  0.044854  \n",
       "Final Neural Network     0.135082  0.044865  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_all = pd.concat([evaluation_metrics, evaluation_metrics_NN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>X_train</th>\n",
       "      <th>X_test</th>\n",
       "      <th>prediction</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>[4.26307895387321, 4.2635497937961855, 4.26266...</td>\n",
       "      <td>0.131488</td>\n",
       "      <td>0.045261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Support Vector Regression</th>\n",
       "      <td>(SVR())</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[4.269757943778172, 4.251419682223208, 4.26035...</td>\n",
       "      <td>0.131189</td>\n",
       "      <td>0.044785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_2, built=True&gt;</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>nostalgia  self-published/debut  story...</td>\n",
       "      <td>[[4.2411165], [4.2411165], [4.2411165], [4.241...</td>\n",
       "      <td>0.136608</td>\n",
       "      <td>0.044854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Neural Network</th>\n",
       "      <td>&lt;Sequential name=sequential_3, built=True&gt;</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>image_0   image_1   image_2   image_3...</td>\n",
       "      <td>[[4.2464857], [4.2464857], [4.2464857], [4.246...</td>\n",
       "      <td>0.135082</td>\n",
       "      <td>0.044865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         model  \\\n",
       "Baseline Support Vector Regression                                     (SVR())   \n",
       "Final Support Vector Regression                                        (SVR())   \n",
       "Baseline Neural Network             <Sequential name=sequential_2, built=True>   \n",
       "Final Neural Network                <Sequential name=sequential_3, built=True>   \n",
       "\n",
       "                                                                              X_train  \\\n",
       "Baseline Support Vector Regression          nostalgia  self-published/debut  story...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                     nostalgia  self-published/debut  story...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                               X_test  \\\n",
       "Baseline Support Vector Regression          nostalgia  self-published/debut  story...   \n",
       "Final Support Vector Regression              image_0   image_1   image_2   image_3...   \n",
       "Baseline Neural Network                     nostalgia  self-published/debut  story...   \n",
       "Final Neural Network                         image_0   image_1   image_2   image_3...   \n",
       "\n",
       "                                                                           prediction  \\\n",
       "Baseline Support Vector Regression  [4.26307895387321, 4.2635497937961855, 4.26266...   \n",
       "Final Support Vector Regression     [4.269757943778172, 4.251419682223208, 4.26035...   \n",
       "Baseline Neural Network             [[4.2411165], [4.2411165], [4.2411165], [4.241...   \n",
       "Final Neural Network                [[4.2464857], [4.2464857], [4.2464857], [4.246...   \n",
       "\n",
       "                                         MAE       MSE  \n",
       "Baseline Support Vector Regression  0.131488  0.045261  \n",
       "Final Support Vector Regression     0.131189  0.044785  \n",
       "Baseline Neural Network             0.136608  0.044854  \n",
       "Final Neural Network                0.135082  0.044865  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise NN\n",
    "\n",
    "# # Plotting Loss And Mean Square Error For both Training And Test Sets\n",
    "# plt.plot(epochs_hist.history['mse'])\n",
    "# plt.plot(epochs_hist.history['val_mse'])\n",
    "# plt.title('MSE')\n",
    "# plt.ylabel('mae')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(output_folder, '{i} mse chart.png'))\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(epochs_hist.history['loss'])\n",
    "# plt.plot(epochs_hist.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.savefig('4.png')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(output_folder, '{i} summary chart.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid (from estimator)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=SVR(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error', \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "grid_search.fit(X_baseline_train_NMF, y_wr_train) \n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Get the best estimator\n",
    "best_estimator = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
